{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a> Â  <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Computation as transformation of representational geometries\n",
    "\n",
    "**Week 1, Day 3: Comparing Artificial And Biological Networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Hossein Adeli\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Patrick Mineault, Hlib Solodzhuk, Alex Murphy\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk, Patrick Mineault, Alex Murphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "In this tutorial, we explore the insight from the Intro Lecture that the computations performed by a neural network can be understood as a sequence of transformations of the representation of the input. We'll see that each transformation can remove information, preserve other information, and change the format in which the information is represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/uwn2g/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/uwn2g/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install and import feedback gadget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install torch torchvision matplotlib numpy scikit-learn scipy vibecheck --quiet\n",
    "!pip install rsatoolbox==0.1.5 --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D3_T2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "# Standard library imports\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "# External libraries: General utilities\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# SciPy for statistical functions\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-Learn for machine learning utilities\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "\n",
    "# RSA toolbox specific imports\n",
    "import rsatoolbox\n",
    "from rsatoolbox.data import Dataset\n",
    "from rsatoolbox.rdm.calc import calc_rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"\n",
    "    Samples a specified number of images from a data loader.\n",
    "\n",
    "    Inputs:\n",
    "    - data_loader (torch.utils.data.DataLoader): Data loader containing images and labels.\n",
    "    - n (int): Number of images to sample per class.\n",
    "    - plot (bool): Whether to plot the sampled images using matplotlib.\n",
    "\n",
    "    Outputs:\n",
    "    - imgs (torch.Tensor): Sampled images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the sampled images.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "        imgs, targets = next(iter(data_loader))\n",
    "\n",
    "        imgs_o = []\n",
    "        labels = []\n",
    "        for value in range(10):\n",
    "            cat_imgs = imgs[np.where(targets == value)][0:n]\n",
    "            imgs_o.append(cat_imgs)\n",
    "            labels.append([value]*len(cat_imgs))\n",
    "\n",
    "        imgs = torch.cat(imgs_o, dim=0)\n",
    "        labels = torch.tensor(labels).flatten()\n",
    "\n",
    "        if plot:\n",
    "            plt.imshow(torch.moveaxis(make_grid(imgs, nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "            plt.axis('off')\n",
    "\n",
    "        return imgs, labels\n",
    "\n",
    "\n",
    "def plot_rdms(model_rdms):\n",
    "    \"\"\"\n",
    "    Plots the Representational Dissimilarity Matrices (RDMs) for each layer of a model.\n",
    "\n",
    "    Inputs:\n",
    "    - model_rdms (dict): A dictionary where keys are layer names and values are the corresponding RDMs.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        gs = fig.add_gridspec(1, len(model_rdms))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        for l in range(len(model_rdms)):\n",
    "\n",
    "            layer = list(model_rdms.keys())[l]\n",
    "            rdm = np.squeeze(model_rdms[layer])\n",
    "\n",
    "            if len(rdm.shape) < 2:\n",
    "                rdm = rdm.reshape( (int(np.sqrt(rdm.shape[0])), int(np.sqrt(rdm.shape[0]))) )\n",
    "\n",
    "            rdm = rdm / np.max(rdm)\n",
    "\n",
    "            ax = plt.subplot(gs[0,l])\n",
    "            ax_ = ax.imshow(rdm, cmap='magma_r')\n",
    "            ax.set_title(f'{layer}')\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        cbar_ax.text(-2.3, 0.05, 'Normalized euclidean distance', size=10, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def rep_path(model_features, model_colors, labels=None, rdm_calc_method='euclidean', rdm_comp_method='cosine'):\n",
    "    \"\"\"\n",
    "    Represents paths of model features in a reduced-dimensional space.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): Dictionary containing model features for each model.\n",
    "    - model_colors (dict): Dictionary mapping model names to colors for visualization.\n",
    "    - labels (array-like, optional): Array of labels corresponding to the model features.\n",
    "    - rdm_calc_method (str, optional): Method for calculating RDMS ('euclidean' or 'correlation').\n",
    "    - rdm_comp_method (str, optional): Method for comparing RDMS ('cosine' or 'corr').\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        path_len = []\n",
    "        path_colors = []\n",
    "        rdms_list = []\n",
    "        ax_ticks = []\n",
    "        tick_colors = []\n",
    "        model_names = list(model_features.keys())\n",
    "        for m in range(len(model_names)):\n",
    "            model_name = model_names[m]\n",
    "            features = model_features[model_name]\n",
    "            path_colors.append(model_colors[model_name])\n",
    "            path_len.append(len(features))\n",
    "            ax_ticks.append(list(features.keys()))\n",
    "            tick_colors.append([model_colors[model_name]]*len(features))\n",
    "            rdms, _ = calc_rdms(features, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "\n",
    "        path_len = np.insert(np.cumsum(path_len),0,0)\n",
    "\n",
    "        if labels is not None:\n",
    "            rdms, _ = calc_rdms({'labels' : F.one_hot(labels).float().to(device)}, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "            ax_ticks.append(['labels'])\n",
    "            tick_colors.append(['m'])\n",
    "            idx_labels = -1\n",
    "\n",
    "        rdms = rsatoolbox.rdm.concat(rdms_list)\n",
    "\n",
    "        #Flatten the list\n",
    "        ax_ticks = [l for model_layers in ax_ticks for l in model_layers]\n",
    "        tick_colors = [l for model_layers in tick_colors for l in model_layers]\n",
    "        tick_colors = ['k' if tick == 'input' else color for tick, color in zip(ax_ticks, tick_colors)]\n",
    "\n",
    "        rdms_comp = rsatoolbox.rdm.compare(rdms, rdms, method=rdm_comp_method)\n",
    "        if rdm_comp_method == 'cosine':\n",
    "            rdms_comp = np.arccos(rdms_comp)\n",
    "        rdms_comp = np.nan_to_num(rdms_comp, nan=0.0)\n",
    "\n",
    "        # Symmetrize\n",
    "        rdms_comp = (rdms_comp + rdms_comp.T) / 2.0\n",
    "\n",
    "        # reduce dim to 2\n",
    "        transformer = manifold.MDS(n_components = 2, max_iter=1000, n_init=10, normalized_stress='auto', dissimilarity=\"precomputed\")\n",
    "        dims= transformer.fit_transform(rdms_comp)\n",
    "\n",
    "        # remove duplicates of the input layer from multiple models\n",
    "        remove_duplicates = np.where(np.array(ax_ticks) == 'input')[0][1:]\n",
    "        for index in remove_duplicates:\n",
    "            del ax_ticks[index]\n",
    "            del tick_colors[index]\n",
    "            rdms_comp = np.delete(np.delete(rdms_comp, index, axis=0), index, axis=1)\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        ax = plt.subplot(gs[0,0])\n",
    "        ax_ = ax.imshow(rdms_comp, cmap='viridis_r')\n",
    "        fig.subplots_adjust(left=0.2)\n",
    "        cbar_ax = fig.add_axes([-0.01, 0.2, 0.01, 0.5])\n",
    "        #cbar_ax.text(-7, 0.05, 'dissimilarity between rdms', size=10, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax,location='left')\n",
    "        ax.set_title('Dissimilarity between layer rdms', fontdict = {'fontsize': 14})\n",
    "        ax.set_xticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7, rotation=83)\n",
    "        ax.set_yticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7)\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.xaxis.get_ticklabels())]\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.yaxis.get_ticklabels())]\n",
    "\n",
    "        ax = plt.subplot(gs[0,1])\n",
    "        amin, amax = dims.min(), dims.max()\n",
    "        amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "\n",
    "        for i in range(len(rdms_list)-1):\n",
    "\n",
    "            path_indices = np.arange(path_len[i], path_len[i+1])\n",
    "            ax.plot(dims[path_indices, 0], dims[path_indices, 1], color=path_colors[i], marker='.')\n",
    "            ax.set_title('Representational geometry path', fontdict = {'fontsize': 14})\n",
    "            ax.set_xlim([amin, amax])\n",
    "            ax.set_ylim([amin, amax])\n",
    "            ax.set_xlabel(f\"dim 1\")\n",
    "            ax.set_ylabel(f\"dim 2\")\n",
    "\n",
    "        # if idx_input is not None:\n",
    "        idx_input = 0\n",
    "        ax.plot(dims[idx_input, 0], dims[idx_input, 1], color='k', marker='s')\n",
    "\n",
    "        if labels is not None:\n",
    "            ax.plot(dims[idx_labels, 0], dims[idx_labels, 1], color='m', marker='*')\n",
    "\n",
    "        ax.legend(model_names, fontsize=8)\n",
    "        fig.tight_layout()\n",
    "\n",
    "def plot_dim_reduction(model_features, labels, transformer_funcs):\n",
    "    \"\"\"\n",
    "    Plots the dimensionality reduction results for model features using various transformers.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): Dictionary containing model features for each layer.\n",
    "    - labels (array-like): Array of labels corresponding to the model features.\n",
    "    - transformer_funcs (list): List of dimensionality reduction techniques to apply ('PCA', 'MDS', 't-SNE').\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        transformers = []\n",
    "        for t in transformer_funcs:\n",
    "            if t == 'PCA': transformers.append(PCA(n_components=2))\n",
    "            if t == 'MDS': transformers.append(manifold.MDS(n_components = 2, normalized_stress='auto'))\n",
    "            if t == 't-SNE': transformers.append(manifold.TSNE(n_components = 2, perplexity=40, verbose=0))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 2.5*len(transformers)))\n",
    "        # and we add one plot per reference point\n",
    "        gs = fig.add_gridspec(len(transformers), len(model_features))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        return_layers = list(model_features.keys())\n",
    "\n",
    "        for f in range(len(transformer_funcs)):\n",
    "\n",
    "            for l in range(len(return_layers)):\n",
    "                layer =  return_layers[l]\n",
    "                feats = model_features[layer].detach().cpu().flatten(1)\n",
    "                feats_transformed= transformers[f].fit_transform(feats)\n",
    "\n",
    "                amin, amax = feats_transformed.min(), feats_transformed.max()\n",
    "                amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "                ax = plt.subplot(gs[f,l])\n",
    "                ax.set_xlim([amin, amax])\n",
    "                ax.set_ylim([amin, amax])\n",
    "                ax.axis(\"off\")\n",
    "                #ax.set_title(f'{layer}')\n",
    "                if f == 0: ax.text(0.5, 1.12, f'{layer}', size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                if l == 0: ax.text(-0.3, 0.5, transformer_funcs[f], size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                # Create a discrete color map based on unique labels\n",
    "                num_colors = len(np.unique(labels))\n",
    "                cmap = plt.get_cmap('viridis_r', num_colors) # 10 discrete colors\n",
    "                norm = mpl.colors.BoundaryNorm(np.arange(-0.5,num_colors), cmap.N)\n",
    "                ax_ = ax.scatter(feats_transformed[:, 0], feats_transformed[:, 1], c=labels, cmap=cmap, norm=norm)\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        fig.colorbar(ax_, cax=cbar_ax, ticks=np.linspace(0,9,10))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for image classification, consisting of two convolutional layers,\n",
    "    followed by two fully connected layers with dropout regularization.\n",
    "\n",
    "    Methods:\n",
    "    - forward(input): Defines the forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "\n",
    "        Layers:\n",
    "        - conv1: First convolutional layer with 1 input channel, 32 output channels, and a 3x3 kernel.\n",
    "        - conv2: Second convolutional layer with 32 input channels, 64 output channels, and a 3x3 kernel.\n",
    "        - dropout1: Dropout layer with a dropout probability of 0.25.\n",
    "        - dropout2: Dropout layer with a dropout probability of 0.5.\n",
    "        - fc1: First fully connected layer with 9216 input features and 128 output features.\n",
    "        - fc2: Second fully connected layer with 128 input features and 10 output features.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - input (torch.Tensor): Input tensor of shape (batch_size, 1, height, width).\n",
    "\n",
    "        Outputs:\n",
    "        - output (torch.Tensor): Output tensor of shape (batch_size, 10) representing the class probabilities for each input sample.\n",
    "        \"\"\"\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "class recurrent_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A recurrent neural network model for image classification, consisting of two convolutional layers\n",
    "    with recurrent connections and a readout layer.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(time_steps=5): Initializes the network layers and sets the number of time steps for recurrence.\n",
    "    - forward(input): Defines the forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_steps=5):\n",
    "        \"\"\"\n",
    "        Initializes the network layers and sets the number of time steps for recurrence.\n",
    "\n",
    "        Layers:\n",
    "        - conv1: First convolutional layer with 1 input channel, 16 output channels, and a 3x3 kernel with a stride of 3.\n",
    "        - conv2: Second convolutional layer with 16 input channels, 16 output channels, and a 3x3 kernel with padding of 1.\n",
    "        - readout: A sequential layer containing:\n",
    "            - dropout: Dropout layer with a dropout probability of 0.25.\n",
    "            - avgpool: Adaptive average pooling layer to reduce spatial dimensions to 1x1.\n",
    "            - flatten: Flatten layer to convert the 2D pooled output to 1D.\n",
    "            - linear: Fully connected layer with 16 input features and 10 output features.\n",
    "        - time_steps (int): Number of time steps for the recurrent connection.\n",
    "        \"\"\"\n",
    "        super(recurrent_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, 1, padding=1)\n",
    "        self.readout = nn.Sequential(OrderedDict([\n",
    "            ('dropout', nn.Dropout(0.25)),\n",
    "            ('avgpool', nn.AdaptiveAvgPool2d(1)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('linear', nn.Linear(16, 10))\n",
    "        ]))\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - input (torch.Tensor): Input tensor of shape (batch_size, 1, height, width).\n",
    "\n",
    "        Outputs:\n",
    "        - output (torch.Tensor): Output tensor of shape (batch_size, 10) representing the class probabilities for each input sample.\n",
    "        \"\"\"\n",
    "        input = self.conv1(input)\n",
    "        x = input\n",
    "        for t in range(0, self.time_steps):\n",
    "            x = input + self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.readout(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Arguments for training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - device (torch.device): The device to use for training (CPU/GPU).\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "    - epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = torch.log(output)  # to make it a log_softmax\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader, return_features=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be evaluated.\n",
    "    - device (torch.device): The device to use for evaluation (CPU/GPU).\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    - return_features (bool): If True, returns the features from the model. Default is False.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = torch.log(output)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def build_args():\n",
    "    \"\"\"\n",
    "    Builds and parses command-line arguments for training.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available() #not args.no_cuda and\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    args.use_cuda = use_cuda\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "def fetch_dataloaders(args):\n",
    "    \"\"\"\n",
    "    Fetches the data loaders for training and testing datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    \"\"\"\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if args.use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "        dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "        dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "def train_model(args, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model using the specified arguments and optimizer.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "\n",
    "    Outputs:\n",
    "    - None: The function trains the model and optionally saves it.\n",
    "    \"\"\"\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "def calc_rdms(model_features, method='correlation'):\n",
    "    \"\"\"\n",
    "    Calculates representational dissimilarity matrices (RDMs) for model features.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): A dictionary where keys are layer names and values are features of the layers.\n",
    "    - method (str): The method to calculate RDMs, e.g., 'correlation'. Default is 'correlation'.\n",
    "\n",
    "    Outputs:\n",
    "    - rdms (pyrsa.rdm.RDMs): RDMs object containing dissimilarity matrices.\n",
    "    - rdms_dict (dict): A dictionary with layer names as keys and their corresponding RDMs as values.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        if args.use_cuda:\n",
    "            feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape) > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        ds = Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = calc_rdm(ds_list, method=method)\n",
    "    rdms_dict = {list(model_features.keys())[i]: rdms.get_matrices()[i] for i in range(len(model_features))}\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Performs FGSM attack on an image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (torch.Tensor): Original image.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "    - data_grad (torch.Tensor): Gradient of the data.\n",
    "\n",
    "    Outputs:\n",
    "    - perturbed_image (torch.Tensor): Perturbed image after FGSM attack.\n",
    "    \"\"\"\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Converts a batch of normalized tensors to their original scale.\n",
    "\n",
    "    Inputs:\n",
    "    - batch (torch.Tensor): Batch of normalized tensors.\n",
    "    - mean (torch.Tensor or list): Mean used for normalization.\n",
    "    - std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - torch.Tensor: Batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(batch.device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(batch.device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "def generate_adversarial(model, imgs, targets, epsilon):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using FGSM attack.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to attack.\n",
    "    - imgs (torch.Tensor): Batch of images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "\n",
    "    Outputs:\n",
    "    - adv_imgs (torch.Tensor): Batch of adversarial images.\n",
    "    \"\"\"\n",
    "    adv_imgs = []\n",
    "\n",
    "    for img, target in zip(imgs, targets):\n",
    "        img = img.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        img.requires_grad = True\n",
    "\n",
    "        output = model(img)\n",
    "        output = torch.log(output)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad = img.grad.data\n",
    "        data_denorm = denorm(img)\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        adv_imgs.append(perturbed_data_normalized.detach())\n",
    "\n",
    "    return torch.cat(adv_imgs)\n",
    "\n",
    "def test_adversarial(model, imgs, targets):\n",
    "    \"\"\"\n",
    "    Tests the model on adversarial examples and prints the accuracy.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be tested.\n",
    "    - imgs (torch.Tensor): Batch of adversarial images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    output = model(imgs)\n",
    "    output = torch.log(output)\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    final_acc = correct / float(len(imgs))\n",
    "    print(f\"adversarial test accuracy = {correct} / {len(imgs)} = {final_acc}\")\n",
    "\n",
    "def extract_features(model, imgs, return_layers, plot='none'):\n",
    "    \"\"\"\n",
    "    Extracts features from specified layers of the model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model from which to extract features.\n",
    "    - imgs (torch.Tensor): Batch of input images.\n",
    "    - return_layers (list): List of layer names from which to extract features.\n",
    "    - plot (str): Option to plot the features. Default is 'none'.\n",
    "\n",
    "    Outputs:\n",
    "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
    "    \"\"\"\n",
    "    if return_layers == 'all':\n",
    "        return_layers, _ = get_graph_node_names(model)\n",
    "    elif return_layers == 'layers':\n",
    "        layers, _ = get_graph_node_names(model)\n",
    "        return_layers = [l for l in layers if 'input' in l or 'conv' in l or 'fc' in l]\n",
    "\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_layers)\n",
    "    model_features = feature_extractor(imgs)\n",
    "\n",
    "    return model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"standard_model.pth\", \"adversarial_model.pth\", \"recurrent_model.pth\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/s5rt6/download\", \"https://osf.io/qv5eb/download\", \"https://osf.io/6hnwk/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"2e63c2cd77bc9f1fa67673d956ec910d\", \"25fb34497377921b54368317f68a7aa7\", \"ee5cea3baa264cb78300102fa6ed66e8\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set device (GPU or CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU)\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 1: Tutorial Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Tutorial Introduction\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'PMtkxrrMDWs'), ('Bilibili', 'BV12J4m1u7Y7')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Submit your feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_tutorial_introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Computation & representational geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We are going to use the same models as in the previous tutorial, namely (i) the standard model and (ii) the adversarially trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grab a standard model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Grab a standard model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"standard_model.pth\"\n",
    "model = torch.load(path, map_location=args.device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In order to better visualize the structure of the model features, we grab 5 test images from each of the ten digit categories. Below is a simple function to perform that. For the purpose of the visualization, we order the test images by category identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Just sample 5 per category to show the order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Just sample 5 per category to show the order\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=5, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First, let's look at the computational steps in the model. Specifically, this step relates to the layer sequences in the model, e.g. convolutional layers, activation functions, fully connected layers, max pooling. These computational steps differentially affect the representations as the input passes through (and is transformed by) via the model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computational steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Computational steps\n",
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We now extract the features from the intermediate layers of the model for the sampled test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Features for different layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Features for different layers\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "print('features gathered for:', list(model_features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Calculating and plotting the RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For each of the test images, we have the model features from each layer of the trained network. We can look at how the representational (dis)similarity between these samples changes across the layers of the network. For each layer, we flatten the features so that each test image is represented by a vector (i.e., a point in space), and then we use the `rsatooblox` to compute the Euclidean distances between the points in that representational space.\n",
    "\n",
    "The result is a matrix of distances between each pair of samples for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now plot the RDMs for the input pixels, the two convolutional, and the two fully connected layers below.\n",
    "\n",
    "The RDMs are of dimension `n_stimuli` by `n_stimuli`. The brighter cells show smaller dissimilarity between the two stimuli. The diagonal has a dissimilarity of 0, as each image is maximally similar to itself.\n",
    "\n",
    "Since the instances of each category are next to each other, the brighter blocks emerging around the diagonal show that the representations for category instances become similar to one another across the levels of the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Putting it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Putting it all together\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20) #grab 20 samples from each category\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Computation in the network can be understood as the transformation of representational geometries as input is transformed into output through the neural network. As we see in the above RDMs, the geometry captures how the representation in later layers becomes more similar for instances of the same category since this model was trained for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Another way to examine the geometry of the representation changing across layers is by reducing the dimensionality and displaying the samples in 2D space.\n",
    "\n",
    "The representational spaces of each layer of a deep network project each stimulus to a point in a very high-dimensional space. Methods such as PCA (principal component analysis), MDS (Multi-dimensional Scaling), and t-SNE (t-distributed stochastic neighbor embedding) attempt to preserve the same geometry that exists in higher dimensions, in lower dimensions.\n",
    "\n",
    "Below, we look at the representational geometry of corresponding network layers using these dimensionality reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For this visualization, we take 500 samples from the test set and color-code them based on their category (class label). The figure below shows the scatter plots for these samples across the layers of the network. Each panel is the 2D projection of the feature representations using a specific dimensionality reduction method method (PCA, MDS, t-SNE).\n",
    "\n",
    "The methods all show that image representations progressively cluster based on category across the layers of the network as the input is transformed into the output prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sequential image representation clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Sequential image representation clustering\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 50 samples from the each category\n",
    "\n",
    "# Alternatively, get a whole test batch\n",
    "#imgs, labels = next(iter(test_loader))\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers)\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational path: from comparing representations to comparing RDMs\n",
    "\n",
    "We can now go a step further and think of the computational steps in a network as steps in a path in the space of representational geometries. Each layer in the network is changing the geometry to make it more and more like the desired geometry, which is a highly similar set of relationships among items from the same class/category, but each distinct from other classes. What we'll do is to *[compare RDM matrices](https://rsatoolbox.readthedocs.io/en/stable/comparing.html)* across layers.\n",
    "\n",
    "## Comparing RDMs\n",
    "\n",
    "The first step, as before, is to calculate RDMs based on the Euclidean distances between the representations of images for each layer. We stack these 2D RDMs into a 3D tensor:\n",
    "\n",
    "$$M(\\text{layer i}, \\text{image j}, \\text{image k})$$\n",
    "\n",
    "The next step is to quantify how the geometries change across the layers. To perform this operation, we first flatten the RDMs to obtain a 2D matrix:\n",
    "\n",
    "$$\\hat M(\\text{layer i}, \\text{image j x image k})$$\n",
    "\n",
    "We can then calculate the cosine similarity between the different rows of this new matrix. By taking the *arccos* (arc cosine, or the *inverse cosine function*) of this measure, we obtain a proper distance between representational geometries of different layers (Williams et al. 2021). So, we are now creating an RDM between the existing RDMs! The resulting matrix has the dimensions of `n_layers` by `n_layers`.\n",
    "\n",
    "The last step to visualize the path is to embed the distances between the geometries in a lower dimensional space. We use MDS to reduce the dimensions to 2 in order to show each computational step as a point in a 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The function below plots the computation that takes place across the network layers as a path in the space of representational geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Representational Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Representational Path\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The left panel shows the dissimilarities between the RDMs that capture the representational geometry for each layer.\n",
    "\n",
    "The brighter colors indicate higher similarity. The diagonal is 0 since each layer is maximally similar to itself. The brighter blocks around the diagonal show that distances between blocks of adjacent layers are small.\n",
    "\n",
    "The figure on the right shows the embedding of these distances in a 2D space using MDS. This figure captures the same distances between the layers shown in the matrix on the left in a 2D space. Connecting the steps starting from the input (shown as the black square) forms a path in the space of representional geometries. The last step in this path (the softmax layer) is similar to the embedding of the labels (denoted by an asterisk) in the same 2D space.\n",
    "\n",
    "There are a few things to note about the path:\n",
    "\n",
    "1. The path is generally smooth. This shows that each layer only slightly changes the representations, which we could also observe from the distance matrix on the left.\n",
    "2. The distance between the representational geometry of the input and the labels is large, but we see these two points somewhat close to each other in the path. This is partly because MDS tries to optimize to capture all the distances in the 2D space; they cannot all be veridically captured, and a stronger curvature is introduced to the path. Therefore, details of the curvature in 2D should be interpreted with caution.\n",
    "3. The size of the steps is generally informative. For example, we see in the left matrix that there is a big change in the representational geometry going from the convolutional to the fully connected layers, corresponding to a larger step in the path in the figure on the right.\n",
    "\n",
    "Thus, the computation performed by this model is a path from the geometry of the input pixels to the geometry of the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. How do you think the representational path will change if we take a smaller sample of images to create the RDMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's find out by taking a sample of 5 images per category and visualizing the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test your idea by executing this cell!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Test your idea by executing this cell!\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=5)\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As you can see, the two paths are very similar. Differences such as reflections and rotations are not meaningful as different runs of the MDS can vary along such factors. We should judge the general paths of the models to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Adversarial images\n",
    "\n",
    "Adversarial images pose a generalization challenge to DNNs. Here, we examine how this challenge and lower performance are reflected in the representational geometry path. First, we use FGSM (Fast Gradient Sign method) to generate adversarial images for the trained feedforward model. This is a \"white-box\" attack that uses the gradients from the model loss with respect to an image to adjust the image in order to maximize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational geometry in standard network\n",
    "\n",
    "Once again, we can observe the signature of these mistakes reflected in the layer-wise geometry of the representation. When the model is applied to the adversarial images, the RDMs no longer show the progressive emergence of block-wise structure strongly around the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract model features from the adversarial images and plot the new RDMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Extract model features from the adversarial images and plot the new RDMs\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "eps = 0.2\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational geometry in an adversarially trained network\n",
    "\n",
    "Let's load up an adversarially trained network and verify that it performs well on a newly generated set of adversarial images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grab an adversarially robust model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Grab an adversarially robust model\n",
    "\n",
    "path = \"adversarial_model.pth\"\n",
    "model_robust = torch.load(path, map_location=args.device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test an adversarially robust model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Test an adversarially robust model\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "adv_imgs = generate_adversarial(model_robust, imgs.to(device), labels.to(device), eps)\n",
    "test_adversarial(model_robust, adv_imgs.to(device), labels.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "And the structure in the RDMs is also restored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract robust model features from the adversarial images and plot the new RDMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Extract robust model features from the adversarial images and plot the new RDMs\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model_robust, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit your feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_representational_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Dimensionality reduction visualization\n",
    "\n",
    "Now, we are going to use MDS to visualize the changes in the representational geometry across the layers of this network in response to the original MNIST test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plot the representational geometry changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plot the representational geometry changes\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) # grab 50 samples from the test set\n",
    "model_features = extract_features(model_robust, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['MDS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit your feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_dimensionality_reduction_visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Comparing representational geometry paths between models\n",
    "\n",
    "We can generalize our method to look at the representational geometry paths of multiple models. We will first compute the RDMs for each layer of each network and then concatenate them all. Then, we look at the dissimilarity between all the layers (from different models).\n",
    "\n",
    "In this case, we will be looking at the feedforward and the robust feedforward models. The left panel in the figure below shows the dissimilarity between the representational geometries of all the layers in these two models. Networks are color-coded for clarity, with the original feedforward model shown in blue and the robust feedforward model shown in green. The input pixels are shown again in black, and the ground truth labels are in magenta.\n",
    "\n",
    "Embedding this matrix in a 2D space with MDS again gives us the path that each network took from the input images to the labels.\n",
    "\n",
    "In this case, they are both tested on the original MNIST test images, and both can perform the task. This is reflected in both models reaching the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 50 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "What if we test the models on adversarial images? We can see below that only the robust model can reach the labels in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 50 samples from the test set\n",
    "\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, adv_imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When we use adversarial images, the two networks are taking diverging paths in the space of representational geometries. Only the robust model's representation converges towards the embedding of the labels.\n",
    "\n",
    "We can see from this example that these paths depend on the chosen stimuli. Try to recall what we looked at in the last tutorial. When images were more similar, there was better generalization. The standard model, when given adversarial inputs, is fooled and struggles to map correctly and complete the path in the rightmost figures above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. Can you identify different aspects of what you see in these paths from the distance matrix on the left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Compare representational paths\n",
    "\n",
    "Train another instance of the model on the original MNIST data for only one epoch with a different seed (the basic model is trained for one epoch too). Then compare the representational paths of the two instances that are both trained on the original MNIST with one another. Display the paths in blue and cyan colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "args = build_args()\n",
    "torch.manual_seed(args.seed+1)\n",
    "args.epochs = 1\n",
    "#build_model\n",
    "model_new_seed = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model_new_seed.parameters(), lr=args.lr)\n",
    "train_model(args, model_new_seed, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_new_seed = extract_features(model_new_seed, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'model': model_features, 'model new seed': model_features_new_seed}\n",
    "model_colors = {'model': 'b', 'model new seed': 'c'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submit your feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_compare_representational_paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. What is your interpretation of how the two paths compare to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Submit your feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_computation_representational_geometries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# The Big Picture\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "- Characterized the computation that happens across different layers of a network as a path, with each step changing the geometry of the representation to go from input pixels to target labels\n",
    "- Examined the representational geometry paths for different model architectures and different inputs and learned how to interpret them\n",
    "\n",
    "We used this method to examine how models trained on adversarial stimulu (vs control) differentially treat inputs that are both normal and adversarial. We saw that the category / class level similarity structure, which was different for the standard model on adversarial stimuli, resulting in lower accuracies, actually has a divergent path during the conversion from input data to output labels. This is another link into the idea of **similarity** as a lens that helps us understand **generalization**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}