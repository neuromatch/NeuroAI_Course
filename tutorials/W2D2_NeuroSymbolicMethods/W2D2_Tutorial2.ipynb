{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W2D2_NeuroSymbolicMethods/student/W2D2_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W2D2_NeuroSymbolicMethods/student/W2D2_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Learning with structure\n",
    "\n",
    "**Week 2, Day 2: Neuro-Symbolic Methods**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ P. Michael Furlong, Chris Eliasmith\n",
    "\n",
    "__Content reviewers:__ Hlib Solodzhuk, Patrick Mineault, Aakash Agrawal, Alish Dipani, Hossein Rezaei, Yousef Ghanbari, Mostafa Abdollahi, Alex Murphy\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk, Alex Murphy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 20 minutes*\n",
    "\n",
    "This tutorial will present you with a couple of play-examples on the usage of basic operations of vector symbolic algebras while generalizing to the new knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "link_id = \"jybuw\"\n",
    "\n",
    "print(f\"If you want to download the slides: 'https://osf.io/download/{link_id}'\")\n",
    "\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup (Colab Users: Please Read)\n",
    "\n",
    "Note that because this tutorial relies on some special Python packages, these packages have requirements for specific versions of common scientific libraries, such as `numpy`. If you're in Google Colab, then as of May 2025, this comes with a later version (2.0.2) pre-installed. We require an older version (we'll be installing `1.24.4`). This causes Colab to force a session restart and then re-running of the installation cells for the new version to take effect. When you run the cell below, you will be prompted to restart the session. This is *entirely expected* and you haven't done anything wrong. Simply click 'Restart' and then run the cells as normal. \n",
    "\n",
    "An additional error might sometimes arise where an exception is raised connected to a missing element of NumPy. If this occurs, please restart the session and re-run the cells as normal and this error will go away. Updated versions of the affected libraries are expected out soon, but sadly not in time for the preparation of this material. We thank you for your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "\n",
    "!pip install numpy==1.24.4 --quiet\n",
    "!pip install scikit-learn==1.6.1 --quiet\n",
    "!pip install scipy==1.15.3 --quiet\n",
    "!pip install git+https://github.com/neuromatch/sspspace@neuromatch --no-deps --quiet\n",
    "!pip install nengo==4.0.0 --quiet\n",
    "!pip install nengo_spa==2.0.0 --quiet\n",
    "!pip install --quiet matplotlib ipywidgets vibecheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W2D2_T2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "#working with data\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "#interactive display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#modeling\n",
    "import sspspace\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import nengo_spa as spa\n",
    "from nengo_spa.algebras.hrr_algebra import HrrProperties, HrrAlgebra\n",
    "from nengo_spa.vector_generation import VectorsWithProperties\n",
    "def make_vocabulary(vector_length):\n",
    "    vec_generator = VectorsWithProperties(vector_length, algebra=HrrAlgebra(), properties = [HrrProperties.UNITARY, HrrProperties.POSITIVE])\n",
    "    vocab = spa.Vocabulary(vector_length, pointer_gen=vec_generator)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def plot_similarity_matrix(sim_mat, labels, values = False):\n",
    "    \"\"\"\n",
    "    Plot the similarity matrix between vectors.\n",
    "\n",
    "    Inputs:\n",
    "    - sim_mat (numpy.ndarray): similarity matrix between vectors.\n",
    "    - labels (list of str): list of strings which represent concepts.\n",
    "    - values (bool): True if we would like to plot values of similarity too.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.imshow(sim_mat, cmap='Greys')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(len(labels)), labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        plt.yticks(np.arange(len(labels)), labels)\n",
    "        if values:\n",
    "            for x in range(sim_mat.shape[1]):\n",
    "                for y in range(sim_mat.shape[0]):\n",
    "                    plt.text(x, y, f\"{sim_mat[y, x]:.2f}\", fontsize = 8, ha=\"center\", va=\"center\", color=\"green\")\n",
    "        plt.title('Similarity between vector-symbols')\n",
    "        plt.xlabel('Symbols')\n",
    "        plt.ylabel('Symbols')\n",
    "        plt.show()\n",
    "\n",
    "def plot_training_and_choice(losses, sims, ant_names, cons_names, action_names):\n",
    "    \"\"\"\n",
    "    Plot loss progression over training as well as predicted similarities for given rules / correct solutions.\n",
    "\n",
    "    Inputs:\n",
    "    - losses (list): list of loss values.\n",
    "    - sims (list): list of similartiy matrices.\n",
    "    - ant_names (list): list of antecedance names.\n",
    "    - cons_names (list): list of consequent names.\n",
    "    - action_names (list): full list of concepts.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.subplot(1, len(ant_names) + 1, 1)\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Training number')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Error')\n",
    "        index = 1\n",
    "        for ant_name, cons_name, sim in zip(ant_names, cons_names, sims):\n",
    "            index += 1\n",
    "            plt.subplot(1, len(ant_names) + 1, index)\n",
    "            plt.bar(range(len(action_names)), sim.flatten())\n",
    "            plt.gca().set_xticks(range(len(action_names)))\n",
    "            plt.gca().set_xticklabels(action_names, rotation=90)\n",
    "            plt.title(f'{ant_name}, not*{cons_name}')\n",
    "\n",
    "def plot_choice(sims, ant_names, cons_names, action_names):\n",
    "    \"\"\"\n",
    "    Plot predicted similarities for given rules / correct solutions.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        index = 0\n",
    "        for ant_name, cons_name, sim in zip(ant_names, cons_names, sims):\n",
    "            index += 1\n",
    "            plt.subplot(1, len(ant_names) + 1, index)\n",
    "            plt.bar(range(len(action_names)), sim.flatten())\n",
    "            plt.gca().set_xticks(range(len(action_names)))\n",
    "            plt.gca().set_xticklabels(action_names, rotation=90)\n",
    "            plt.ylabel(\"Similarity\")\n",
    "            plt.title(f'{ant_name}, not*{cons_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=None):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "symbol_names = ['monarch','heir','male','female']\n",
    "discrete_space = sspspace.DiscreteSPSpace(symbol_names, ssp_dim=1024, optimize=False)\n",
    "\n",
    "objs = {n:discrete_space.encode(n) for n in symbol_names}\n",
    "\n",
    "objs['king'] = objs['monarch'] * objs['male']\n",
    "objs['queen'] = objs['monarch'] * objs['female']\n",
    "objs['prince'] = objs['heir'] * objs['male']\n",
    "objs['princess'] = objs['heir'] * objs['female']\n",
    "\n",
    "bundle_objs = {n:discrete_space.encode(n) for n in symbol_names}\n",
    "\n",
    "bundle_objs['king'] = (bundle_objs['monarch'] + bundle_objs['male']).normalize()\n",
    "bundle_objs['queen'] = (bundle_objs['monarch'] + bundle_objs['female']).normalize()\n",
    "bundle_objs['prince'] = (bundle_objs['heir'] + bundle_objs['male']).normalize()\n",
    "bundle_objs['princess'] = (bundle_objs['heir'] + bundle_objs['female']).normalize()\n",
    "\n",
    "bundle_objs['princess_query'] = (bundle_objs['prince'] - bundle_objs['king']) + bundle_objs['queen']\n",
    "\n",
    "new_symbol_names = ['dollar', 'peso', 'ottawa', 'mexico-city', 'currency', 'capital']\n",
    "new_discrete_space = sspspace.DiscreteSPSpace(new_symbol_names, ssp_dim=1024, optimize=False)\n",
    "\n",
    "new_objs = {n:new_discrete_space.encode(n) for n in new_symbol_names}\n",
    "\n",
    "cleanup = sspspace.Cleanup(new_objs)\n",
    "\n",
    "new_objs['canada'] = ((new_objs['currency'] * new_objs['dollar']) + (new_objs['capital'] * new_objs['ottawa'])).normalize()\n",
    "new_objs['mexico'] = ((new_objs['currency'] * new_objs['peso']) + (new_objs['capital'] * new_objs['mexico-city'])).normalize()\n",
    "\n",
    "card_states = ['red','blue','odd','even','not','green','prime','implies','ant','relation','cons']\n",
    "encoder = sspspace.DiscreteSPSpace(card_states, ssp_dim=1024, optimize=False)\n",
    "vocab = {c:encoder.encode(c) for c in card_states}\n",
    "\n",
    "for a in ['red','blue','odd','even','green','prime']:\n",
    "    vocab[f'not*{a}'] = vocab['not'] * vocab[a]\n",
    "\n",
    "action_names = ['red','blue','odd','even','green','prime','not*red','not*blue','not*odd','not*even','not*green','not*prime']\n",
    "action_space = np.array([vocab[x] for x in action_names]).squeeze()\n",
    "\n",
    "rules = [\n",
    "    (vocab['ant'] * vocab['blue'] + vocab['relation'] * vocab['implies'] + vocab['cons'] * vocab['even']).normalize(),\n",
    "    (vocab['ant'] * vocab['odd'] + vocab['relation'] * vocab['implies'] + vocab['cons'] * vocab['green']).normalize(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1: Wason Card Task\n",
    "\n",
    "One of the powerful benefits of using these structured representations is being able to generalize to other circumstances. To demonstrate this, we are going to show you this in a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Wason Card Task Intro\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'BAju3MNHCq8'), ('Bilibili', 'BV1Qf421X7MB')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_wason_card_task_intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: Wason Card Task\n",
    "\n",
    "We are going to test the generalization property on the Wason Card Task, where a person is told a rule of the form \"if the card is even, then the back is blue\", they are then presented with a number of cards with either an odd number, an even number, a red back, or a blue back. The participant is asked which cards they have to flip to determine that the rule is true.\n",
    "\n",
    "In this case, the participant needs to flip only the even card(s), and any card where the back is not blue, as the rule does not state whether or not odd numbers can have blue backs, and a red-backed card with an even number would violate the rule.  We can get this from Boolean logic:\n",
    "\n",
    "$$\n",
    "\\mathrm{even} \\implies \\mathrm{blue}\n",
    "$$\n",
    "\n",
    "which is equal to \n",
    "\n",
    "$$ \n",
    "\\neg \\mathrm{even} \\vee \\mathrm{blue}\n",
    "$$\n",
    "\n",
    "where $\\neg$ means a logical not.  If we want to find cards that violate the rule, then we negate the rule, providing:\n",
    "\n",
    "$$ \n",
    "\\neg (\\neg \\mathrm{even} \\vee \\mathrm{blue}) = \\mathrm{even} \\wedge \\neg \\mathrm{blue}.\n",
    "$$\n",
    "\n",
    "Hence the cards that can violate the rule are even and not blue.  \n",
    "\n",
    "At first, we will define all needed concepts. For all noun concepts we would also like to have `not concept` presented in the space, please complete missing code parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "vector_length = 1024\n",
    "\n",
    "card_states = ['RED','BLUE','ODD','EVEN','NOT','GREEN','PRIME','IMPLIES','ANT','RELATION','CONS']\n",
    "vocab = make_vocabulary(vector_length)\n",
    "vocab.populate(';'.join(card_states))\n",
    "\n",
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete creating `not x` concepts.\")\n",
    "###################################################################\n",
    "\n",
    "for a in ['RED','BLUE','ODD','EVEN','GREEN','PRIME']:\n",
    "    vocab.add(f'NOT_{a}', vocab['NOT'] * vocab[a])\n",
    "\n",
    "action_names = ['RED','BLUE','ODD','EVEN','GREEN','PRIME','NOT_RED','NOT_BLUE','NOT_ODD','NOT_EVEN','NOT_GREEN','NOT_PRIME']\n",
    "action_space = np.array([vocab[x].v for x in action_names]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "vector_length = 1024\n",
    "\n",
    "card_states = ['RED','BLUE','ODD','EVEN','NOT','GREEN','PRIME','IMPLIES','ANT','RELATION','CONS']\n",
    "vocab = make_vocabulary(vector_length)\n",
    "vocab.populate(';'.join(card_states))\n",
    "\n",
    "\n",
    "for a in ['RED','BLUE','ODD','EVEN','GREEN','PRIME']:\n",
    "    vocab.add(f'NOT_{a}', vocab['NOT'] * vocab[a])\n",
    "\n",
    "action_names = ['RED','BLUE','ODD','EVEN','GREEN','PRIME','NOT_RED','NOT_BLUE','NOT_ODD','NOT_EVEN','NOT_GREEN','NOT_PRIME']\n",
    "action_space = np.array([vocab[x].v for x in action_names]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we are going to set up a simple perceptron-style learning rule, using the HRR (Holographic Reduced Representations) algebra.  We are going to learn a target transformation, $T$, such that given a learning rule, $A^{*} = T\\circledast R$, where $A^{*}$ is the antecedant value bundled with $\\texttt{not}$ bound with the consequent value, because we are trying to learn the cards that can violate the rule, described above, and $R$ is the rule to be learned. \n",
    "\n",
    "Rules themselves are going to be composed like the data structures representing different countries in the previous section. `ant`, `relation` and `cons` are extra concepts which define the structure and which will bind to the specific instances. \n",
    "\n",
    "If we have a rule, $X \\implies Y$, then we would create the VSA representation:\n",
    "\n",
    "$$R = \\texttt{ant}\\circledast X + \\texttt{relation}\\circledast \\text{implies} + \\texttt{cons} \\circledast Y$$,\n",
    "\n",
    "and the ideal output is:\n",
    "\n",
    "$$\n",
    "A^{*} = X + \\texttt{not}\\circledast Y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the cell below, let us define two rules:\n",
    "\n",
    "$$\\text{blue} \\implies \\text{even}$$\n",
    "$$\\text{odd} \\implies \\text{green}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete creating rules as defined above.\")\n",
    "###################################################################\n",
    "\n",
    "rules = [\n",
    "    (vocab['ANT'] * vocab['BLUE'] + vocab['RELATION'] * vocab['IMPLIES'] + vocab['CONS'] * vocab[...]).normalized(),\n",
    "    (vocab[...] * vocab[...] + vocab[...] * vocab[...] + vocab[...] * vocab[...]).normalized(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "rules = [\n",
    "    (vocab['ANT'] * vocab['BLUE'] + vocab['RELATION'] * vocab['IMPLIES'] + vocab['CONS'] * vocab['EVEN']).normalized(),\n",
    "    (vocab['ANT'] * vocab['ODD'] + vocab['RELATION'] * vocab['IMPLIES'] + vocab['CONS'] * vocab['GREEN']).normalized(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we are ready to derive the transformation! For that, we will iterate through the rules and solutions for specified number of iterations and update it as the following:\n",
    "\n",
    "$$\\Delta T \\leftarrow T - \\text{lr}*(A^{*} \\circledast \\sim R)$$\n",
    "\n",
    "where $\\text{lr}$ is learning rate constant value. Ultimately, we want $A^{*} = T\\circledast R$, so we unbind $R$ to recorver the desired transform, and use the learning rule to update our current estimated transform.\n",
    "\n",
    "We will also compute loss progression over the time and log loss function between perfect similarity (ones only for antecedance value and not consequent one) and the one we obtain between prediciton for current transformation and full action space. Complete missing parts of the code in the next cell to complete training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete training loop.\")\n",
    "###################################################################\n",
    "\n",
    "num_iters = 500\n",
    "losses = []\n",
    "sims = []\n",
    "lr = 1e-1\n",
    "ant_names = [\"BLUE\", \"ODD\"]\n",
    "cons_names = [\"EVEN\", \"GREEN\"]\n",
    "vector_length = 1024\n",
    "\n",
    "transform = np.zeros((vector_length))\n",
    "for i in range(num_iters):\n",
    "    loss = 0\n",
    "    for rule, ant_name, cons_name in zip(rules, ant_names, cons_names):\n",
    "\n",
    "        #perfect similarity\n",
    "        y_true = np.eye(len(action_names))[action_names.index(ant_name),:] + np.eye(len(action_names))[4+action_names.index(cons_name),:]\n",
    "\n",
    "        #prediction with current transform (a_hat = transform * rule)\n",
    "        a_hat = spa.SemanticPointer(transform) * ...\n",
    "\n",
    "        #similarity with current transform\n",
    "        sim_mat = np.einsum('nd,d->n', action_space, a_hat.v)\n",
    "\n",
    "        #cleanup\n",
    "        y_hat = softmax(sim_mat)\n",
    "\n",
    "        #true solution (a* = ant_name + not * cons_name)\n",
    "        a_true = (vocab[ant_name] + vocab['NOT']*vocab[...]).normalized()\n",
    "\n",
    "        #calculate loss\n",
    "        loss += log_loss(y_true, y_hat)\n",
    "\n",
    "        #update transform (T <- T - lr * (A* * (~rule)))\n",
    "        transform -= (lr) * (transform - (a_true * ~rule).v)\n",
    "        transform = transform / np.linalg.norm(transform)\n",
    "\n",
    "        #save predicted similarities if it is last iteration\n",
    "        if i == num_iters - 1:\n",
    "            sims.append(sim_mat)\n",
    "\n",
    "    #save loss\n",
    "    losses.append(np.copy(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "num_iters = 500\n",
    "losses = []\n",
    "sims = []\n",
    "lr = 1e-1\n",
    "ant_names = [\"BLUE\", \"ODD\"]\n",
    "cons_names = [\"EVEN\", \"GREEN\"]\n",
    "vector_length = 1024\n",
    "\n",
    "transform = np.zeros((vector_length))\n",
    "for i in range(num_iters):\n",
    "    loss = 0\n",
    "    for rule, ant_name, cons_name in zip(rules, ant_names, cons_names):\n",
    "\n",
    "        #perfect similarity\n",
    "        y_true = np.eye(len(action_names))[action_names.index(ant_name),:] + np.eye(len(action_names))[4+action_names.index(cons_name),:]\n",
    "\n",
    "        #prediction with current transform (a_hat = transform * rule)\n",
    "        a_hat = spa.SemanticPointer(transform) * rule\n",
    "\n",
    "        #similarity with current transform\n",
    "        sim_mat = np.einsum('nd,d->n', action_space, a_hat.v)\n",
    "\n",
    "        #cleanup\n",
    "        y_hat = softmax(sim_mat)\n",
    "\n",
    "        #true solution (a* = ant_name + not * cons_name)\n",
    "        a_true = (vocab[ant_name] + vocab['NOT']*vocab[cons_name]).normalized()\n",
    "\n",
    "        #calculate loss\n",
    "        loss += log_loss(y_true, y_hat)\n",
    "\n",
    "        #update transform (T <- T - lr * (A* * (~rule)))\n",
    "        transform -= (lr) * (transform - (a_true * ~rule).v)\n",
    "        transform = transform / np.linalg.norm(transform)\n",
    "\n",
    "        #save predicted similarities if it is last iteration\n",
    "        if i == num_iters - 1:\n",
    "            sims.append(sim_mat)\n",
    "\n",
    "    #save loss\n",
    "    losses.append(np.copy(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plot_training_and_choice(losses, sims, ant_names, cons_names, action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's see what happens when we test it on a new rule it hasn't seen before. This time we will use the rule that $\\text{red} \\implies \\text{prime}$. Your task is to complete new rule in the cell below and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete new rule and predict for it.\")\n",
    "###################################################################\n",
    "\n",
    "new_rule = (vocab['ANT'] * vocab[...] + vocab['RELATION'] * ... + vocab['CONS'] * vocab[...]).normalized()\n",
    "\n",
    "#apply transform on new rule to test the generalization of the transform\n",
    "a_hat = spa.SemanticPointer(transform) * ...\n",
    "\n",
    "new_sims = np.einsum('nd,d->n', action_space, a_hat.v)\n",
    "y_hat = softmax(new_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "new_rule = (vocab['ANT'] * vocab['RED'] + vocab['RELATION'] * vocab['IMPLIES'] + vocab['CONS'] * vocab['PRIME']).normalized()\n",
    "\n",
    "#apply transform on new rule to test the generalization of the transform\n",
    "a_hat = spa.SemanticPointer(transform) * new_rule\n",
    "\n",
    "new_sims = np.einsum('nd,d->n', action_space, a_hat.v)\n",
    "y_hat = softmax(new_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plot_choice([new_sims], [\"RED\"], [\"PRIME\"], action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's compare how a standard MLP that isn't aware of the structure in the representation performs. Here, features are going to be the rules and output - solutions. Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete MLP training.\")\n",
    "###################################################################\n",
    "\n",
    "#features - rules\n",
    "X_train = np.array(...).squeeze()\n",
    "\n",
    "#output - a* for each rule\n",
    "y_train = np.array([\n",
    "    (vocab[ant_names[0]] + vocab['NOT']*vocab[cons_names[0]]).normalized().v,\n",
    "    (vocab[ant_names[1]] + vocab['NOT']*vocab[cons_names[1]]).normalized().v,\n",
    "]).squeeze()\n",
    "\n",
    "regr = MLPRegressor(random_state=1, hidden_layer_sizes=(1024,1024), max_iter=1000).fit(..., ...)\n",
    "\n",
    "a_mlp = regr.predict(new_rule.v[None,:])\n",
    "\n",
    "mlp_sims = np.einsum('nd,md->nm', action_space, a_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "#features - rules\n",
    "X_train = np.array([r.v for r in rules]).squeeze()\n",
    "\n",
    "#output - a* for each rule\n",
    "y_train = np.array([\n",
    "    (vocab[ant_names[0]] + vocab['NOT']*vocab[cons_names[0]]).normalized().v,\n",
    "    (vocab[ant_names[1]] + vocab['NOT']*vocab[cons_names[1]]).normalized().v,\n",
    "]).squeeze()\n",
    "\n",
    "regr = MLPRegressor(random_state=1, hidden_layer_sizes=(1024,1024), max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "a_mlp = regr.predict(new_rule.v[None,:])\n",
    "\n",
    "mlp_sims = np.einsum('nd,md->nm', action_space, a_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choice([mlp_sims], [\"RED\"], [\"PRIME\"], action_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As you can see, this model, even though it is a more expressive neural network, simply learns to predict the values it had seen before when presented with a novel stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_wason_card_task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Wason Card Task Outro\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', '_77GHH8gfvk'), ('Bilibili', 'BV1rM4m1U7M3')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_wason_card_task_outro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# The Big Picture\n",
    "\n",
    "What we saw in this tutorial is the power of generalization at work, that the two top responses were indeed GREEN and NOT PRIME, as Michael outlined in the Outro video. The theme of generalization is the cornerstone of this course and one of the most fundamental aspects we want to convey to you in your NeuroAI journey with us. This tutorial highlights yet another way in which this can be achieved, through the special language of VSA. The MLP does not have access to the structural components and fails to generalize on this task. This is one of the key benefits of the VSA method, where the connection to learning underlying structures allows for generalization to occur. \n",
    "\n",
    "We have a bonus tutorial today (Tutorial 4) that covers some other cases connected to analogies, which will help to clarify and demonstrate the power of VSAs even more. We really encourage you to check out that material and then maybe revisit this tutorial if you had any trouble following along. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D2_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
