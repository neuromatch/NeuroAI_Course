{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "#!pip install numpy matplotlib torch tqdm requests torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa35e2b",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard Libraries for file and operating system operations, security, and web requests\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Core Python Data Science and Visualization Libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import profiler\n",
    "\n",
    "# Additional Utilities\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af95e4-b42d-4e65-afff-d77470f1716d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\") # update this to match your course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf7d34-d33a-4683-88e8-b27ae830ef73",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def plot_inputs_over_time(timesteps, avg_inputs):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the average inputs over time to visualize changes in input values.\n",
    "\n",
    "    Inputs:\n",
    "    - timesteps (list or array-like): A sequence of time steps at which the inputs were recorded.\n",
    "      This acts as the x-axis in the plot, representing the progression of time.\n",
    "    - avg_inputs (list or array-like): The average values of inputs corresponding to each time step.\n",
    "      These values are plotted on the y-axis, showing the magnitude of inputs over time.\n",
    "\n",
    "    Returns:\n",
    "    This function generates and displays a plot using Matplotlib.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Assuming avg_inputs is a 2D array-like structure where each column is a feature\n",
    "    num_features = avg_inputs.shape[1] if hasattr(avg_inputs, 'shape') else len(avg_inputs[0])\n",
    "\n",
    "    for feature_idx in range(num_features):\n",
    "        # Extract the current feature across all timesteps\n",
    "        current_feature_values = avg_inputs[:, feature_idx] if hasattr(avg_inputs, 'shape') else [row[feature_idx] for row in avg_inputs]\n",
    "\n",
    "        # Plot the current feature\n",
    "        plt.plot(timesteps, current_feature_values, label=f'Feature {feature_idx + 1}')\n",
    "\n",
    "    plt.title('Inputs over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Average Value')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.2, 1.1), ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_muscles_over_time(timesteps, avg_output):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the average outputs over time to visualize changes in output values.\n",
    "\n",
    "    Inputs:\n",
    "    - timesteps (list or array-like): A sequence of time steps at which the outputs were recorded.\n",
    "      This acts as the x-axis in the plot, representing the progression of time.\n",
    "    - avg_outputs (list or array-like): The average values of outputs corresponding to each time step.\n",
    "      These values are plotted on the y-axis, showing the magnitude of outputs over time.\n",
    "\n",
    "    Returns:\n",
    "    This function generates and displays a plot using Matplotlib.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(timesteps, avg_output, label='Muscle')\n",
    "    plt.title('Muscles over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Average Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_validation_losses(epoch_losses, val_losses, actual_num_epochs, title):\n",
    "\n",
    "    \"\"\"\n",
    "    This function plots the training and validation losses over epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - epoch_losses (list of float): List containing the training loss for each epoch. Each element is a float\n",
    "      representing the loss calculated after each epoch of training.\n",
    "    - val_losses (list of float): List containing the validation loss for each epoch. Similar to `epoch_losses`, but\n",
    "      for the validation set, allowing for the comparison between training and validation performance.\n",
    "    - actual_num_epochs (int): The actual number of epochs the training went through. This could be different from\n",
    "      the initially set number of epochs if early stopping was employed. It determines the range of the x-axis\n",
    "      in the plot.\n",
    "    - title (str): A string that sets the title of the plot. This allows for customization of the plot for better\n",
    "      readability and interpretation.\n",
    "\n",
    "    Outputs:\n",
    "    This function generates and displays a plot using matplotlib.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, actual_num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "    plt.plot(range(1, actual_num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_psth(data, title, bin_size=10):\n",
    "    \"\"\"\n",
    "    This function takes neural data, averages it across conditions\n",
    "    and time delays for each time bin, and plots the averaged neural activity for each feature\n",
    "    across time bins.\n",
    "\n",
    "    Args:\n",
    "        data (Tensor): A 4D tensor containing the neural data with dimensions corresponding to\n",
    "                       [conditions, delays, time, features].\n",
    "        title (str): The title for the PSTH plot. This allows users to specify the context or the\n",
    "                     experiment from which the data is derived.\n",
    "        bin_size (int, optional): The size of the time bins in units of the 'time' dimension. This\n",
    "                                  parameter allows the user to specify how much temporal data should\n",
    "                                  be averaged together to calculate the mean activity. Default is 10.\n",
    "\n",
    "    Outputs:\n",
    "    This function directly generates and displays a plot using matplotlib\n",
    "    to visually represent the averaged neural activity across time bins for each feature.\n",
    "    \"\"\"\n",
    "    # Averaging neural activity across conditions, delays for each time bin\n",
    "    mean_data = data.mean(dim=(0, 1))  # Mean across conditions and delays\n",
    "\n",
    "    # Number of bins\n",
    "    n_bins = mean_data.shape[0] // bin_size\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    binned_data = mean_data[:n_bins*bin_size].unfold(0, bin_size, bin_size).mean(dim=2)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(binned_data.shape[1]):  # Iterate over each feature/channel\n",
    "        plt.plot(binned_data[:, i], label=f'Feature {i+1}')\n",
    "    plt.xlabel('Time (bins)')\n",
    "    plt.ylabel('Average Activity')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_hidden_unit_activations(hidden_states, timesteps, neurons_to_plot=5, title='PSTHs of Hidden Units'):\n",
    "    \"\"\"\n",
    "    This function plots the average activation of a specified number of neurons from the hidden layers\n",
    "    of a neural network over a certain number of timesteps.\n",
    "\n",
    "    Inputs:\n",
    "        hidden_states (np.ndarray): A 3D numpy array containing the hidden states of a network. The dimensions\n",
    "                                     should be (time, batch, features), where 'time' represents the sequence of\n",
    "                                     timesteps, 'batch' represents different data samples, and 'features' represents\n",
    "                                     the neuron activations or features at each timestep.\n",
    "        timesteps (int): The number of timesteps to consider from the end of the hidden states array. This allows\n",
    "                         focusing on the recent activity by looking at the last 'timesteps' number of steps.\n",
    "        neurons_to_plot (int, optional): The number of neuron activations to plot, starting from the first neuron.\n",
    "                                         Defaults to 5.\n",
    "        title (str, optional): The title of the plot, allowing customization for specific analyses or presentations.\n",
    "                               Defaults to 'PSTHs of Hidden Units'.\n",
    "\n",
    "    This function generates and displays a plot of the average activation of specified\n",
    "    neurons over the selected timesteps, providing a visual analysis of neuron behavior within the network.\n",
    "    \"\"\"\n",
    "    # Slicing to take only the last 'timesteps' timesteps\n",
    "    last_hidden_states = hidden_states[-timesteps:]\n",
    "\n",
    "    # Apply the nonlinearity to each hidden state before averaging\n",
    "    rectified_tanh = lambda x: np.where(x > 0, np.tanh(x), 0)\n",
    "    hidden_states_rectified = rectified_tanh(np.array(last_hidden_states))\n",
    "\n",
    "    # Calculate the mean across all batches for each time step\n",
    "    mean_activations = np.mean(hidden_states_rectified, axis=1)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(min(neurons_to_plot, hidden_states_rectified.shape[2])):\n",
    "        plt.plot(range(timesteps), mean_activations[:, i], label=f'Neuron {i+1}')\n",
    "\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Average Activation')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_perturbation_results(perturbation_strengths, results_simple, results_complex, title):\n",
    "    \"\"\"\n",
    "    This function plots the normalized error percentages of two models (simple and complex) under various\n",
    "    perturbation strengths.\n",
    "\n",
    "    Inputs:\n",
    "        perturbation_strengths (list of float): A list of perturbation strengths tested, representing the\n",
    "                                                 magnitude of perturbations applied to the model input or parameters.\n",
    "        results_simple (list of tuples): Each tuple contains (mean error, standard deviation) for the simple model\n",
    "                                         at each perturbation strength.\n",
    "        results_complex (list of tuples): Each tuple contains (mean error, standard deviation) for the complex model\n",
    "                                          at each perturbation strength.\n",
    "        title (str): The title of the plot, allowing for customization to reflect the analysis context.\n",
    "\n",
    "    The function generates and displays a bar plot comparing the normalized error\n",
    "    rates of simple and complex models under different perturbation strengths, with error bars representing the\n",
    "    standard deviation of errors, normalized to percentage scale.\n",
    "    \"\"\"\n",
    "    mean_errors_simple, std_errors_simple = zip(*results_simple)\n",
    "    mean_errors_complex, std_errors_complex = zip(*results_complex)\n",
    "\n",
    "    # Normalizing errors and standard deviations\n",
    "    max_error_simple = max(mean_errors_simple)\n",
    "    max_error_complex = max(mean_errors_complex)\n",
    "\n",
    "    print(\"mean_errors_simple\", mean_errors_simple)\n",
    "    print(\"mean_errors_complex\", mean_errors_complex)\n",
    "\n",
    "    normalized_mean_errors_simple = [(x / max_error_simple) * 100 for x in mean_errors_simple]\n",
    "    normalized_std_errors_simple = [(y / max_error_simple) * 100 for y in std_errors_simple]\n",
    "\n",
    "    normalized_mean_errors_complex = [(x / max_error_complex) * 100 for x in mean_errors_complex]\n",
    "    normalized_std_errors_complex = [(y / max_error_complex) * 100 for y in std_errors_complex]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    bar_width = 0.35\n",
    "    bar_positions = np.arange(len(perturbation_strengths))\n",
    "\n",
    "    plt.bar(bar_positions - bar_width/2, normalized_mean_errors_simple, width=bar_width, color='blue', yerr=normalized_std_errors_simple, capsize=5, label='Simple Model')\n",
    "    plt.bar(bar_positions + bar_width/2, normalized_mean_errors_complex, width=bar_width, color='red', yerr=normalized_std_errors_complex, capsize=5, label='Complex Model')\n",
    "\n",
    "    plt.xlabel('Perturbation Magnitude')\n",
    "    plt.ylabel('Normalized Error (%)')\n",
    "    plt.title(title)\n",
    "    plt.xticks(bar_positions, [f\"{x:.5f}\" if x < 0.1 else f\"{x}\" for x in perturbation_strengths])\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b8e4b-f2ca-42af-bcd1-27fb6cecbc30",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "# @markdown\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e7c22-bd2a-4a6e-867e-462646e3b496",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed, when using `pytorch`\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fea2ec-e1ee-4ebe-9dff-d3b253c3ba0b",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "# Variables for file and download URL\n",
    "fname = \"condsForSimJ2moMuscles.mat\"  # The name of the file to be downloaded\n",
    "url = \"https://osf.io/wak7e/download\" # URL from where the file will be downloaded\n",
    "expected_md5 = \"257d16c4d92759d615bf5cac75dd9a1f\" # MD5 hash for verifying file integrity\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        # Attempt to download the file\n",
    "        r = requests.get(url) # Make a GET request to the specified URL\n",
    "    except requests.ConnectionError:\n",
    "        # Handle connection errors during the download\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        # No connection errors, proceed to check the response\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            # Check if the HTTP response status code indicates a successful download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            # Verify the integrity of the downloaded file using MD5 checksum\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            # If download is successful and data is not corrupted, save the file\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2b4f7-14f6-4374-a8a4-a17799d1f787",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def prepare_dataset(file_path, feature_idx=7, muscle_idx=1):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a .mat file for RNN training.\n",
    "\n",
    "    Args:\n",
    "    - file_path: str, path to the .mat file containing the dataset.\n",
    "    - feature_idx: int, index for individual features for plotting. Max 14.\n",
    "    - muscle_idx: int, index for muscles for plotting. Max 1.\n",
    "\n",
    "    Returns:\n",
    "    - normalised_inputs: Tensor, normalized and concatenated Plan and Go Envelope tensors.\n",
    "    - avg_output: Tensor, average muscle activity across conditions and delays.\n",
    "    - timesteps: np.ndarray, array of time steps for plotting.\n",
    "    \"\"\"\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "\n",
    "    # Extract condsForSim struct\n",
    "    conds_for_sim = data['condsForSim']\n",
    "\n",
    "    # Initialize lists to store data for all conditions\n",
    "    go_envelope_all, plan_all, muscle_all = [], [], []\n",
    "\n",
    "    # Get the number of conditions (rows) and delay durations (columns)\n",
    "    num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "    for i in range(num_conditions):  # Loop through each condition\n",
    "        go_envelope_condition, plan_condition, muscle_condition = [], [], []\n",
    "\n",
    "        for j in range(num_delays):  # Loop through each delay duration\n",
    "            condition = conds_for_sim[i, j]\n",
    "            go_envelope, plan, muscle = condition['goEnvelope'], condition['plan'], condition['muscle']\n",
    "            selected_muscle_data = muscle[:, [3, 4]]  # Select only specific muscles\n",
    "            go_envelope_condition.append(go_envelope)\n",
    "            plan_condition.append(plan)\n",
    "            muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "        # Convert lists of arrays to tensors and append to all conditions\n",
    "        go_envelope_all.append(torch.tensor(np.array(go_envelope_condition), dtype=torch.float32))\n",
    "        plan_all.append(torch.tensor(np.array(plan_condition), dtype=torch.float32))\n",
    "        muscle_all.append(torch.tensor(np.array(muscle_condition), dtype=torch.float32))\n",
    "\n",
    "    # Stack tensors for all conditions\n",
    "    go_envelope_tensor, plan_tensor, output = torch.stack(go_envelope_all), torch.stack(plan_all), torch.stack(muscle_all)\n",
    "\n",
    "    # Cleanup to free memory\n",
    "    del data, conds_for_sim, go_envelope_all, plan_all, muscle_all\n",
    "    gc.collect()\n",
    "\n",
    "    # Normalize and Standardize Plan Tensor\n",
    "    plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "\n",
    "    # Normalise and concatenate Plan and Go Envelope Tensors\n",
    "    normalised_inputs = normalize_and_standardize(torch.cat([plan_tensor, go_envelope_tensor], dim=3))\n",
    "\n",
    "    return normalised_inputs, output, num_conditions, num_delays\n",
    "\n",
    "def normalize_and_standardize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize and standardize a given tensor.\n",
    "\n",
    "    Args:\n",
    "    - tensor: Tensor, the tensor to be normalized and standardized.\n",
    "\n",
    "    Returns:\n",
    "    - standardized_normalized_tensor: Tensor, the normalized and standardized tensor.\n",
    "    \"\"\"\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)  # Normalize\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    return (tensor - mean) / std  # Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a24687-14c4-4978-9270-feb4408cf419",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "##Generalization in Neuroscience and ANNs\n",
    "\n",
    "#Generalization involves the brain's ability to adapt to changes in external and internal circumstances over a lifetime.\n",
    "#External changes include alterations in environmental features, rules, and rewards.\n",
    "#Internal changes encompass modifications within the body and brain, including homeostatic shifts and noise.\n",
    "\n",
    "##Paradigm of Artificial Neural Networks (ANNs)\n",
    "\n",
    "#ANNs are computational models inspired by the human brain, designed to mimic neuronal information processing.\n",
    "#These models utilize interconnected layers of artificial neurons to process inputs and generate outputs.\n",
    "\n",
    "##Key Components of ANNs\n",
    "\n",
    "#Neurons: basic computational units that receive, process, and generate information.\n",
    "#Weights: influence the strength of connections between neurons, akin to synaptic efficacy in biological neurons.\n",
    "#Activation Function: determines neuron activation based on weighted inputs, mirroring biological neuron thresholding.\n",
    "\n",
    "##Learning Process in ANNs\n",
    "\n",
    "#ANNs adjust connection weights based on errors between actual and desired outputs, reflecting learning mechanisms similar to synaptic plasticity in the brain.\n",
    "#This weight adjustment is crucial for the ANN's ability to generalize from learned experiences to new, unseen situations.\n",
    "\n",
    "##Inductive Biases and Task-Driven Neural Networks\n",
    "\n",
    "#Correct implementation of inductive biases is essential for effective generalization and adaptation in both the brain and ANNs.\n",
    "#Inductive biases are the model's assumptions to predict outputs for previously unseen inputs.\n",
    "#Task-driven neural networks, focused on specific tasks, are pivotal in exploring these biases, providing insights into both artificial and biological intelligence.\n",
    "\n",
    "##Synergies Between Neuroscience and AI\n",
    "\n",
    "#The study of generalization in ANNs offers valuable perspectives on the computational strategies underlying human cognition.\n",
    "#Exploring the parallels between brain adaptability and ANN functionality enhances our understanding of learning and generalization mechanisms.\n",
    "#This exploration underscores the interconnected advancements in neuroscience and artificial intelligence, shedding light on the principles of intelligence and adaptability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16cdcf-d366-452c-ac7d-4fd11221a717",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the path to the dataset file containing conditions for simulation of muscles\n",
    "file_path = 'condsForSimJ2moMuscles.mat'\n",
    "\n",
    "# Prepare the dataset by loading and processing it from the specified file path\n",
    "normalised_inputs, output, num_conditions, num_delays = prepare_dataset(file_path)\n",
    "\n",
    "print(\"Shape of the inputs\", normalised_inputs.shape)\n",
    "print(\"Shape of the output\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6d286-fc8b-4b1c-babe-c58279781637",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Averaging across conditions and delays\n",
    "avg_inputs = normalised_inputs.mean(dim=[0, 1]).squeeze()\n",
    "avg_output = output.mean(dim=[0, 1])\n",
    "\n",
    "# Time steps\n",
    "timesteps = np.arange(296)\n",
    "avg_inputs[:, -1] *= 20\n",
    "\n",
    "#Plot inputs and outputs\n",
    "plot_inputs_over_time(timesteps, avg_inputs)\n",
    "plot_muscles_over_time(timesteps, avg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba07524-81d6-4706-a88e-398e5f33fac5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Naive comparison fails without ideas from generalization: regularization.\n",
    "#The same exact ANN architecture is un-brainlike without it (SimpleRNN vs Complicated RNN)\n",
    "\n",
    "##Generalization and Regularization:\n",
    "#Generalization: The ability of a model to perform well on previously unseen data,\n",
    "#analogous to the brain's ability to understand new situations based on past experiences.\n",
    "#Regularization: Techniques used to prevent overfitting, ensuring the model generalizes\n",
    "#well by introducing constraints on the model's complexity (e.g., weight penalties, dropout).\n",
    "\n",
    "##SimpleRNN: a recurrent neural network (RNN) model with continuous-time dynamics.\n",
    "#The model is heavily regularized and therefore optimized to find simple solutions.\n",
    "\n",
    "##Complicated RNN: a recurrent neural network (RNN) model with no regularization\n",
    "#Without regulariztion, architectures can overfit and fail to generalize, becoming \"un-brainlike\"\n",
    "#in their inability to adapt to new or unseen scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde5eda-c684-4d3f-87a7-e60a19f8955d",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#The regularized models employe multiple delays between the onset of the preparatory\n",
    "#input and the onset of the hold cue. We added this feature to avoid concerns about\n",
    "#implicit time locking of model activity to the beginning of the simulation, and to\n",
    "#ensure that the model was in fact producing EMG in response to the offset of the hold cue\n",
    "\n",
    "flattened_inputs = normalised_inputs.view(-1, *normalised_inputs.shape[2:])\n",
    "flattened_targets = output.view(-1, *output.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec07a1-d43a-4ba4-ac21-3bb34a35490c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class SimpleTimeseriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        print(\"Shape of inputs - SimpleRNN\", self.inputs.shape)\n",
    "        print(\"Shape of targets - SimpleRNN\", self.targets.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Create the SimpleRNN dataset\n",
    "simple_dataset = SimpleTimeseriesDataset(flattened_inputs, flattened_targets)\n",
    "\n",
    "# Split the dataset\n",
    "simple_train_size = int(0.6 * len(simple_dataset))\n",
    "simple_val_size = int(0.2 * len(simple_dataset))\n",
    "simple_test_size = len(simple_dataset) - simple_train_size - simple_val_size\n",
    "\n",
    "simple_train_dataset, simple_val_dataset, simple_test_dataset = random_split(simple_dataset, [simple_train_size, simple_val_size, simple_test_size])\n",
    "\n",
    "batch_size = 31\n",
    "\n",
    "# Create DataLoaders\n",
    "simple_train_loader = DataLoader(simple_train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker)\n",
    "simple_val_loader = DataLoader(simple_val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)\n",
    "simple_test_loader = DataLoader(simple_test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417463ca-29c8-4229-baed-58dcf390747a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class ComplicatedTimeseriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, delay_idx):\n",
    "        \"\"\"\n",
    "        inputs: Tensor of shape [27, 8, 296, input_features]\n",
    "        targets: Tensor of shape [27, 8, 296, output_features]\n",
    "        delay_idx: Fixed index of the delay to be used\n",
    "        \"\"\"\n",
    "        self.inputs = inputs[:, delay_idx]\n",
    "        self.targets = targets[:, delay_idx]\n",
    "        self.num_conditions = inputs.shape[0]\n",
    "\n",
    "        print(\"Shape of inputs - ComplicatedRNN\", self.inputs.shape)\n",
    "        print(\"Shape of targets - ComplicatedRNN\", self.targets.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_conditions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Choose the delay index\n",
    "fixed_delay_idx = 3\n",
    "\n",
    "# Create the dataset with the fixed delay\n",
    "complicated_dataset = ComplicatedTimeseriesDataset(normalised_inputs, output, fixed_delay_idx)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.6 * len(complicated_dataset))\n",
    "val_size = int(0.2 * len(complicated_dataset))\n",
    "test_size = len(complicated_dataset) - train_size - val_size\n",
    "\n",
    "complicated_train_dataset, complicated_val_dataset, complicated_test_dataset = random_split(complicated_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 1\n",
    "complicated_train_loader = DataLoader(complicated_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "complicated_val_loader = DataLoader(complicated_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "complicated_test_loader = DataLoader(complicated_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c841c9a-27b3-414d-b406-56547c285e1c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Capture any additional outputs in 'rest'\n",
    "                output, h, *rest = model(inputs[:, t, :], h)\n",
    "\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24464e6-e897-4836-b716-3e3eaf498e43",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_test_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Capture any additional outputs in 'rest'\n",
    "                output, h, *rest = model(inputs[:, t, :], h)\n",
    "\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1bc85-f55d-40b6-b4b2-b78227996d8c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define a custom Rectified Tanh activation function\n",
    "\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0)\n",
    "def grad_rectified_tanh(x):\n",
    "    return torch.where(x > 0, 1 - torch.tanh(x)**2, 0)\n",
    "def grad_tanh(x):\n",
    "    return 1 - torch.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float))))\n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = rectified_tanh\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "        # Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)\n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1))\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))\n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)\n",
    "        # Output calculation\n",
    "        output = self.output_linear(firing_rate)\n",
    "        # Regularization terms\n",
    "        firing_rate_reg = hidden.pow(2).sum()\n",
    "        dynamic_reg = torch.linalg.norm(torch.matmul(self.J, grad_rectified_tanh(hidden.transpose(0, 1))), ord='fro', dim=(-2, -1)).sum()\n",
    "\n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16 # Features + Go Cue\n",
    "hidden_size = 150\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.5  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 1e-5\n",
    "beta = 0.003\n",
    "gamma = 1e-6\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "epoch_losses = []\n",
    "val_losses = []\n",
    "\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# get available device\n",
    "device = set_device()\n",
    "\n",
    "# Model instantiation\n",
    "set_seed(seed=2024)\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Initialize hidden states\n",
    "    hidden_states_for_plot = []\n",
    "\n",
    "    for inputs, targets in simple_train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        with autocast():  # Enable automatic mixed precision\n",
    "            for t in range(inputs.shape[1]):\n",
    "                output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "                hidden_states_for_plot.append(h.detach().cpu().numpy())\n",
    "                total_firing_rate_reg += firing_rate_reg\n",
    "                total_dynamic_reg += dynamic_reg\n",
    "\n",
    "            # Compute loss and regularization terms\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "            rfr_reg = beta * total_firing_rate_reg / inputs.shape[1] / hidden_size / num_conditions #CNT - C is 27 conditions, N is 300 neurons and T is 296 timesteps\n",
    "            rj_reg = gamma * total_dynamic_reg / inputs.shape[1] / num_conditions #CT\n",
    "            total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "\n",
    "        scaler.scale(total_loss).backward()  # Scale loss and perform backward pass\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        scaler.step(optimizer)  # Update optimizer\n",
    "        scaler.update()  # Update scaler\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(simple_train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {avg_loss}')\n",
    "\n",
    "    # Validation phase after completing the training for one epoch\n",
    "    val_loss = validate_model(model, simple_val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered at epoch\", epoch + 1)\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Clear CUDA cache if needed\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Check if training was stopped by early stopping\n",
    "if early_stop:\n",
    "    print('Training stopped due to early stopping at epoch', epoch + 1)\n",
    "else:\n",
    "    print('Finished Training')\n",
    "\n",
    "# Testing phase\n",
    "test_loss = test_model(model, simple_test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Clear cache after training\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Determine the number of epochs for which you have loss data\n",
    "actual_num_epochs = len(epoch_losses)  # This will be less than num_epochs if early stopping was triggered\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_validation_losses(epoch_losses, val_losses, actual_num_epochs, \"SimpleRNN: Training and Validation Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# ComplicatedRNN class\n",
    "class ComplicatedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(ComplicatedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Weight initialization (unchanged)\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float))))\n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity (unchanged)\n",
    "        self.nonlinearity = rectified_tanh\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass logic (same as SimpleRNN but without regularization terms)\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10\n",
    "        #Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)\n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1))\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))\n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)\n",
    "        output = self.output_linear(firing_rate)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# Training loop\n",
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 300\n",
    "output_size = 2  # Number of muscles\n",
    "g = 4  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "epoch_losses = []\n",
    "val_losses = []\n",
    "\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# get available device\n",
    "device = set_device()\n",
    "\n",
    "# Model instantiation\n",
    "complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "complicated_model.to(device)\n",
    "\n",
    "# Loss function and optimizer (no weight decay)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(complicated_model.parameters(), lr=0.001, weight_decay=0)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    hidden_states_for_plot_cm = []\n",
    "\n",
    "    complicated_model.train()  # Set the model to training mode\n",
    "    for inputs, targets in complicated_train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = complicated_model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Apply automatic mixed precision\n",
    "            for t in range(inputs.shape[1]):\n",
    "                output, h = complicated_model(inputs[:, t, :], h)\n",
    "                hidden_states_for_plot_cm.append(h.detach().cpu().numpy())\n",
    "\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "\n",
    "        scaler.scale(loss).backward()  # Scale loss for backward pass\n",
    "        scaler.step(optimizer)  # Update optimizer with scaled gradients\n",
    "        scaler.update()  # Update the scaler\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(complicated_train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {avg_loss}')\n",
    "\n",
    "    # Validation phase after completing the training for one epoch\n",
    "    val_loss = validate_model(complicated_model, complicated_val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered at epoch\", epoch + 1)\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Clear CUDA cache if needed\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Check if training was stopped by early stopping\n",
    "if early_stop:\n",
    "    print('Training stopped due to early stopping at epoch', epoch + 1)\n",
    "else:\n",
    "    print('Finished Training')\n",
    "\n",
    "# Testing phase\n",
    "test_loss = test_model(complicated_model, complicated_test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Clear cache after training\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Determine the number of epochs for which you have loss data\n",
    "actual_num_epochs = len(epoch_losses)  # This will be less than num_epochs if early stopping was triggered\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_validation_losses(epoch_losses, val_losses, actual_num_epochs, \"ComplicatedRNN - Training and validation losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd598c-a42e-4219-aa9b-5b7e54710749",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "file_path = 'condsForSimJ2moMuscles.mat'\n",
    "normalised_inputs, output, num_conditions, num_delays = prepare_dataset(file_path)\n",
    "\n",
    "# Plot PSTH for arm movement\n",
    "plot_psth(output, \"PSTH for Arm Movement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5362de-9acf-4380-af3b-68acded8eb1f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot hidden units in SimpleRNN\n",
    "plot_hidden_unit_activations(hidden_states=hidden_states_for_plot, timesteps=296, neurons_to_plot=5, title='PSTHs of Hidden Units in SimpleRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b011e-0fa3-4e5f-a423-0074f069f30c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot hidden units in ComplicatedRNN\n",
    "plot_hidden_unit_activations(hidden_states=hidden_states_for_plot_cm, timesteps=296, neurons_to_plot=5, title='PSTHs of Hidden Units in ComplicatedRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fdb1f0-a593-4682-8886-a27ae9d1c6b5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Indeed, regularized nets generalize to perturbations, emphasizing its generalization that we care about.\n",
    "\n",
    "#This design philosophy aligns with the principle of Occam's razor, where simpler models are preferred\n",
    "#for their generalizability and robustness. By focusing on simple solutions, Simple RNNs can effectively\n",
    "#capture the essential patterns in the data without overfitting to the noise or specific details,\n",
    "#making them more resilient to changes or errors in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee0281-a790-4cce-b4f2-2bd0e8da93a6",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def perturb_inputs(model, inputs, perturbation_strength):\n",
    "    device = inputs.device\n",
    "    # Perturb the inputs by adding random noise scaled by the perturbation strength and input strength\n",
    "    input_strength = torch.norm(inputs, p=2, dim=-1, keepdim=True)  # Calculate the L2 norm of inputs\n",
    "    noise = torch.rand(inputs.shape[0], 1, inputs.shape[2], device=device) * perturbation_strength * input_strength\n",
    "    perturbed_inputs = inputs + noise\n",
    "    return perturbed_inputs\n",
    "\n",
    "def compute_loss(model, inputs, targets, criterion, device):\n",
    "    batch_size = inputs.size(0)\n",
    "    h = model.init_hidden(batch_size).to(device)  # Initialize hidden state\n",
    "    losses = []\n",
    "    for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "        model_output = model(inputs[:, t, :], h)\n",
    "        output, h, *rest = model_output[:2]\n",
    "        loss = criterion(output, targets[:, t])  # Assume targets is a sequence of same length as inputs\n",
    "        losses.append(loss)\n",
    "    mean_loss = torch.mean(torch.stack(losses)).item()\n",
    "    return mean_loss\n",
    "\n",
    "def test_perturbed_inputs(model, perturbation_strengths, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    perturbation_results = []\n",
    "\n",
    "    for strength in perturbation_strengths:\n",
    "        all_errors = []  # Store all errors for each perturbation strength to compute mean and s.d.\n",
    "        print(f\"Testing perturbation strength {strength}\")\n",
    "        for iteration in range(30):  # Repeat the procedure 20 times\n",
    "            batch_errors = []  # Store errors for each batch\n",
    "            print(f\" Iteration {iteration+1}/50\")\n",
    "\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                # Compute error for original inputs\n",
    "                original_loss = compute_loss(model, inputs, targets, criterion, device)\n",
    "                # Compute error for perturbed inputs\n",
    "                perturbed_inputs = perturb_inputs(model, inputs, strength)\n",
    "                perturbed_loss = compute_loss(model, perturbed_inputs, targets, criterion, device)\n",
    "\n",
    "                # Store the normalized error difference\n",
    "                error_diff = abs(perturbed_loss - original_loss) / original_loss * 100  # Normalize as percentage\n",
    "                error_diff = min(error_diff, 100)  # Truncate at 100%\n",
    "                batch_errors.append(error_diff)\n",
    "\n",
    "            all_errors.extend(batch_errors)\n",
    "\n",
    "        mean_error = np.mean(all_errors)\n",
    "        std_error = np.std(all_errors)\n",
    "        perturbation_results.append((mean_error, std_error))\n",
    "        print(f\"Completed testing for perturbation strength {strength}.\")\n",
    "\n",
    "    return perturbation_results\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "perturbation_strengths = [0.01, 0.1, 1]\n",
    "results_complex = test_perturbed_inputs(complicated_model, perturbation_strengths, complicated_train_loader, criterion, device)\n",
    "results_simple = test_perturbed_inputs(model, perturbation_strengths, simple_train_loader, criterion, device)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268bf57-9f00-4acb-b181-f025c65b3b38",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Plot perturbation results\n",
    "plot_perturbation_results(perturbation_strengths, results_simple, results_complex, \"Perturbation of the inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73366bc5-4ce8-4ad0-8eef-9da6044deef4",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_mean_absolute_strength(model):\n",
    "    # Calculate the mean absolute connection strength of the recurrent weight matrix\n",
    "    return torch.mean(torch.abs(model.J)).item()\n",
    "\n",
    "def perturb_recurrent_weights(model, mean_strength, perturbation_percentage):\n",
    "    # Perturb the recurrent weight matrix J according to a normalized percentage of the mean absolute strength\n",
    "    perturbation_strength = mean_strength * perturbation_percentage\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(model.J) * perturbation_strength\n",
    "        perturbed_weights = model.J + noise\n",
    "        return perturbed_weights\n",
    "\n",
    "def test_perturbed_structure(model, perturbation_percentages, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    mean_strength = calculate_mean_absolute_strength(model)\n",
    "    perturbation_results = []  # List to store (mean error, std dev) tuples\n",
    "\n",
    "    original_weights = model.J.data.clone()  # Save the original weights\n",
    "\n",
    "    for percentage in perturbation_percentages:\n",
    "        multiple_perturbations_error = []\n",
    "        print(f\"Testing perturbation percentage {percentage:.4f}\")\n",
    "\n",
    "        for perturbation in range(30):  # Perturb 50 times for each strength\n",
    "            batch_errors = []\n",
    "            perturbed_weights = perturb_recurrent_weights(model, mean_strength, percentage)\n",
    "            model.J.data = perturbed_weights.data\n",
    "            print(f\" Perturbation {perturbation+1}/50\")\n",
    "\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "                for t in range(inputs.shape[1]):\n",
    "                    model_output = model(inputs[:, t, :], h)\n",
    "                    output, h = model_output[:2]\n",
    "\n",
    "                loss = criterion(output, targets[:, -1, :]).item()\n",
    "                batch_errors.append(loss)\n",
    "\n",
    "            model.J.data = original_weights.data  # Reset to original weights after each perturbation\n",
    "            multiple_perturbations_error.append(np.mean(batch_errors))\n",
    "\n",
    "        mean_error = np.mean(multiple_perturbations_error)  # Average over the 50 perturbations\n",
    "        std_dev_error = np.std(multiple_perturbations_error)  # Standard deviation for error bars\n",
    "        perturbation_results.append((mean_error, std_dev_error))\n",
    "        print(f\"Completed testing for perturbation percentage {percentage:.4f}. Mean error: {mean_error:.4f}, Std. dev.: {std_dev_error:.4f}\\n\")\n",
    "\n",
    "    return perturbation_results\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define perturbation strengths as percentages\n",
    "perturbation_strengths = [0.01, 0.1, 1]\n",
    "\n",
    "# Function calls for simple and complex models\n",
    "simple_model_errors_2 = test_perturbed_structure(model, perturbation_strengths, simple_train_loader, criterion, device)\n",
    "complex_model_errors_2 = test_perturbed_structure(complicated_model, perturbation_strengths, complicated_train_loader, criterion, device)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257161c9-9155-4dfd-b089-b12ac2725dde",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Plot perturbation results\n",
    "plot_perturbation_results(perturbation_strengths, simple_model_errors_2, complex_model_errors_2, \"Perturbation of the weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874e29b-f20d-41ce-b188-5d140a335481",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_mean_absolute_strength(model):\n",
    "    # Calculate the mean absolute connection strength of the recurrent weight matrix\n",
    "    return torch.mean(torch.abs(model.J)).item()\n",
    "\n",
    "def perturb_recurrent_weights(model, mean_strength, perturbation_percentage):\n",
    "    # Perturb the recurrent weight matrix J according to a normalized percentage of the mean absolute strength\n",
    "    perturbation_strength = mean_strength * perturbation_percentage\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(model.J) * perturbation_strength\n",
    "        perturbed_weights = model.J + noise\n",
    "        return perturbed_weights\n",
    "\n",
    "def test_perturbed_structure(model, perturbation_percentages, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    mean_strength = calculate_mean_absolute_strength(model)\n",
    "    perturbation_results = []  # List to store (mean error, std dev) tuples\n",
    "\n",
    "    original_weights = model.J.data.clone()  # Save the original weights\n",
    "\n",
    "    for percentage in perturbation_percentages:\n",
    "        multiple_perturbations_error = []\n",
    "        print(f\"Testing perturbation percentage {percentage:.4f}\")\n",
    "\n",
    "        for perturbation in range(30):  # Perturb 50 times for each strength\n",
    "            batch_errors = []\n",
    "            perturbed_weights = perturb_recurrent_weights(model, mean_strength, percentage)\n",
    "            model.J.data = perturbed_weights.data\n",
    "            print(f\" Perturbation {perturbation+1}/50\")\n",
    "\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "                for t in range(inputs.shape[1]):\n",
    "                    model_output = model(inputs[:, t, :], h)\n",
    "                    output, h = model_output[:2]\n",
    "\n",
    "                loss = criterion(output, targets[:, -1, :]).item()\n",
    "                batch_errors.append(loss)\n",
    "\n",
    "            model.J.data = original_weights.data  # Reset to original weights after each perturbation\n",
    "            multiple_perturbations_error.append(np.mean(batch_errors))\n",
    "\n",
    "        mean_error = np.mean(multiple_perturbations_error)  # Average over the 50 perturbations\n",
    "        std_dev_error = np.std(multiple_perturbations_error)  # Standard deviation for error bars\n",
    "        perturbation_results.append((mean_error, std_dev_error))\n",
    "        print(f\"Completed testing for perturbation percentage {percentage:.4f}. Mean error: {mean_error:.4f}, Std. dev.: {std_dev_error:.4f}\\n\")\n",
    "\n",
    "    return perturbation_results\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define perturbation strengths as percentages\n",
    "perturbation_strengths = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Function calls for simple and complex models\n",
    "simple_model_errors_2 = test_perturbed_structure(model, perturbation_strengths, simple_train_loader, criterion, device)\n",
    "complex_model_errors_2 = test_perturbed_structure(complicated_model, perturbation_strengths, complicated_train_loader, criterion, device)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45999d8c-0454-4585-91bb-72caa6abe829",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Is the SimpleRNN how the brain works in motor cortex?\n",
    "#If not, why is it useful to model this way?\n",
    "#Does this experiment suggest any equally generalizing system would be similarly brain-like? If not, what else is required?\n",
    "#Plasticity? Complex decision-making? Sensory integration? Agency | Autonomy?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial2",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
