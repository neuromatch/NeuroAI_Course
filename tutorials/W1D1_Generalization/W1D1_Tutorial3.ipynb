{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78e57f5-dcf1-4fea-a607-dc8b812be7eb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 3: Generalization in Cognitive Science\n",
    "\n",
    "**Week 1, Day 1: Generalization**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab73da2-b26e-457e-a1ae-0e7815c47592",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "\n",
    "1. Explore the goals of cognitive science. Understand the aims of cognitive science such as unraveling the complexities of human cognition.\n",
    "\n",
    "2. Investigate core research areas. Delve into critical research areas including decision making, problem solving, learning, thinking, and perceiving.\n",
    "\n",
    "3. Grasp the notion of generalization. Learn about generalization in cognitive science, with an emphasis on the human mind's abilities and constraints in both controlled and natural settings.\n",
    "\n",
    "4. Contrast historical cognitive approaches. Gain a perspective on historical cognitive approaches, specifically the connectionist and symbolic frameworks, and how symbolic approaches attempt to encode inductive biases.\n",
    "\n",
    "5. Develop coding skills for cognitive modeling. Acquire practical coding experience by working on parts of cognitive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a02ee-d0ae-4923-ad3c-d978b9b4aa46",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51b72e-386a-49ad-94e8-648d62124e1f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac5d8b-5d72-423b-bbb2-975e61668304",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "!pip install matplotlib numpy Pillow scikit-image scikit-learn torch torchvision scipy ipywidgets tqdm\n",
    "\n",
    "# Install GNS and pyBPL\n",
    "#!pip install git+https://github.com/neuromatch/GNS-Modeling\n",
    "#!pip install git+https://github.com/neuromatch/pyBPL\n",
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2870-3fd6-4a63-8360-36763673490d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard libraries\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from importlib import reload\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Data handling and visualization\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# Interactive controls in Jupyter notebooks\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Libraries for specific tasks related to gns and pybpl\n",
    "import gns\n",
    "from gns import MODEL_SAVE_PATH\n",
    "from gns.inference.parsing import get_topK_parses\n",
    "from gns.omniglot.classification import ClassificationDataset\n",
    "from gns.rendering import Renderer as DefaultRenderer\n",
    "from gns.type import TypeModel\n",
    "from gns.utils.experiments import mkdir, time_string\n",
    "import pybpl\n",
    "from pybpl import splines, parameters\n",
    "from pybpl.util import nested_map\n",
    "from pybpl.util.stroke import dist_along_traj\n",
    "from pybpl.util.general import fspecial\n",
    "\n",
    "# Utility for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload gns\n",
    "reload(gns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870241d-73fb-455f-a8ef-c92f9b2bdaf3",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452c1b0-d79d-4c17-8738-b0b342180a09",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_images(probe, options):\n",
    "    # Open the probe image and the option images\n",
    "    probe_image = Image.open(probe)\n",
    "    option_images = [Image.open(img_path) for img_path in options]\n",
    "\n",
    "    # Create a figure with the probe and the 3x3 grid for the options directly below\n",
    "    fig = plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "\n",
    "    # Add the probe image to the top of the figure with a red border\n",
    "    ax_probe = fig.add_subplot(4, 3, (1, 3))  # Span the probe across the top 3 columns\n",
    "    ax_probe.imshow(probe_image)\n",
    "    ax_probe.axis('off')\n",
    "    rect = patches.Rectangle((0, 0), probe_image.width-1, probe_image.height-1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax_probe.add_patch(rect)\n",
    "\n",
    "    # Position the 3x3 grid of option images directly below the probe image\n",
    "    for index, img in enumerate(option_images):\n",
    "        row = (index // 3) + 1  # Calculate row in the 3x3 grid, starting directly below the probe\n",
    "        col = (index % 3) + 1   # Calculate column in the 3x3 grid\n",
    "        ax_option = fig.add_subplot(4, 3, row * 3 + col)  # Adjust grid position to directly follow the probe\n",
    "        ax_option.imshow(img)\n",
    "        ax_option.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5deda-c9f0-4733-a01b-3d077a96bafb",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval for zip files\n",
    "# @markdown\n",
    "\n",
    "def handle_file_operations(fname, url, expected_md5, extract_to='data'):\n",
    "    \"\"\"Handles downloading, verifying, and extracting a file.\"\"\"\n",
    "\n",
    "    # Define helper functions for download, verify, and extract operations\n",
    "    def download_file(url, filename):\n",
    "        \"\"\"Downloads file from the given URL and saves it locally.\"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(filename, \"wb\") as fid:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    fid.write(chunk)\n",
    "            print(\"Download successful.\")\n",
    "            return True\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"!!! Failed to download data: {e} !!!\")\n",
    "            return False\n",
    "\n",
    "    def verify_file_md5(filename, expected_md5):\n",
    "        \"\"\"Verifies the file's MD5 checksum.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filename, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        if hash_md5.hexdigest() == expected_md5:\n",
    "            print(\"MD5 checksum verified.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "            return False\n",
    "\n",
    "    def extract_zip_file(filename, extract_to):\n",
    "        \"\"\"Extracts the ZIP file to the specified directory.\"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"File extracted successfully to {extract_to}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"!!! The ZIP file is corrupted or not a zip file !!!\")\n",
    "\n",
    "    # Main operation\n",
    "    if not os.path.isfile(fname) or not verify_file_md5(fname, expected_md5):\n",
    "        if download_file(url, fname) and verify_file_md5(fname, expected_md5):\n",
    "            extract_zip_file(fname, extract_to)\n",
    "    else:\n",
    "        print(f\"File '{fname}' already exists and is verified. Proceeding to extraction.\")\n",
    "        extract_zip_file(fname, extract_to)\n",
    "\n",
    "# Example usage\n",
    "file_info = [\n",
    "    {\"fname\": \"one-shot-classification.zip\", \"url\": \"https://osf.io/aw6eq/download\", \"expected_md5\": \"9376412e7fb74d64f045644b51e641a6\"},\n",
    "    {\"fname\": \"omniglot-py.zip\", \"url\": \"https://osf.io/bazxp/download\", \"expected_md5\": \"f7a4011f5c25460c6d95ee1428e377ed\"},\n",
    "    {\"fname\": \"parses.zip\", \"url\": \"https://osf.io/97bpq/download\", \"expected_md5\": \"f9e42660d860d1f95296a3729e4bd2e3\"},\n",
    "    {\"fname\": \"targets.zip\", \"url\": \"https://osf.io/stk9f/download\", \"expected_md5\": \"38bbf4a87910fe56a02ee7a77a9ded3e\"}\n",
    "]\n",
    "\n",
    "for file in file_info:\n",
    "    handle_file_operations(**file)\n",
    "\n",
    "#Current directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cf0b6-954a-414b-80b9-e356349bc934",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval for torch models\n",
    "# @markdown\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL and save it in the specified directory.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, filename)  # Ensure the file is saved in base_dir\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "\n",
    "def verify_checksum(filename, expected_checksum):\n",
    "    \"\"\"\n",
    "    Verify the MD5 checksum of a file\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the file\n",
    "    expected_checksum (str): Expected MD5 checksum\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the checksum matches, False otherwise\n",
    "    \"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            md5.update(chunk)\n",
    "\n",
    "    return md5.hexdigest() == expected_checksum\n",
    "\n",
    "def load_models(model_files, directory, map_location='cpu'):\n",
    "    \"\"\"\n",
    "    Load multiple models from a specified directory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for model_file in model_files:\n",
    "        full_path = os.path.join(directory, model_file)  # Correctly join paths\n",
    "        models[model_file] = torch.load(full_path, map_location=map_location)\n",
    "    return models\n",
    "\n",
    "def verify_models_in_destination(model_files, destination_directory):\n",
    "    \"\"\"\n",
    "    Verify the presence of model files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to verify.\n",
    "    destination_directory (str): The directory where the models are supposed to be.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all models are found in the directory, False otherwise.\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    for model_file in model_files:\n",
    "        # Construct the full path to where the model should be\n",
    "        full_path = os.path.join(destination_directory, model_file)\n",
    "        # Check if the model exists at the location\n",
    "        if not os.path.exists(full_path):\n",
    "            missing_files.append(model_file)\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"Missing model files in destination: {missing_files}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"All models are correctly located in the destination directory.\")\n",
    "        return True\n",
    "\n",
    "# URLs and checksums for the models\n",
    "models_info = {\n",
    "    'location_model.pt': ('https://osf.io/zmd7y/download', 'dfd51cf7c3a277777ad941c4fcc23813'),\n",
    "    'stroke_model.pt': ('https://osf.io/m6yc7/download', '511ea7bd12566245d5d11a85d5a0abb0'),\n",
    "    'terminate_model.pt': ('https://osf.io/dsmhc/download', '2f3e26cfcf36ce9f9172c15d8b1079d1')\n",
    "}\n",
    "\n",
    "destination_directory = base_dir\n",
    "\n",
    "# Define model_files based on the keys of models_info to ensure we have the filenames\n",
    "model_files = list(models_info.keys())\n",
    "\n",
    "# Iterate over the models to download and verify\n",
    "for model_name, (url, checksum) in models_info.items():\n",
    "    download_file(url, model_name)  # Downloads directly into base_dir\n",
    "    if verify_checksum(os.path.join(base_dir, model_name), checksum):\n",
    "        print(f\"Successfully verified {model_name}\")\n",
    "    else:\n",
    "        print(f\"Checksum does not match for {model_name}. Download might be corrupted.\")\n",
    "\n",
    "# Verify the presence of the models in the destination directory\n",
    "if verify_models_in_destination(model_files, destination_directory):\n",
    "    print(\"Verification successful: All models are in the correct directory.\")\n",
    "else:\n",
    "    print(\"Verification failed: Some models are missing from the destination directory.\")\n",
    "\n",
    "# Load the models from the destination directory\n",
    "models = load_models(model_files, destination_directory, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3609707-5107-4b50-8b0a-a607890adadb",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def check_float_tesnor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def drawings_to_cpu(drawings):\n",
    "    if isinstance(drawings[0], list):\n",
    "        if drawings[0][0].is_cuda:\n",
    "            drawings = [[stk.cpu() for stk in drawing] for drawing in drawings]\n",
    "    elif drawings[0].is_cuda:\n",
    "        drawings = [stk.cpu() for stk in drawings]\n",
    "    return drawings\n",
    "\n",
    "def broaden_filter(a, b, device=None):\n",
    "    H = b*torch.tensor(\n",
    "        [[a/12, a/6, a/12],\n",
    "         [a/6, 1-a, a/6],\n",
    "         [a/12, a/6, a/12]],\n",
    "        dtype=torch.get_default_dtype(),\n",
    "        device=device\n",
    "    )\n",
    "    H = H[None, None]\n",
    "    return H\n",
    "\n",
    "def blur_filter(fsize, sigma, device=None):\n",
    "    H = fspecial(fsize, sigma, ftype='gaussian', device=device)\n",
    "    H = H[None,None]\n",
    "    return H\n",
    "\n",
    "def check_float_tensor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def select_random_images_within_alphabet(base_path, alphabet_path, exclude_character_path, num_images=8):\n",
    "    chosen_images = []\n",
    "    all_characters = [char for char in os.listdir(alphabet_path) if os.path.isdir(os.path.join(alphabet_path, char)) and os.path.join(alphabet_path, char) != exclude_character_path]\n",
    "    while len(chosen_images) < num_images:\n",
    "        if not all_characters:\n",
    "            break\n",
    "        character = random.choice(all_characters)\n",
    "        character_path = os.path.join(alphabet_path, character)\n",
    "        all_images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        if not all_images:\n",
    "            continue\n",
    "        image_file = random.choice(all_images)\n",
    "        image_path = os.path.join(character_path, image_file)\n",
    "        chosen_images.append(image_path)\n",
    "    return chosen_images\n",
    "\n",
    "def run_trial(base_path, num_trials):\n",
    "    for _ in range(num_trials):\n",
    "        languages = [lang for lang in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, lang))]\n",
    "        selected_language = random.choice(languages)\n",
    "        language_path = os.path.join(base_path, selected_language)\n",
    "        characters = [char for char in os.listdir(language_path) if os.path.isdir(os.path.join(language_path, char))]\n",
    "        selected_character = random.choice(characters)\n",
    "        character_path = os.path.join(language_path, selected_character)\n",
    "        images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        probe_image_path, correct_answer_image_path = random.sample(images, 2)\n",
    "        probe_image_path = os.path.join(character_path, probe_image_path)\n",
    "        correct_answer_image_path = os.path.join(character_path, correct_answer_image_path)\n",
    "        wrong_answers = select_random_images_within_alphabet(base_path, language_path, character_path, num_images=8)\n",
    "        options = wrong_answers\n",
    "        options.insert(random.randint(0, len(options)), correct_answer_image_path)\n",
    "        display_images(probe_image_path, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624859f8-1735-4611-918e-29dafc295ab5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 1: Exploring the Omniglot dataset\n",
    "\n",
    "The goal of this section is to interact with a simple widget to perform the categorization task in Lake et al. (2015). We start by loading some data.\n",
    "\n",
    "Your task is to conduct a series of trials to explore the Omniglot dataset, focusing on its structure and diversity of characters. Specifically, the function will randomly selecting languages, characters, and images to create a simple recognition task. Here's a breakdown of what will happen:\n",
    "\n",
    "1. **Navigate through the Omniglot dataset**: the function will start by accessing the Omniglot dataset.\n",
    "\n",
    "2. **Select random characters and images**: for each trial, it'll randomly pick a language, then a character within that language, and finally two images of that character. These images serve as the \"probe\" and \"correct answer\" in your recognition task.\n",
    "\n",
    "3. **Generate wrong answers**: to create a challenging task, it'll also select additional images from the same language but different characters to serve as wrong answers. \n",
    "\n",
    "4. **Display the task**: it'll arrange the correct answer among the wrong ones and present them alongside the probe image. This setup tests your ability to recognize variations of a character amidst similar but incorrect options.\n",
    "\n",
    "5. **Repeat for multiple trials**: this process is repeated for a specified number of trials, each time selecting new characters and images to maintain the challenge's novelty and test the dataset's variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf54d-c1b2-4f10-8f70-a96e12620dee",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = \"data/omniglot-py/images_background\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c3277-7a98-4314-ba05-ca505aa25642",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we interact with the widget. This is a categorization task. You need to match the prompt (surrounded by a red square) with one of the six options based on the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaabfe9-40f0-4b43-992b-c33fdb852b6b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Running the trial\n",
    "run_trial(base_path, num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c68c49-e7e6-4104-9d76-b7bed3cbf084",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reflection activity\n",
    "\n",
    "How do you think you, as a human, are performing a task like Omniglot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47666813-c5dd-45fa-a331-308c14582956",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 2: Building alternative bottom-up parses\n",
    "\n",
    "Now, we build a widget that uses the sample bottom up parsing script to build alternative bottom up parses.\n",
    "\n",
    "In Activity 2, you will use a script for bottom-up parsing to create alternative parses for images. This involves:\n",
    "\n",
    "- **Model score function**: calculates scores for parses by transforming spline representations into strokes, evaluating them on a model, and inversely relating the scores to model losses.\n",
    "\n",
    "- **Collect image results**: organizes parses and their log probabilities in a structured format for analysis or visualization, bypassing the need for disk storage.\n",
    "\n",
    "- **Get base parses in memory**: initializes a model and dataset, then computes top-K parses for each image based on model scoring. It emphasizes the generation of alternative interpretations without minor detail consideration.\n",
    "\n",
    "- **Visualize parses**: displays the reference image alongside alternative parses, illustrating the model's ability to capture diverse interpretations.\n",
    "\n",
    "The process entails scoring parses, collecting data, generating alternative interpretations, and visualizing results to understand the model's generative capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f7499-c4b4-47e6-8dfe-137333c33c7e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_score_fn(model, parses):\n",
    "    drawings = nested_map(lambda x: splines.get_stk_from_bspline(x), parses)\n",
    "    if torch.cuda.is_available():\n",
    "        drawings = nested_map(lambda x: x.cuda(), drawings)\n",
    "        parses = nested_map(lambda x: x.cuda(), parses)\n",
    "    losses = model.losses_fn(parses, drawings, filter_small=False, denormalize=True)\n",
    "    return -losses.cpu()\n",
    "\n",
    "def collect_img_results(img_id, parses, log_probs, reverse):\n",
    "    \"\"\"Collects parses and log probabilities without saving to disk.\"\"\"\n",
    "    appendix = 'test' if reverse else 'train'\n",
    "    data = {\n",
    "        'img_id': img_id,\n",
    "        'appendix': appendix,\n",
    "        'parses': parses,\n",
    "        'log_probs': log_probs,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_base_parses_in_memory(run_id, trials_per=800, reverse=False, dry_run=False):\n",
    "    print('run_id: %i' % run_id)\n",
    "    print('Loading model...')\n",
    "    # Correctly identify the base directory of the `gns` package\n",
    "    gns_base_dir = os.path.dirname(gns.__file__)\n",
    "\n",
    "    # Specify the model save directory within the `gns` package\n",
    "    model_save_path = os.path.join(gns_base_dir, 'model_saves')\n",
    "\n",
    "    # Initialize TypeModel\n",
    "    type_model = TypeModel().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        type_model = type_model.cuda()\n",
    "    score_fn = lambda parses: model_score_fn(type_model, parses)\n",
    "\n",
    "    # Use the dynamically determined path\n",
    "    osc_path = os.path.join(os.getcwd(), 'data/one-shot-classification')\n",
    "    print('Loading classification dataset...')\n",
    "    dataset = ClassificationDataset(osc_folder=osc_path)\n",
    "    run = dataset.runs[run_id]\n",
    "    imgs = run.test_imgs if reverse else run.train_imgs\n",
    "\n",
    "    collected_data = []  # In-memory storage for parses and log_probs\n",
    "\n",
    "    print('Collecting top-K parses for each train image...')\n",
    "    nimg = len(imgs)\n",
    "    for i in range(nimg):\n",
    "        start_time = time.time()\n",
    "        parses, log_probs = get_topK_parses(\n",
    "            imgs[i], k=5, score_fn=score_fn, configs_per=1,\n",
    "            trials_per=trials_per)\n",
    "        total_time = time.time() - start_time\n",
    "        print(f'image {i+1}/{nimg} took {time_string(total_time)}')\n",
    "        if dry_run:\n",
    "            continue\n",
    "        img_results = collect_img_results(i, parses, log_probs, reverse)\n",
    "        collected_data.append(img_results)\n",
    "\n",
    "    return collected_data\n",
    "\n",
    "# Example usage\n",
    "run_id = 10\n",
    "trials_per = 800\n",
    "reverse = False\n",
    "dry_run = False\n",
    "\n",
    "# Call the function\n",
    "collected_data = get_base_parses_in_memory(run_id=run_id, trials_per=trials_per, reverse=reverse, dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c406-e512-466e-b8ff-34acda11fc86",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Use retina mode for matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Function to visualize parses\n",
    "def visualize_parses(renderer, collected_data, item, run):\n",
    "    # Load the classification labels to map test images to their corresponding training images\n",
    "    with open(f'./data/one-shot-classification/all_runs/run{run+1:02}/class_labels.txt') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    test_to_train = {}\n",
    "    for line in data:\n",
    "        left, right = line.split(' ')\n",
    "        test_to_train[left.split('/')[-1]] = right.split('/')[-1].strip()\n",
    "\n",
    "    plt.figure(figsize=(12.5, 2.5))\n",
    "    train_im = test_to_train[f'item{item:02}.png']\n",
    "\n",
    "    # Reference image visualization\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(1 - plt.imread(f'./data/one-shot-classification/all_runs/run{run+1:02}/training/{train_im}'), cmap='gray')\n",
    "    plt.title('Reference image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Load parses from the collected data\n",
    "    for img_data in collected_data:\n",
    "        if img_data['img_id'] == item - 1:\n",
    "            parses = img_data['parses']\n",
    "            break\n",
    "\n",
    "    r = renderer()\n",
    "    # Visualization of parses\n",
    "    for i, parse in enumerate(parses):\n",
    "        drawings = [splines.get_stk_from_bspline(x) for x in parse]\n",
    "        plt.subplot(1, 6, i+2)\n",
    "        plt.imshow(r(drawings), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Parse {i+1}')\n",
    "\n",
    "        colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
    "        for i, d in enumerate(drawings):\n",
    "            plt.plot(d[:, 0], -d[:, 1], colors[i], linewidth=1)\n",
    "            plt.plot(d[0, 0], -d[0, 1], 'w.', markersize=20, fillstyle='none')\n",
    "            plt.plot(d[-1, 0], -d[-1, 1], 'ws', markersize=10, fillstyle='none')\n",
    "\n",
    "# Example of visualizing parses for a specific item and run\n",
    "visualize_parses(DefaultRenderer, collected_data, item=9, run=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e452c-58ff-4830-9b6a-282313aceca6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 3: Building a single stroke model\n",
    "\n",
    "Build parts of the forward model necessary to build a differentiable renderer for a single stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cfe54b-3c21-4a10-a5ac-44d98ce43e15",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In activity 3, you are tasked with constructing the forward model components necessary for a differentiable renderer focused on rendering single strokes. Parameters that will govern the rendering process are already initialized - parameters that define the image size and characteristics of ink distribution on the canvas, spline parameters for trajectory creation, image modeling parameters to manage noise, MCMC parameters for simulation steps, and search parameters for algorithmic inference.\n",
    "\n",
    "Your primary goal is to develop the `Renderer` and `Painter` classes. The `BroadenAndBlur` module applies broadening to simulate ink spread and blurring to simulate its interaction with paper, controlled by specified parameters. The `Renderer` class uses these effects to process the strokes, while the `Painter` class is responsible for converting vector stroke representations into rasterized images, managing ink distribution based on stroke trajectory, and ensuring that strokes adhere to canvas bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6c02b-77a4-4676-b1d7-fc42f1150716",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Parameters\n",
    "# @markdown\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # Library to use\n",
    "        self.libname = 'library'\n",
    "        self.set_rendering_params()\n",
    "        self.set_spline_params()\n",
    "        self.set_image_model_params()\n",
    "        self.set_mcmc_params()\n",
    "        self.set_search_params()\n",
    "\n",
    "    def set_rendering_params(self):\n",
    "        self.imsize = torch.Size([105, 105]) # image size\n",
    "\n",
    "        ## ink-add parameters\n",
    "        self.ink_pp = 2. # amount of ink per point\n",
    "        self.ink_max_dist = 2. # distance between points to which you get full ink\n",
    "\n",
    "        ## broadening parameters\n",
    "        self.ink_ncon = 2 # number of convolutions\n",
    "        self.ink_a = 0.5 # parameter 1\n",
    "        self.ink_b = 6. # parameter 2\n",
    "        self.broaden_mode = 'Lake' # broadening version (must be either \"Lake\" or \"Hinton\")\n",
    "\n",
    "        ## blurring parameters\n",
    "        self.fsize = 11 # convolution size for blurring\n",
    "\n",
    "    def set_spline_params(self):\n",
    "        \"\"\"\n",
    "        Parameters for creating a trajectory from a spline\n",
    "        \"\"\"\n",
    "        self.spline_max_neval = 200 # maxmium number of evaluations\n",
    "        self.spline_min_neval = 10 # minimum number of evaluations\n",
    "        self.spline_grain = 1.5 # 1 trajectory point for every this many units pixel distance\n",
    "\n",
    "    def set_image_model_params(self):\n",
    "        \"\"\"\n",
    "        Max/min noise parameters for image model\n",
    "        \"\"\"\n",
    "        self.max_blur_sigma = torch.tensor(16, dtype=torch.float) # min/max blur sigma\n",
    "        self.min_blur_sigma = torch.tensor(0.5, dtype=torch.float) # min/max blur sigma\n",
    "        self.max_epsilon = torch.tensor(0.5, dtype=torch.float) # min/max pixel epsilon\n",
    "        self.min_epsilon = torch.tensor(1e-4, dtype=torch.float) # min/max pixel epsilon\n",
    "\n",
    "    def set_mcmc_params(self):\n",
    "        \"\"\"\n",
    "        MCMC parameters\n",
    "        \"\"\"\n",
    "        ## chain parameters\n",
    "        self.mcmc_nsamp_type_chain = 200 # number of samples to take in the MCMC chain (for classif.)\n",
    "        self.mcmc_nsamp_type_store = 10 # number of samples to store from this chain (for classif.)\n",
    "        self.mcmc_nsamp_token_chain = 25 # for completion (we take last sample in this chain)\n",
    "\n",
    "        ## mcmc proposal parameters\n",
    "        self.mcmc_prop_gpos_sd = 1 # global position move\n",
    "        self.mcmc_prop_shape_sd = 3/2 # shape move\n",
    "        self.mcmc_prop_scale_sd = 0.0235 # scale move\n",
    "        self.mcmc_prop_relmid_sd = 0.2168 # attach relation move\n",
    "        self.mcmc_prop_relpos_mlty = 2 # multiply the sd of the standard position noise by this to propose new positions from prior\n",
    "\n",
    "    def set_search_params(self):\n",
    "        \"\"\"\n",
    "        Parameters of search algorithm (part of inference)\n",
    "        \"\"\"\n",
    "        self.K = 5 # number of particles to use in search algorithm\n",
    "        self.max_affine_scale_change = 2 # scale changes must be less than a factor of 2\n",
    "        self.max_affine_shift_change = 50 # shift changes must less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82efd4d-06eb-43ce-a67d-0f1d1afb73e1",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Broaden and Blur\n",
    "# @markdown\n",
    "\n",
    "class BroadenAndBlur(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        if blur_fsize is None:\n",
    "            blur_fsize = PM.fsize\n",
    "        assert blur_fsize % 2 == 1, 'blur conv filter size must be odd'\n",
    "        self.register_buffer('H_broaden', broaden_filter(PM.ink_a, PM.ink_b))\n",
    "        self.register_buffer('H_blur', blur_filter(blur_fsize, blur_sigma))\n",
    "        self.nbroad = PM.ink_ncon\n",
    "        self.blur_pad = blur_fsize//2\n",
    "        self.blur_sigma = blur_sigma\n",
    "        self.blur_fsize = blur_fsize\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.H_broaden.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.H_broaden.is_cuda\n",
    "\n",
    "    def forward(self, x, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] pre-conv image probabilities\n",
    "        blur_sigma : float | None\n",
    "            amount of blur. 'None' means use value from __init__ call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] post-conv image probabilities\n",
    "        \"\"\"\n",
    "        if self.is_cuda:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if blur_sigma is None:\n",
    "            H_blur = self.H_blur\n",
    "            blur_sigma = self.blur_sigma\n",
    "        else:\n",
    "            blur_sigma = check_float_tesnor(blur_sigma, self.device)\n",
    "            H_blur = blur_filter(self.blur_fsize, blur_sigma, device=self.device)\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        else:\n",
    "            epsilon = check_float_tesnor(epsilon, self.device)\n",
    "\n",
    "        # unsqueeze\n",
    "        x = x.unsqueeze(1)\n",
    "        # apply broaden\n",
    "        for i in range(self.nbroad):\n",
    "            x = F.conv2d(x, self.H_broaden, padding=1)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # return if no blur\n",
    "        if blur_sigma == 0:\n",
    "            x = x.squeeze(1)\n",
    "            return x\n",
    "        # apply blur\n",
    "        for i in range(2):\n",
    "            x = F.conv2d(x, H_blur, padding=self.blur_pad)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # apply pixel noise\n",
    "        if epsilon > 0:\n",
    "            x = (1-epsilon)*x + epsilon*(1-x)\n",
    "        # squeeze\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9321991-a37a-4173-bab0-44175b7e13ba",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Hidden methods for Renderer and Painter\n",
    "# @markdown\n",
    "\n",
    "def cuda(self, device=None):\n",
    "    self.painter = self.painter.cpu()\n",
    "    self.broaden_and_blur = self.broaden_and_blur.cuda(device)\n",
    "    return self\n",
    "\n",
    "def forward_partial_r(self, drawing, blur_sigma=None, epsilon=None, concat=False):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    if isinstance(drawing[0], list):\n",
    "        pimgs = [self.painter.forward_partial(d) for d in drawing]\n",
    "        lengths = [len(p) for p in pimgs]\n",
    "        pimgs = torch.cat(pimgs)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        if concat:\n",
    "            return pimgs\n",
    "        pimgs = torch.split(pimgs, lengths, 0)\n",
    "        return list(pimgs)\n",
    "    else:\n",
    "        pimgs = self.painter.forward_partial(drawing)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        return pimgs\n",
    "\n",
    "def forward_partial_p(self, drawing):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    assert not self.is_cuda\n",
    "    drawing = drawings_to_cpu(drawing)\n",
    "    ns = len(drawing)\n",
    "    pimgs = torch.zeros(ns+1, *self.imsize)\n",
    "    canvas = torch.zeros(*self.imsize)\n",
    "    for i, stk in enumerate(drawing):\n",
    "        canvas, _ = self.add_stroke(canvas, stk)\n",
    "        pimgs[i+1] = canvas\n",
    "    return pimgs\n",
    "\n",
    "def check_bounds(self, myt):\n",
    "    xt = myt[:,0]\n",
    "    yt = myt[:,1]\n",
    "    x_out = (torch.floor(xt) < 0) | (torch.ceil(xt) >= self.imsize[0])\n",
    "    y_out = (torch.floor(yt) < 0) | (torch.ceil(yt) >= self.imsize[1])\n",
    "    out = x_out | y_out\n",
    "    return out\n",
    "\n",
    "def seqadd(self, D, lind_x, lind_y, inkval):\n",
    "    lind = self.index_mat[lind_x.long(), lind_y.long()]\n",
    "    D = D.view(-1)\n",
    "    D = D.scatter_add(0, lind, inkval)\n",
    "    D = D.view(self.imsize)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae763d6-dc78-4e5e-9ae9-233dd53d384b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        pimgs = ...\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = ...\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            pimgs[i] = ...\n",
    "        return pimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60529a18-f91c-446a-8926-70e16ec7ed8c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to remove solution\n",
    "\n",
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        pimgs = self.painter(drawings)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = self.add_stroke(pimg, stk)\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            pimgs[i] = self.draw(pimgs[i], drawings[i])\n",
    "        return pimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d88b1-0e54-44bd-b67a-4b032fbf727b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Example of visualizing parses for a specific item and run\n",
    "\n",
    "# Add hidden methods (otherwise code would have been too long)\n",
    "Renderer.cuda = cuda\n",
    "Renderer.forward_partial_r = forward_partial_r\n",
    "Painter.forward_partial_p = forward_partial_p\n",
    "Painter.check_bounds = check_bounds\n",
    "Painter.seqadd = seqadd\n",
    "\n",
    "# Now create instances of Renderer and Painter\n",
    "renderer_instance = Renderer()\n",
    "painter_instance = Painter()\n",
    "\n",
    "visualize_parses(Renderer, collected_data, item=9, run=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial3",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
