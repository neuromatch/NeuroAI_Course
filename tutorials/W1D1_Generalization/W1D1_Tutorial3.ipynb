{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195ff9f1-1b9c-4772-853a-f8498926ba63",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/student/W1D1_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/student/W1D1_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e57f5-dcf1-4fea-a607-dc8b812be7eb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 3: Generalization in Cognitive Science\n",
    "\n",
    "**Week 1, Day 1: Generalization**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab73da2-b26e-457e-a1ae-0e7815c47592",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "This tutorial will introduce you to generalization in the context of cognitive science. We'll close the loop of our exploration of different views of handwriting with a model that combines aspects of the models we covered in the neuroscience and AI tutorials, including both generative and discriminative components. In particular, we'll discuss how one cognitive model, [Feinman and Lake](https://arxiv.org/abs/2006.14448) (2020), attempts to solve the problem of handwritten symbol recognition using a neuro-symbolic method. We'll introduce one-shot learning in the context of cognitive science and discuss how that relates to generalization. We'll talk about the Omniglot dataset, and how it can be used to infer how humans and machines generalize.\n",
    "\n",
    "Our learning goals for this tutorial are as follows::\n",
    "\n",
    "1. Describe the goals of cognitive science, including the study of decision making, problem solving, learning, thinking, perceiving.\n",
    "\n",
    "2. Learn about what generalization means in the context of cognitive science.\n",
    "\n",
    "3. Contrast historical cognitive approaches. Gain a perspective on historical cognitive approaches, specifically the connectionist and symbolic frameworks, and how symbolic approaches attempt to encode inductive biases.\n",
    "\n",
    "4. Develop coding skills for cognitive modeling. Acquire practical coding experience by working on parts of cognitive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a02ee-d0ae-4923-ad3c-d978b9b4aa46",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51b72e-386a-49ad-94e8-648d62124e1f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac5d8b-5d72-423b-bbb2-975e61668304",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "#!pip install matplotlib numpy Pillow scikit-learn torch torchvision scipy ipywidgets tqdm\n",
    "\n",
    "# Install GNS and pyBPL\n",
    "!pip install numba\n",
    "!pip install scikit-image\n",
    "!pip install git+https://github.com/neuromatch/GNS-Modeling\n",
    "!pip install git+https://github.com/neuromatch/pyBPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2870-3fd6-4a63-8360-36763673490d",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard libraries\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from importlib import reload\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Data handling and visualization\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# Interactive controls in Jupyter notebooks\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Libraries for specific tasks related to gns and pybpl\n",
    "import gns\n",
    "from gns import MODEL_SAVE_PATH\n",
    "from gns.inference.parsing import get_topK_parses\n",
    "from gns.omniglot.classification import ClassificationDataset\n",
    "from gns.rendering import Renderer as DefaultRenderer\n",
    "from gns.type import TypeModel\n",
    "from gns.utils.experiments import mkdir, time_string\n",
    "import pybpl\n",
    "from pybpl import splines, parameters\n",
    "from pybpl.util import nested_map\n",
    "from pybpl.util.stroke import dist_along_traj\n",
    "from pybpl.util.general import fspecial\n",
    "\n",
    "# Utility for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload gns\n",
    "reload(gns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870241d-73fb-455f-a8ef-c92f9b2bdaf3",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452c1b0-d79d-4c17-8738-b0b342180a09",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_images(probe, options):\n",
    "    # Open the probe image and the option images\n",
    "    probe_image = Image.open(probe)\n",
    "    option_images = [Image.open(img_path) for img_path in options]\n",
    "\n",
    "    # Create a figure with the probe and the 3x3 grid for the options directly below\n",
    "    fig = plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "\n",
    "    # Add the probe image to the top of the figure with a red border\n",
    "    ax_probe = fig.add_subplot(4, 3, (1, 3))  # Span the probe across the top 3 columns\n",
    "    ax_probe.imshow(probe_image)\n",
    "    ax_probe.axis('off')\n",
    "    rect = patches.Rectangle((0, 0), probe_image.width-1, probe_image.height-1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax_probe.add_patch(rect)\n",
    "\n",
    "    # Position the 3x3 grid of option images directly below the probe image\n",
    "    for index, img in enumerate(option_images):\n",
    "        row = (index // 3) + 1  # Calculate row in the 3x3 grid, starting directly below the probe\n",
    "        col = (index % 3) + 1   # Calculate column in the 3x3 grid\n",
    "        ax_option = fig.add_subplot(4, 3, row * 3 + col)  # Adjust grid position to directly follow the probe\n",
    "        ax_option.imshow(img)\n",
    "        ax_option.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5deda-c9f0-4733-a01b-3d077a96bafb",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval for zip files\n",
    "# @markdown\n",
    "\n",
    "def handle_file_operations(fname, url, expected_md5, extract_to='data'):\n",
    "    \"\"\"Handles downloading, verifying, and extracting a file.\"\"\"\n",
    "\n",
    "    # Define helper functions for download, verify, and extract operations\n",
    "    def download_file(url, filename):\n",
    "        \"\"\"Downloads file from the given URL and saves it locally.\"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(filename, \"wb\") as fid:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    fid.write(chunk)\n",
    "            print(\"Download successful.\")\n",
    "            return True\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"!!! Failed to download data: {e} !!!\")\n",
    "            return False\n",
    "\n",
    "    def verify_file_md5(filename, expected_md5):\n",
    "        \"\"\"Verifies the file's MD5 checksum.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filename, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        if hash_md5.hexdigest() == expected_md5:\n",
    "            print(\"MD5 checksum verified.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "            return False\n",
    "\n",
    "    def extract_zip_file(filename, extract_to):\n",
    "        \"\"\"Extracts the ZIP file to the specified directory.\"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"File extracted successfully to {extract_to}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"!!! The ZIP file is corrupted or not a zip file !!!\")\n",
    "\n",
    "    # Main operation\n",
    "    if not os.path.isfile(fname) or not verify_file_md5(fname, expected_md5):\n",
    "        if download_file(url, fname) and verify_file_md5(fname, expected_md5):\n",
    "            extract_zip_file(fname, extract_to)\n",
    "    else:\n",
    "        print(f\"File '{fname}' already exists and is verified. Proceeding to extraction.\")\n",
    "        extract_zip_file(fname, extract_to)\n",
    "\n",
    "# Example usage\n",
    "file_info = [\n",
    "    {\"fname\": \"one-shot-classification.zip\", \"url\": \"https://osf.io/aw6eq/download\", \"expected_md5\": \"9376412e7fb74d64f045644b51e641a6\"},\n",
    "    {\"fname\": \"omniglot-py.zip\", \"url\": \"https://osf.io/bazxp/download\", \"expected_md5\": \"f7a4011f5c25460c6d95ee1428e377ed\"},\n",
    "    {\"fname\": \"parses.zip\", \"url\": \"https://osf.io/97bpq/download\", \"expected_md5\": \"f9e42660d860d1f95296a3729e4bd2e3\"},\n",
    "    {\"fname\": \"targets.zip\", \"url\": \"https://osf.io/stk9f/download\", \"expected_md5\": \"38bbf4a87910fe56a02ee7a77a9ded3e\"}\n",
    "]\n",
    "\n",
    "for file in file_info:\n",
    "    handle_file_operations(**file)\n",
    "\n",
    "#Current directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cf0b6-954a-414b-80b9-e356349bc934",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval for torch models\n",
    "# @markdown\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL and save it in the specified directory.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, filename)  # Ensure the file is saved in base_dir\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "\n",
    "def verify_checksum(filename, expected_checksum):\n",
    "    \"\"\"\n",
    "    Verify the MD5 checksum of a file\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the file\n",
    "    expected_checksum (str): Expected MD5 checksum\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the checksum matches, False otherwise\n",
    "    \"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            md5.update(chunk)\n",
    "\n",
    "    return md5.hexdigest() == expected_checksum\n",
    "\n",
    "def load_models(model_files, directory, map_location='cpu'):\n",
    "    \"\"\"\n",
    "    Load multiple models from a specified directory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for model_file in model_files:\n",
    "        full_path = os.path.join(directory, model_file)  # Correctly join paths\n",
    "        models[model_file] = torch.load(full_path, map_location=map_location)\n",
    "    return models\n",
    "\n",
    "def verify_models_in_destination(model_files, destination_directory):\n",
    "    \"\"\"\n",
    "    Verify the presence of model files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to verify.\n",
    "    destination_directory (str): The directory where the models are supposed to be.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all models are found in the directory, False otherwise.\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    for model_file in model_files:\n",
    "        # Construct the full path to where the model should be\n",
    "        full_path = os.path.join(destination_directory, model_file)\n",
    "        # Check if the model exists at the location\n",
    "        if not os.path.exists(full_path):\n",
    "            missing_files.append(model_file)\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"Missing model files in destination: {missing_files}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"All models are correctly located in the destination directory.\")\n",
    "        return True\n",
    "\n",
    "# URLs and checksums for the models\n",
    "models_info = {\n",
    "    'location_model.pt': ('https://osf.io/zmd7y/download', 'dfd51cf7c3a277777ad941c4fcc23813'),\n",
    "    'stroke_model.pt': ('https://osf.io/m6yc7/download', '511ea7bd12566245d5d11a85d5a0abb0'),\n",
    "    'terminate_model.pt': ('https://osf.io/dsmhc/download', '2f3e26cfcf36ce9f9172c15d8b1079d1')\n",
    "}\n",
    "\n",
    "destination_directory = base_dir\n",
    "\n",
    "# Define model_files based on the keys of models_info to ensure we have the filenames\n",
    "model_files = list(models_info.keys())\n",
    "\n",
    "# Iterate over the models to download and verify\n",
    "for model_name, (url, checksum) in models_info.items():\n",
    "    download_file(url, model_name)  # Downloads directly into base_dir\n",
    "    if verify_checksum(os.path.join(base_dir, model_name), checksum):\n",
    "        print(f\"Successfully verified {model_name}\")\n",
    "    else:\n",
    "        print(f\"Checksum does not match for {model_name}. Download might be corrupted.\")\n",
    "\n",
    "# Verify the presence of the models in the destination directory\n",
    "if verify_models_in_destination(model_files, destination_directory):\n",
    "    print(\"Verification successful: All models are in the correct directory.\")\n",
    "else:\n",
    "    print(\"Verification failed: Some models are missing from the destination directory.\")\n",
    "\n",
    "# Load the models from the destination directory\n",
    "models = load_models(model_files, destination_directory, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3609707-5107-4b50-8b0a-a607890adadb",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def check_float_tesnor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def drawings_to_cpu(drawings):\n",
    "    if isinstance(drawings[0], list):\n",
    "        if drawings[0][0].is_cuda:\n",
    "            drawings = [[stk.cpu() for stk in drawing] for drawing in drawings]\n",
    "    elif drawings[0].is_cuda:\n",
    "        drawings = [stk.cpu() for stk in drawings]\n",
    "    return drawings\n",
    "\n",
    "def broaden_filter(a, b, device=None):\n",
    "    H = b*torch.tensor(\n",
    "        [[a/12, a/6, a/12],\n",
    "         [a/6, 1-a, a/6],\n",
    "         [a/12, a/6, a/12]],\n",
    "        dtype=torch.get_default_dtype(),\n",
    "        device=device\n",
    "    )\n",
    "    H = H[None, None]\n",
    "    return H\n",
    "\n",
    "def blur_filter(fsize, sigma, device=None):\n",
    "    H = fspecial(fsize, sigma, ftype='gaussian', device=device)\n",
    "    H = H[None,None]\n",
    "    return H\n",
    "\n",
    "def check_float_tensor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def select_random_images_within_alphabet(base_path, alphabet_path, exclude_character_path, num_images=8):\n",
    "    chosen_images = []\n",
    "    all_characters = [char for char in os.listdir(alphabet_path) if os.path.isdir(os.path.join(alphabet_path, char)) and os.path.join(alphabet_path, char) != exclude_character_path]\n",
    "    while len(chosen_images) < num_images:\n",
    "        if not all_characters:\n",
    "            break\n",
    "        character = random.choice(all_characters)\n",
    "        character_path = os.path.join(alphabet_path, character)\n",
    "        all_images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        if not all_images:\n",
    "            continue\n",
    "        image_file = random.choice(all_images)\n",
    "        image_path = os.path.join(character_path, image_file)\n",
    "        chosen_images.append(image_path)\n",
    "    return chosen_images\n",
    "\n",
    "def run_trial(base_path, num_trials):\n",
    "    for _ in range(num_trials):\n",
    "        languages = [lang for lang in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, lang))]\n",
    "        selected_language = random.choice(languages)\n",
    "        language_path = os.path.join(base_path, selected_language)\n",
    "        characters = [char for char in os.listdir(language_path) if os.path.isdir(os.path.join(language_path, char))]\n",
    "        selected_character = random.choice(characters)\n",
    "        character_path = os.path.join(language_path, selected_character)\n",
    "        images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        probe_image_path, correct_answer_image_path = random.sample(images, 2)\n",
    "        probe_image_path = os.path.join(character_path, probe_image_path)\n",
    "        correct_answer_image_path = os.path.join(character_path, correct_answer_image_path)\n",
    "        wrong_answers = select_random_images_within_alphabet(base_path, language_path, character_path, num_images=8)\n",
    "        options = wrong_answers\n",
    "        options.insert(random.randint(0, len(options)), correct_answer_image_path)\n",
    "        display_images(probe_image_path, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e22650",
   "metadata": {},
   "source": [
    "# Section 1: How people recognize new characters\n",
    "\n",
    "Let's put ourselves in the mindset of a cognitive scientist studying handwriting. We're interested in how people learn to recognize new characters. Indeed, humans display low sample complexity when it comes to learning new visual concepts: they seem to grasp new concepts with very few presentations. We'd like to come up with a cognitive model that demonstrates how a human can learn from a small number of example characters and generalize to new characters. In AI, learning from $k$ examples is often referred to as a $k$-shot learning; few-shot learning and one-shot learning are related concepts.\n",
    "\n",
    "A good dataset to investigate these issues is the Omniglot dataset. Omniglot has sometimes been described as *MNIST, transposed*. Instead of thousands of examples from 10 digit classes, Omniglot consists of 20 instances from 1623 character classes. These character classes are sourced from 50 alphabets, both natural (e.g. Cherokee or Greek) and constructed (e.g. the alien alphabet from the TV show Futurama). \n",
    "\n",
    "![Sample characters from the Omniglot dataset](https://github.com/brendenlake/omniglot/raw/master/omniglot_grid.jpg)\n",
    "\n",
    "To convince ourselves that humans can indeed learn quickly from examples, let's explore parts of the Omniglot dataset and see if we can successfully perform one-shot classification over unfamiliar characters. Observing human behavior in the lab to infer their strategies is an important way that cognitive scientists make progress in understanding human cognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e800e6-635c-4eba-b785-92bde5d6f770",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 1: Exploring the Omniglot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abf9d8-56b5-4cbb-8858-80ed62d7289a",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Generalization in Cognitive Science\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# video_ids = [('Youtube', '<video_id_1>'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "# tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "# tabs = widgets.Tab()\n",
    "# tabs.children = tab_contents\n",
    "# for i in range(len(tab_contents)):\n",
    "#   tabs.set_title(i, video_ids[i][0])\n",
    "# display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624859f8-1735-4611-918e-29dafc295ab5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The goal of this section is to interact with a simple widget to perform the categorization task in Lake et al. (2015). We start by loading some data.\n",
    "\n",
    "Your task is to conduct a series of trials to explore the Omniglot dataset, focusing on its structure and diversity of characters. Specifically, the function will randomly selecting languages, characters, and images to create a simple recognition task. Here's a breakdown of what will happen:\n",
    "\n",
    "1. **Navigate through the Omniglot dataset**: the function will start by accessing the Omniglot dataset.\n",
    "\n",
    "2. **Select random characters and images**: for each trial, it'll randomly pick a language, then a character within that language, and finally two images of that character. These images serve as the \"probe\" and \"correct answer\" in your recognition task.\n",
    "\n",
    "3. **Generate wrong answers**: to create a challenging task, it'll also select additional images from the same language but different characters to serve as wrong answers. \n",
    "\n",
    "4. **Display the task**: it'll arrange the correct answer among the wrong ones and present them alongside the probe image. This setup tests your ability to recognize variations of a character amidst similar but incorrect options.\n",
    "\n",
    "5. **Repeat for multiple trials**: this process is repeated for a specified number of trials, each time selecting new characters and images to maintain the challenge's novelty and test the dataset's variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf54d-c1b2-4f10-8f70-a96e12620dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = \"data/omniglot-py/images_background\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c3277-7a98-4314-ba05-ca505aa25642",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we interact with the widget. This is a categorization task. You need to match the prompt (surrounded by a red square) with one of the six options based on the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaabfe9-40f0-4b43-992b-c33fdb852b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the trial\n",
    "run_trial(base_path, num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cd6ae-f818-42dc-87f5-78294926f172",
   "metadata": {},
   "source": [
    "Were you able to identify the matching images from the examples? How easy or how hard was the task? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c68c49-e7e6-4104-9d76-b7bed3cbf084",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reflection activity\n",
    "\n",
    "How do you think you, as a human, are performing a task like Omniglot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f01bd9-8016-4ddf-9009-981f4e0dbf4d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_Omniglot_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901a1ca-e1f3-4b3d-8686-ad36a6876953",
   "metadata": {},
   "source": [
    "# Section 2: Building a model of one-shot learning\n",
    "\n",
    "Feinman and Lake (2020) propose a cognitive model to explain how humans perform one-shot recognition tasks like Omniglot. Their model is based on the insight that the generative model for handwriting characters is highly structured: if we could reverse that generative model, we could figure out the intent of the writer. This allows us to perform one-shot recognition of characters. \n",
    "\n",
    "When we write down a character on a piece of paper or a screen, we might implicitly perform a sequence of steps:\n",
    "\n",
    "0. Prepare a global motor plan to write a character based on prior experience\n",
    "1. Decide where to put down the pen for the first stroke\n",
    "2. Draw a stroke in an appropriate direction.\n",
    "   a. Look at the sheet of paper during the writing to adjust the direction of the stroke\n",
    "3. Find a location for the second strike, and so on...\n",
    "4. When satisfied, stop drawing strokes\n",
    "\n",
    "Feinman and Lake (2020) propose to embed these assumptions into a generative model for how a single character is generated from strokes.\n",
    "\n",
    "![Model parts](https://github.com/neuromatch/NeuroAI_Course/raw/main/tutorials/W1D1_Generalization/static/model_parts.png)\n",
    "\n",
    "The result is a highly structured Bayesian generative model containing both discrete components (e.g. strokes) and continuous components (e.g. the location of the next stroke is a continuous variable). It combines a structured generative model over symbol primitives (strokes) as well as standard ANN components, e.g. a CNN that looks at the currently generated image to decide where to draw the next stroke. This combination of using symbols and neural networks is known as a **neuro-symbolic** approach.\n",
    "\n",
    "Different capabilities, including new unconditioned and conditioned character generation, as well as one-shot learning, can be recovered by Bayesian inference. This flexibility does comes at a significant price: we often have to design custom methods to marginalize, condition and sample from complex distributions. We'll dive deeper into some of these components in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47666813-c5dd-45fa-a331-308c14582956",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 2: Building alternative bottom-up parses\n",
    "\n",
    "The first step in recognizing a new character via a neuro-symbolic approach is to perform  multiple bottom-up parses to generate potential ways of chopping up a visible image into multiple strokes.\n",
    "\n",
    "This involves:\n",
    "\n",
    "- **Model score function**: calculates scores for parses by transforming spline representations into strokes, evaluating them on a model, and inversely relating the scores to model losses.\n",
    "\n",
    "- **Collect image results**: organizes parses and their log probabilities in a structured format for analysis or visualization, bypassing the need for disk storage.\n",
    "\n",
    "- **Get base parses in memory**: initializes a model and dataset, then computes top-K parses for each image based on model scoring. \n",
    "\n",
    "- **Visualize parses**: displays the reference image alongside alternative parses, illustrating the model's ability to capture diverse interpretations.\n",
    "\n",
    "The process entails scoring parses, collecting data, generating alternative interpretations, and visualizing results to understand the model's generative capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f7499-c4b4-47e6-8dfe-137333c33c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_score_fn(model, parses):\n",
    "    drawings = nested_map(lambda x: splines.get_stk_from_bspline(x), parses)\n",
    "    if torch.cuda.is_available():\n",
    "        drawings = nested_map(lambda x: x.cuda(), drawings)\n",
    "        parses = nested_map(lambda x: x.cuda(), parses)\n",
    "    losses = model.losses_fn(parses, drawings, filter_small=False, denormalize=True)\n",
    "    return -losses.cpu()\n",
    "\n",
    "def collect_img_results(img_id, parses, log_probs, reverse):\n",
    "    \"\"\"Collects parses and log probabilities without saving to disk.\"\"\"\n",
    "    appendix = 'test' if reverse else 'train'\n",
    "    data = {\n",
    "        'img_id': img_id,\n",
    "        'appendix': appendix,\n",
    "        'parses': parses,\n",
    "        'log_probs': log_probs,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_base_parses(run_id, item, trials_per=800, reverse=False, dry_run=False):\n",
    "    print('run_id: %i' % run_id)\n",
    "    print('Loading model...')\n",
    "    # Correctly identify the base directory of the `gns` package\n",
    "    gns_base_dir = os.path.dirname(gns.__file__)\n",
    "\n",
    "    # Specify the model save directory within the `gns` package\n",
    "    model_save_path = os.path.join(gns_base_dir, 'model_saves')\n",
    "\n",
    "    # Initialize TypeModel\n",
    "    type_model = TypeModel().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        type_model = type_model.cuda()\n",
    "    score_fn = lambda parses: model_score_fn(type_model, parses)\n",
    "\n",
    "    # Use the dynamically determined path\n",
    "    osc_path = os.path.join(os.getcwd(), 'data/one-shot-classification')\n",
    "    print('Loading classification dataset...')\n",
    "    dataset = ClassificationDataset(osc_folder=osc_path)\n",
    "    run = dataset.runs[run_id]\n",
    "    imgs = run.test_imgs if reverse else run.train_imgs\n",
    "\n",
    "    collected_data = []  # In-memory storage for parses and log_probs\n",
    "\n",
    "    print('Collecting top-K parses for each train image...')\n",
    "    i = item\n",
    "    \n",
    "    start_time = time.time()\n",
    "    parses, log_probs = get_topK_parses(\n",
    "        imgs[i], k=5, score_fn=score_fn, configs_per=1,\n",
    "        trials_per=trials_per)\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'image {i+1} took {time_string(total_time)}')\n",
    "    img_results = collect_img_results(i, parses, log_probs, reverse)\n",
    "\n",
    "    return img_results['parses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c406-e512-466e-b8ff-34acda11fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use retina mode for matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Function to visualize parses\n",
    "def visualize_parses(renderer, collected_data, item, run):\n",
    "    # Load the classification labels to map test images to their corresponding training images\n",
    "    with open(f'./data/one-shot-classification/all_runs/run{run+1:02}/class_labels.txt') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # Example usage\n",
    "    run_id = 10\n",
    "    trials_per = 20\n",
    "    reverse = False\n",
    "    dry_run = False\n",
    "\n",
    "    parses = get_base_parses(run_id=run_id, item=item, trials_per=trials_per, reverse=reverse, dry_run=dry_run)\n",
    "\n",
    "    test_to_train = {}\n",
    "    for line in data:\n",
    "        left, right = line.split(' ')\n",
    "        test_to_train[left.split('/')[-1]] = right.split('/')[-1].strip()\n",
    "\n",
    "    plt.figure(figsize=(12.5, 2.5))\n",
    "    train_im = test_to_train[f'item{item+1:02}.png']\n",
    "\n",
    "    # Reference image visualization\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(1 - plt.imread(f'./data/one-shot-classification/all_runs/run{run+1:02}/training/{train_im}'), cmap='gray')\n",
    "    plt.title('Reference image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    r = renderer()\n",
    "    # Visualization of parses\n",
    "    for i, parse in enumerate(parses):\n",
    "        drawings = [splines.get_stk_from_bspline(x) for x in parse]\n",
    "        plt.subplot(1, 6, i+2)\n",
    "        plt.imshow(r(drawings), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Parse {i+1}')\n",
    "\n",
    "        colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
    "        for i, d in enumerate(drawings):\n",
    "            plt.plot(d[:, 0], -d[:, 1], colors[i], linewidth=1)\n",
    "            plt.plot(d[0, 0], -d[0, 1], 'w.', markersize=20, fillstyle='none')\n",
    "            plt.plot(d[-1, 0], -d[-1, 1], 'ws', markersize=10, fillstyle='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac5600-1d6b-4513-a499-8cf64e0740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "@widgets.interact\n",
    "def interactive_visualize(\n",
    "    item = widgets.IntSlider(description=\"Image #\", min=0, max=19, step=1, value=0)):\n",
    "    visualize_parses(DefaultRenderer, collected_data, item=item, run=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83394249-eef5-42c5-9637-9e6a132fcd3d",
   "metadata": {},
   "source": [
    "Thus, each image is decomposed into a set of bottom up candidate parses, each composed of multiple strokes. Each stroke has a start and a direction. These bottom up parses are formed by a combination of classical computer vision techniques, evaluation with a pretrained forward model, and heuristics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49df57-a56c-4cb4-93da-709bd2d4b0d0",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_Visualize_Parses_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d531e-19a3-46d9-b585-987f1aa322ca",
   "metadata": {},
   "source": [
    "# Section 3: Rendering model\n",
    "\n",
    "To do one-shot learning in this model, we will need to evaluate how well the parse of a reference character accounts for the data from a test character. This means adjusting the parameters of a set of strokes to maximize the likelihood of the data. Thus we'll need to backpropropagate through a **renderer**. In deep learning, it's common to backpropagate through fully-connected, convolutional, or recurrent layers to learn model parameters. We can use this same idea to backpropagate through a *rendering layer* that takes the parameters of a stroke and renders them into an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cfe54b-3c21-4a10-a5ac-44d98ce43e15",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In activity 3, you are tasked with constructing the forward model components necessary for a differentiable renderer focused on rendering single strokes. Parameters that will govern the rendering process are already initialized - parameters that define the image size and characteristics of ink distribution on the canvas, spline parameters for trajectory creation, image modeling parameters to manage noise, MCMC parameters for simulation steps, and search parameters for algorithmic inference.\n",
    "\n",
    "Your primary goal is to develop the `Renderer` and `Painter` classes. The `BroadenAndBlur` module applies broadening to simulate ink spread and blurring to simulate its interaction with paper, controlled by specified parameters. The `Renderer` class uses these effects to process the strokes, while the `Painter` class is responsible for converting vector stroke representations into rasterized images, managing ink distribution based on stroke trajectory, and ensuring that strokes adhere to canvas bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6c02b-77a4-4676-b1d7-fc42f1150716",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Parameters\n",
    "# @markdown\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # Library to use\n",
    "        self.libname = 'library'\n",
    "        self.set_rendering_params()\n",
    "        self.set_spline_params()\n",
    "        self.set_image_model_params()\n",
    "        self.set_mcmc_params()\n",
    "        self.set_search_params()\n",
    "\n",
    "    def set_rendering_params(self):\n",
    "        self.imsize = torch.Size([105, 105]) # image size\n",
    "\n",
    "        ## ink-add parameters\n",
    "        self.ink_pp = 2. # amount of ink per point\n",
    "        self.ink_max_dist = 2. # distance between points to which you get full ink\n",
    "\n",
    "        ## broadening parameters\n",
    "        self.ink_ncon = 2 # number of convolutions\n",
    "        self.ink_a = 0.5 # parameter 1\n",
    "        self.ink_b = 6. # parameter 2\n",
    "        self.broaden_mode = 'Lake' # broadening version (must be either \"Lake\" or \"Hinton\")\n",
    "\n",
    "        ## blurring parameters\n",
    "        self.fsize = 11 # convolution size for blurring\n",
    "\n",
    "    def set_spline_params(self):\n",
    "        \"\"\"\n",
    "        Parameters for creating a trajectory from a spline\n",
    "        \"\"\"\n",
    "        self.spline_max_neval = 200 # maxmium number of evaluations\n",
    "        self.spline_min_neval = 10 # minimum number of evaluations\n",
    "        self.spline_grain = 1.5 # 1 trajectory point for every this many units pixel distance\n",
    "\n",
    "    def set_image_model_params(self):\n",
    "        \"\"\"\n",
    "        Max/min noise parameters for image model\n",
    "        \"\"\"\n",
    "        self.max_blur_sigma = torch.tensor(16, dtype=torch.float) # min/max blur sigma\n",
    "        self.min_blur_sigma = torch.tensor(0.5, dtype=torch.float) # min/max blur sigma\n",
    "        self.max_epsilon = torch.tensor(0.5, dtype=torch.float) # min/max pixel epsilon\n",
    "        self.min_epsilon = torch.tensor(1e-4, dtype=torch.float) # min/max pixel epsilon\n",
    "\n",
    "    def set_mcmc_params(self):\n",
    "        \"\"\"\n",
    "        MCMC parameters\n",
    "        \"\"\"\n",
    "        ## chain parameters\n",
    "        self.mcmc_nsamp_type_chain = 200 # number of samples to take in the MCMC chain (for classif.)\n",
    "        self.mcmc_nsamp_type_store = 10 # number of samples to store from this chain (for classif.)\n",
    "        self.mcmc_nsamp_token_chain = 25 # for completion (we take last sample in this chain)\n",
    "\n",
    "        ## mcmc proposal parameters\n",
    "        self.mcmc_prop_gpos_sd = 1 # global position move\n",
    "        self.mcmc_prop_shape_sd = 3/2 # shape move\n",
    "        self.mcmc_prop_scale_sd = 0.0235 # scale move\n",
    "        self.mcmc_prop_relmid_sd = 0.2168 # attach relation move\n",
    "        self.mcmc_prop_relpos_mlty = 2 # multiply the sd of the standard position noise by this to propose new positions from prior\n",
    "\n",
    "    def set_search_params(self):\n",
    "        \"\"\"\n",
    "        Parameters of search algorithm (part of inference)\n",
    "        \"\"\"\n",
    "        self.K = 5 # number of particles to use in search algorithm\n",
    "        self.max_affine_scale_change = 2 # scale changes must be less than a factor of 2\n",
    "        self.max_affine_shift_change = 50 # shift changes must less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82efd4d-06eb-43ce-a67d-0f1d1afb73e1",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Broaden and Blur\n",
    "# @markdown\n",
    "\n",
    "class BroadenAndBlur(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        if blur_fsize is None:\n",
    "            blur_fsize = PM.fsize\n",
    "        assert blur_fsize % 2 == 1, 'blur conv filter size must be odd'\n",
    "        self.register_buffer('H_broaden', broaden_filter(PM.ink_a, PM.ink_b))\n",
    "        self.register_buffer('H_blur', blur_filter(blur_fsize, blur_sigma))\n",
    "        self.nbroad = PM.ink_ncon\n",
    "        self.blur_pad = blur_fsize//2\n",
    "        self.blur_sigma = blur_sigma\n",
    "        self.blur_fsize = blur_fsize\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.H_broaden.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.H_broaden.is_cuda\n",
    "\n",
    "    def forward(self, x, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] pre-conv image probabilities\n",
    "        blur_sigma : float | None\n",
    "            amount of blur. 'None' means use value from __init__ call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] post-conv image probabilities\n",
    "        \"\"\"\n",
    "        if self.is_cuda:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if blur_sigma is None:\n",
    "            H_blur = self.H_blur\n",
    "            blur_sigma = self.blur_sigma\n",
    "        else:\n",
    "            blur_sigma = check_float_tesnor(blur_sigma, self.device)\n",
    "            H_blur = blur_filter(self.blur_fsize, blur_sigma, device=self.device)\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        else:\n",
    "            epsilon = check_float_tesnor(epsilon, self.device)\n",
    "\n",
    "        # unsqueeze\n",
    "        x = x.unsqueeze(1)\n",
    "        # apply broaden\n",
    "        for i in range(self.nbroad):\n",
    "            x = F.conv2d(x, self.H_broaden, padding=1)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # return if no blur\n",
    "        if blur_sigma == 0:\n",
    "            x = x.squeeze(1)\n",
    "            return x\n",
    "        # apply blur\n",
    "        for i in range(2):\n",
    "            x = F.conv2d(x, H_blur, padding=self.blur_pad)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # apply pixel noise\n",
    "        if epsilon > 0:\n",
    "            x = (1-epsilon)*x + epsilon*(1-x)\n",
    "        # squeeze\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9321991-a37a-4173-bab0-44175b7e13ba",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Hidden methods for Renderer and Painter\n",
    "# @markdown\n",
    "\n",
    "def cuda(self, device=None):\n",
    "    self.painter = self.painter.cpu()\n",
    "    self.broaden_and_blur = self.broaden_and_blur.cuda(device)\n",
    "    return self\n",
    "\n",
    "def forward_partial_r(self, drawing, blur_sigma=None, epsilon=None, concat=False):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    if isinstance(drawing[0], list):\n",
    "        pimgs = [self.painter.forward_partial(d) for d in drawing]\n",
    "        lengths = [len(p) for p in pimgs]\n",
    "        pimgs = torch.cat(pimgs)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        if concat:\n",
    "            return pimgs\n",
    "        pimgs = torch.split(pimgs, lengths, 0)\n",
    "        return list(pimgs)\n",
    "    else:\n",
    "        pimgs = self.painter.forward_partial(drawing)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        return pimgs\n",
    "\n",
    "def forward_partial_p(self, drawing):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    assert not self.is_cuda\n",
    "    drawing = drawings_to_cpu(drawing)\n",
    "    ns = len(drawing)\n",
    "    pimgs = torch.zeros(ns+1, *self.imsize)\n",
    "    canvas = torch.zeros(*self.imsize)\n",
    "    for i, stk in enumerate(drawing):\n",
    "        canvas, _ = self.add_stroke(canvas, stk)\n",
    "        pimgs[i+1] = canvas\n",
    "    return pimgs\n",
    "\n",
    "def check_bounds(self, myt):\n",
    "    xt = myt[:,0]\n",
    "    yt = myt[:,1]\n",
    "    x_out = (torch.floor(xt) < 0) | (torch.ceil(xt) >= self.imsize[0])\n",
    "    y_out = (torch.floor(yt) < 0) | (torch.ceil(yt) >= self.imsize[1])\n",
    "    out = x_out | y_out\n",
    "    return out\n",
    "\n",
    "def seqadd(self, D, lind_x, lind_y, inkval):\n",
    "    lind = self.index_mat[lind_x.long(), lind_y.long()]\n",
    "    D = D.view(-1)\n",
    "    D = D.scatter_add(0, lind, inkval)\n",
    "    D = D.view(self.imsize)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae763d6-dc78-4e5e-9ae9-233dd53d384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        # First call the painter to paint each stroke onto a canvas.\n",
    "        pimgs = ...\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = ...\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            # Incrementally add a single stroke to the image.\n",
    "            pimgs[i] = ...\n",
    "        return pimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60529a18-f91c-446a-8926-70e16ec7ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove solution\n",
    "\n",
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        # First call the painter to paint each stroke onto a canvas.\n",
    "        pimgs = self.painter(drawings)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = self.add_stroke(pimg, stk)\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            # Incrementally add a single stroke to the image.\n",
    "            pimgs[i] = self.draw(pimgs[i], drawings[i])\n",
    "        return pimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d88b1-0e54-44bd-b67a-4b032fbf727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of visualizing parses for a specific item and run\n",
    "\n",
    "# Add hidden methods (otherwise code would have been too long)\n",
    "Renderer.cuda = cuda\n",
    "Renderer.forward_partial_r = forward_partial_r\n",
    "Painter.forward_partial_p = forward_partial_p\n",
    "Painter.check_bounds = check_bounds\n",
    "Painter.seqadd = seqadd\n",
    "\n",
    "# Now create instances of Renderer and Painter\n",
    "visualize_parses(Renderer, collected_data, item=9, run=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf7a7c-463e-4d43-bb9d-2cb5e0786023",
   "metadata": {},
   "source": [
    "Thus, images of the characters are built iteratively by adding strokes to a canvas. This operation is done in a way that's compatible with backpropagation, such that we can optimize the location of specific keypoints, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b312c9-c0be-4f2f-99a7-001f4f771721",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_Building_Stroke_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48345a-f9e2-44fb-b302-21bd584cd11a",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "* Cognitive scientists model the cognitive processes of humans\n",
    "* Humans display remarkable low sample complexity when it comes to recognizing characters and display excellent generalization\n",
    "* One way they might do this is by inferring from an image what sequence of strokes led to them, and doing inference on strokes rather than pixels\n",
    "* Neuro-symbolic models can be used to infer complex causal structures from data\n",
    "* Neuro-symbolic models can be complex to implement and computationally intensive, but they elucidate human cognition."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial3",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
