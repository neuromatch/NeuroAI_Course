{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e53498-57e6-477e-8e43-7b8eb1d95882",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/student/W1D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D1_Generalization/student/W1D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207f0be-cbd8-4b93-a823-61af51421e2a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Generalization in AI\n",
    "\n",
    "**Week 1, Day 1: Generalization**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Samuele Bolotta & Patrick Mineault\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Lily Chamakura, RyeongKyung Yoon, Yizhou Chen, Ruiyi Zhang, Aakash Agrawal, Alish Dipani, Hossein Rezaei, Yousef Ghanbari, Mostafa Abdollahi\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d71fc6-0728-41bd-b84e-ac78a622ece4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of the whole tutorial in minutes]*\n",
    "\n",
    "This tutorial will introduce you to generalization in the context of modern AI systems. We'll look at a particular system trained for handwriting recognition–TrOCR. We'll review what makes that model tick–the transformer architecture–and explore what goes on into training and finetuning large-scale models. We'll look at how augmentations can bake in particular inductive biases in transformers. Finally, we'll have a bonus section on scaling laws.\n",
    "\n",
    "Our learning objectives for today are:\n",
    "\n",
    "1. Identify and articulate common objectives pursued by developers of operational AI systems, such as:\n",
    "\n",
    "- OOD robustness; latency; Size, Weight, Power, and Cost (SWaP-C)\n",
    "- Explainability and understanding\n",
    "\n",
    "2. Explain at least three strategies for enhancing the generalization capabilities of AI systems, including the contemporary trend of training generic large-scale models on extensive datasets, commonly referred to as the \"bitter lesson.\"\n",
    "\n",
    "3. Gain practical experience with the fundamentals of deep learning and PyTorch.\n",
    "\n",
    "**Important note**: this tutorial leverages GPU acceleration. Using a GPU runtime in colab will make the the tutorial run 10x faster.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae629d9-c104-4716-8eb9-8e53e5151395",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"x4pa5\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee444563-8ef0-4b20-b8d7-92eeace85488",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a8ed3-dcdd-4c7d-8528-7314520fb187",
   "metadata": {
    "execution": {}
   },
   "source": [
    "##  Install and import feedback gadget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acdfc5-c864-40a0-b648-be385d5c3eb5",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck Pillow matplotlib torch torchvision transformers gradio protobuf sentencepiece gradio torchmetrics --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt - leave this as is\n",
    "        notebook_section,\n",
    "        {\n",
    "        \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "        \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "        \"user_key\": \"y1x3mpx5\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "feedback_prefix = \"W1D1_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40270953",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "# Standard Libraries for file and operating system operations, security, and web requests\n",
    "import os\n",
    "import functools\n",
    "import hashlib\n",
    "import requests\n",
    "import logging\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Core python data science and image processing libraries\n",
    "import numpy as np\n",
    "from PIL import Image as IMG\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# Deep Learning and model specific libraries\n",
    "import torch\n",
    "import torchmetrics.functional.text as fm\n",
    "import transformers\n",
    "from torchvision import transforms\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Utility and interface libraries\n",
    "import gradio as gr\n",
    "from IPython.display import IFrame, display, Image\n",
    "import sentencepiece\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85361588-fbef-4d89-8997-0c4a391a33d9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "##  Figure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa95f5",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e750b29-dea0-4eec-a278-b78e0b715cf4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf34b9a-1dd5-458a-b390-0fa12609d532",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def display_image(image_path):\n",
    "    \"\"\"Display an image from a given file path.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): The path to the image file.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "def display_transformed_images(image, transformations):\n",
    "    \"\"\"\n",
    "    Apply a list of transformations to an image and display them.\n",
    "\n",
    "    Inputs:\n",
    "    - image (Tensor): The input image as a tensor.\n",
    "    - transformations (list): A list of torchvision transformations to apply.\n",
    "    \"\"\"\n",
    "    # Convert tensor image to PIL Image for display\n",
    "    pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "    fig, axs = plt.subplots(len(transformations) + 1, 1, figsize=(5, 15))\n",
    "    axs[0].imshow(pil_image, cmap='gray')\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, transform in enumerate(transformations):\n",
    "        # Apply transformation if it's not the placeholder\n",
    "        if transform != \"Custom ElasticTransform Placeholder\":\n",
    "            transformed_image = transform(image)\n",
    "            # Convert transformed tensor image to PIL Image for display\n",
    "            display_image = transforms.ToPILImage()(transformed_image)\n",
    "            axs[i+1].imshow(display_image, cmap='gray')\n",
    "            axs[i+1].set_title(transform.__class__.__name__)\n",
    "            axs[i+1].axis('off')\n",
    "        else:\n",
    "            axs[i+1].text(0.5, 0.5, 'ElasticTransform Placeholder', ha='center')\n",
    "            axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_original_and_transformed_images(original_tensor, transformed_tensor):\n",
    "    \"\"\"\n",
    "    Display the original and transformed images side by side.\n",
    "\n",
    "    Inputs:\n",
    "    - original_tensor (Tensor): The original image as a tensor.\n",
    "    - transformed_tensor (Tensor): The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display original image\n",
    "    original_image = original_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[0].imshow(original_image, cmap='gray')\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display transformed image\n",
    "    transformed_image = transformed_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[1].imshow(transformed_image, cmap='gray')\n",
    "    axs[1].set_title('Transformed')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_generated_images(generator):\n",
    "    \"\"\"\n",
    "    Display images generated from strings.\n",
    "\n",
    "    Inputs:\n",
    "    - generator (GeneratorFromStrings): A generator that produces images from strings.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, (text_img, lbl) in enumerate(generator, 1):\n",
    "        ax = plt.subplot(1, len(generator.strings) * generator.count // len(generator.strings), i)\n",
    "        plt.imshow(text_img)\n",
    "        plt.title(f\"Example {i}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to generate an image with text\n",
    "def generate_image(text, font_path, space_width=2, skewing_angle=8):\n",
    "    \"\"\"Generate an image with text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to be rendered in the image.\n",
    "        font_path (str): Path to the font file.\n",
    "        space_width (int): Space width between characters.\n",
    "        skewing_angle (int): Angle to skew the text image.\n",
    "    \"\"\"\n",
    "    image_size = (350, 50)\n",
    "    background_color = (255, 255, 255)\n",
    "    speckle_threshold = 0.05\n",
    "    speckle_color = (200, 200, 200)\n",
    "    background = np.random.rand(image_size[1], image_size[0], 1) * 64 + 191\n",
    "    background = np.tile(background, [1, 1, 4])\n",
    "    background[:, :, -1] = 255\n",
    "    image = IMG.fromarray(background.astype('uint8'), 'RGBA')\n",
    "    image2 = IMG.new('RGBA', image_size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(image2)\n",
    "    font = ImageFont.truetype(font_path, size=36)\n",
    "    text_size = draw.textlength(text, font=font)\n",
    "    text_position = ((image_size[0] - text_size) // 2, (image_size[1] - font.size) // 2)\n",
    "    draw.text(text_position, text, font=font, fill=(0, 0, 0), spacing=space_width)\n",
    "    image2 = image2.rotate(skewing_angle)\n",
    "    image.paste(image2, mask=image2)\n",
    "    return image\n",
    "\n",
    "# Function to generate images for multiple strings\n",
    "def image_generator(strings, font_path, space_width=2, skewing_angle=8):\n",
    "    \"\"\"Generate images for multiple strings.\n",
    "\n",
    "    Args:\n",
    "        strings (list): List of strings to generate images for.\n",
    "        font_path (str): Path to the font file.\n",
    "        space_width (int): Space width between characters.\n",
    "        skewing_angle (int): Angle to skew the text image.\n",
    "    \"\"\"\n",
    "    for text in strings:\n",
    "        yield generate_image(text, font_path, space_width, skewing_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79beae9-4343-4655-ad1f-fa7d0225087a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a34ac-fa41-4e90-ab45-82c14384a83e",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "def download_file(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and saves it locally.\n",
    "    Verifies the integrity of the file using an MD5 checksum.\n",
    "\n",
    "    Args:\n",
    "    - fname (str): The local filename/path to save the downloaded file.\n",
    "    - url (str): The URL from which to download the file.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        except (requests.ConnectionError, requests.HTTPError) as e:\n",
    "            print(f\"!!! Failed to download {fname} due to: {str(e)} !!!\")\n",
    "            return\n",
    "        if hashlib.md5(r.content).hexdigest() == expected_md5:\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)\n",
    "            print(f\"{fname} has been downloaded successfully.\")\n",
    "        else:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "\n",
    "def extract_zip(zip_fname, folder='.'):\n",
    "    \"\"\"\n",
    "    Extracts a ZIP file to the specified folder.\n",
    "\n",
    "    Args:\n",
    "    - zip_fname (str): The filename/path of the ZIP file to be extracted.\n",
    "    - folder (str): Destination folder where the ZIP contents will be extracted.\n",
    "    \"\"\"\n",
    "    if zipfile.is_zipfile(zip_fname):\n",
    "        with zipfile.ZipFile(zip_fname, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder)\n",
    "            print(f\"Extracted {zip_fname} to {folder}.\")\n",
    "    else:\n",
    "        print(f\"Skipped extraction for {zip_fname} as it is not a zip file.\")\n",
    "\n",
    "# Define the list of files to download, including both ZIP files and other file types\n",
    "file_info = [\n",
    "    (\"Dancing_Script.zip\", \"https://osf.io/32yed/download\", \"d59bd3201b58a37d0d3b4cd0b0ec7400\", '.'),\n",
    "    (\"lines.zip\", \"https://osf.io/8a753/download\", \"6815ed3987f8eb2fd3bc7678c11f2e9e\", 'lines'),\n",
    "    (\"transcripts.csv\", \"https://osf.io/9hgr8/download\", \"d81d9ade10db55603cc893345debfaa2\", None),\n",
    "    (\"neuroai_hello_world.png\", \"https://osf.io/zg4w5/download\", \"f08b81e47f2fe66b5f25b2ccc204c780\", None)  # New image file\n",
    "]\n",
    "\n",
    "\n",
    "# Process the downloads and extractions\n",
    "for fname, url, expected_md5, folder in file_info:\n",
    "    download_file(fname, url, expected_md5)\n",
    "    if folder is not None:\n",
    "        extract_zip(fname, folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef020325-5f43-4e07-bb44-5a5a4d816219",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Motivation: building a handwriting recognition app with AI\n",
    "\n",
    "Let’s put ourselves into the mindset of an AI developer who wants to build a note app featuring handwriting recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbaa92-2a2d-4e42-a236-0bae90c7f753",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "![Picture which shows W1D1 goal.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/W1D1_goal.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0367ba9-a7ff-49c1-8bd1-0860130e7f39",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Our intrepid goes on HuggingFace and finds a suitable model: [TrOCR](https://huggingface.co/docs/transformers/en/model_doc/trocr)! It's a Transformer-based model that performs Optical Character Recognition and handwriting transcription. Several checkpoints are available, finetuned for different downstream applications like handwriting transcription and printed character recognition. Our relieved developer draws a deep sigh: they don't have to start from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7fdfb-4333-42b0-9906-aa7a54b7bba8",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "![Picture which shows trocr architecture.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/trocr_architecture.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd7a5a-b3cb-4999-a373-3dda956ba77f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this tutorial, we'll look at the design considerations that go into training and deploying a model like TrOCR, what goes on inside the model's transformers, and how it achieves good–or sometimes not-so-good–out-of-distribution generalization. While the NeuroAI course as a whole will explore new ideas at the frontier of neuroscience and AI, we'll first want to understand one of the bread-and-butter building blocks used in industrial AI: the transformer.\n",
    "\n",
    "Let's try out this model ourselves!\n",
    "\n",
    "## Interactive demo 1: TrOCR\n",
    "\n",
    "We load a pretrained TrOCR checkpoint from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00326ca-807d-460f-adf4-767e94bc0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained TrOCR model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model.to(device=device)\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20e1f0-c052-42f5-a33a-87d89548c90c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We write a callback function that calls the preloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1debc6-b3b1-4173-839c-f2a4240efb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to recognize text from an image\n",
    "def recognize_text(processor, model, image):\n",
    "    \"\"\"\n",
    "    This function takes an image as input and uses a pre-trained language model to generate text from the image.\n",
    "\n",
    "    Inputs:\n",
    "    - processor: The processor to use\n",
    "    - model: The model to use\n",
    "    - image (PIL Image or Tensor): The input image containing text to be recognized.\n",
    "\n",
    "    Outputs:\n",
    "    - text (str): The recognized text extracted from the input image.\n",
    "    \"\"\"\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values.to(device))\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88658698-5e50-4740-8b46-087a8417d189",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We build a simple interface in gradio to try out the model interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46ff0-958b-41f1-b00a-2fd0464b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=functools.partial(recognize_text, processor, model),\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Interactive demo: TrOCR\",\n",
    "    description=\"Demo for Microsoft’s TrOCR, an encoder-decoder model for OCR on single-text line images.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e86d56-4d29-4021-87ca-aa6d9d595ef4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Go ahead and try some example text to see how it works. You can use images from the internet, or scan your own handwriting. Just make sure that the text fits on one line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a36a5-a62d-4603-8506-50994c775975",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "![Picture which shows sample 0.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/sample_0.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d061d-87da-492b-8648-b3fed5dcadc6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Picture which shows sample 1.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/sample_1.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3c2a7-7e8f-44f4-be90-ea2ead92de07",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Picture which shows sample 2.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/sample_2.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de5c06-4bdb-40be-8354-5cc3af424f05",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Picture which shows sample 3.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/sample_3.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479e8f3-33ae-4ce5-87f6-a2c34b25152a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "How effective is the model's performance? Does it exhibit generalization beyond its training vocabulary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93083419-5d48-4858-9bd7-979d5238e9d7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 2: Measuring out-of-distribution generalization in TrOCR\n",
    "\n",
    "How well does TrOCR work in practice? Our developer needs to know!\n",
    "\n",
    "Something you will see a lot of in machine learning papers are tables filled with benchmarks. The tables in the [TrOCR official paper](https://arxiv.org/abs/2109.10282) include measures of performance on different benchmark datasets, including IAM, [a handwriting database assembled in 1999](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database). The base and large model variants (334M and 558M parameters) display **character error rates (CER) of 3.42 and 2.89, respectively**.\n",
    "\n",
    "\"Wow!\", our developer thinks, \"That's probably good enough for my notes app! Guess I can go ahead and deploy it\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a422ca8-d407-4207-9921-d18ad82ee3a4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Think! 1\n",
    "\n",
    "What are some reasons why the character error rate measured on IAM might be too optimistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff99d0-b420-40a9-a77f-913d41f7b1d8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding activity 1: measuring out-of-distribution generalization\n",
    "\n",
    "Our developer reads through the fine print in the paper and realizes that the TrOCR is both *trained* on IAM and *tested* on IAM, on a different set of subjects. To be clear, the train and test splits *are* distinct; but samples come from the same underlying distribution. Our developer realizes that the reported error rates might be too optimistic:\n",
    "\n",
    "* IAM was recorded on a tablet. Our developer wants to be able to recognize lines of text handwritten on paper.\n",
    "* IAM is 25 years old. Maybe people write differently now compared to in the past. Do they even write in cursive anymore?\n",
    "* The sentences in IAM are based on a widely published corpus. Maybe TrOCR has memorized that corpus.\n",
    "\n",
    "The more the developer thinks about it, the more they realize that the paper is really estimating *in-distribution generalization*. However, what they care about is how well the model will work when it's deployed *in the wild*, which is closer to **out-of-distribution generalization**.\n",
    "\n",
    "In this coding activity, you'll measure out-of-distribution generalization on a small subset of the CVL database:\n",
    "\n",
    "> Kleber, F., Fiel, S., Diem, M., & Sablatnig, R. (2018). CVL Database - An Off-line Database for Writer Retrieval, Writer Identification and Word Spotting [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1492267\n",
    "\n",
    "Let's first have a look at this new out-of-distribution dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a369a-d65c-401f-8ef5-72a38d6d2eec",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to visualize dataset.\n",
    "def get_images_and_transcripts(df, subject):\n",
    "    df_ = df[df.subject == subject]\n",
    "    transcripts = df_.transcript.values.tolist()\n",
    "\n",
    "    # Load the corresponding images\n",
    "    images = []\n",
    "    for _, row in df_.iterrows():\n",
    "        images.append(IMG.open(row.filename))\n",
    "\n",
    "    return images, transcripts\n",
    "\n",
    "def visualize_images_and_transcripts(images, transcripts):\n",
    "    for img in images:\n",
    "        display(img)\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        print(transcript)\n",
    "\n",
    "df = pd.read_csv('transcripts.csv')\n",
    "df['filename'] = df.apply(lambda x: f\"lines/{x.subject:04}-{x.line}.jpg\", axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9bbd1-5ddf-4851-aa13-b7a9916b4027",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is a small test set with 94 lines sampled from 10 different subjects. Let's have a look at the data from subject 54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64ebb4-2e02-4c20-a889-614fddd4c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, true_transcripts = get_images_and_transcripts(df, 52)\n",
    "visualize_images_and_transcripts(images, true_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c13ca-1409-47a9-a0f2-6026371b855b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The text is transcribed from a passage in the novel [Flatland by Edwin Abbott Abbott](https://en.wikipedia.org/wiki/Flatland). How well does the model recognize the text? Run this cell to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bdfcd-4dab-4864-8f22-fb64d6bcb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_images(all_images, model, processor):\n",
    "    \"\"\"\n",
    "    Transcribe a batch of images using an OCR model.\n",
    "\n",
    "    Args:\n",
    "        all_images: a list of PIL images.\n",
    "        model: the model to do image-to-token ids\n",
    "        processor: the processor which maps token ids to text\n",
    "\n",
    "    Returns:\n",
    "        a list of the transcribed text.\n",
    "    \"\"\"\n",
    "    pixel_values = processor(images=all_images, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values.to(device))\n",
    "    decoded_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded_text\n",
    "\n",
    "transcribed_text = transcribe_images(images, model, processor)\n",
    "print(transcribed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b114a8-4aa5-420e-a490-c3b40b9b66f8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Code exercise 1.1: calculate CER and WER\n",
    "\n",
    "The model is not perfect but it performs far better than chance. Let's measure the character and word error rates on this subject's data. Fill in missing code to measure character and word error rates on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ce371-2074-4258-a6a1-b73a0776a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Clean string prior to comparison\n",
    "\n",
    "    Args:\n",
    "        input_string (str): the input string\n",
    "\n",
    "    Returns:\n",
    "        (str) a cleaned string, lowercase, alphabetical characters only, no double spaces\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert all characters to lowercase\n",
    "    lowercase_string = input_string.lower()\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    alpha_string = re.sub(r'[^a-z\\s]', '', lowercase_string)\n",
    "\n",
    "    # Remove double spaces and start and end spaces\n",
    "    return re.sub(r'\\s+', ' ', alpha_string).strip()\n",
    "\n",
    "\n",
    "def calculate_mismatch(estimated_text, reference_text):\n",
    "    \"\"\"\n",
    "    Calculate mismatch (character and word error rates) between estimated and true text.\n",
    "\n",
    "    Args:\n",
    "        estimated_text: a list of strings\n",
    "        reference_text: a list of strings\n",
    "\n",
    "    Returns:\n",
    "        A tuple, (CER and WER)\n",
    "    \"\"\"\n",
    "    # Lowercase the text and remove special characters for the comparison\n",
    "    estimated_text = [clean_string(x) for x in estimated_text]\n",
    "    reference_text = [clean_string(x) for x in reference_text]\n",
    "\n",
    "    ############################################################\n",
    "    # Fill in this code to calculate character error rate and word error rate.\n",
    "    # Hint: have a look at the torchmetrics documentation for the proper\n",
    "    # metrics.\n",
    "    #\n",
    "    # https://lightning.ai/docs/torchmetrics/stable/\n",
    "    raise NotImplementedError(\"Student has to fill in these lines\")\n",
    "    ############################################################\n",
    "\n",
    "    # Calculate the character error rate and word error rates. They should be\n",
    "    # raw floats, not tensors.\n",
    "    cer = ...\n",
    "    wer = ...\n",
    "    return (cer, wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933f09f-eaa2-42ec-8443-6cf7addb052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Clean string prior to comparison\n",
    "\n",
    "    Args:\n",
    "        input_string (str): the input string\n",
    "\n",
    "    Returns:\n",
    "        (str) a cleaned string, lowercase, alphabetical characters only, no double spaces\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert all characters to lowercase\n",
    "    lowercase_string = input_string.lower()\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    alpha_string = re.sub(r'[^a-z\\s]', '', lowercase_string)\n",
    "\n",
    "    # Remove double spaces and start and end spaces\n",
    "    return re.sub(r'\\s+', ' ', alpha_string).strip()\n",
    "\n",
    "\n",
    "def calculate_mismatch(estimated_text, reference_text):\n",
    "    \"\"\"\n",
    "    Calculate mismatch (character and word error rates) between estimated and true text.\n",
    "\n",
    "    Args:\n",
    "        estimated_text: a list of strings\n",
    "        reference_text: a list of strings\n",
    "\n",
    "    Returns:\n",
    "        A tuple, (CER and WER)\n",
    "    \"\"\"\n",
    "    # Lowercase the text and remove special characters for the comparison\n",
    "    estimated_text = [clean_string(x) for x in estimated_text]\n",
    "    reference_text = [clean_string(x) for x in reference_text]\n",
    "\n",
    "    # Calculate the character error rate and word error rates. They should be\n",
    "    # raw floats, not tensors.\n",
    "    cer = fm.char_error_rate(estimated_text, reference_text).item()\n",
    "    wer = fm.word_error_rate(estimated_text, reference_text).item()\n",
    "    return (cer, wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d82e89-04d2-4fae-a983-b483a6907103",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer, wer = calculate_mismatch(transcribed_text, true_transcripts)\n",
    "assert isinstance(cer, float)\n",
    "cer, wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0116627-1f93-4f00-8cb5-8bac5cbcbb44",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For this particular subject, the character error rate is 3.3%, while the word error rate is 10%. Not bad, and in line with the results in the paper.\n",
    "\n",
    "### Code exercise 1.2: calculate CER and WER across all subjects\n",
    "\n",
    "Let's measure the same metric, this time across all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4be1ee-635e-487f-8708-d5e6287fde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_mismatch(df, model, processor):\n",
    "    \"\"\"\n",
    "    Calculate CER and WER for all subjects in a dataset\n",
    "\n",
    "    Args:\n",
    "        df: a dataframe containing information about images and transcripts\n",
    "        model: an image-to-text model\n",
    "        processor: a processor object\n",
    "\n",
    "    Returns:\n",
    "        a list of dictionaries containing a per-subject breakdown of the\n",
    "        results\n",
    "    \"\"\"\n",
    "    subjects = df.subject.unique().tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Calculate CER and WER for all subjects\n",
    "    for subject in tqdm.tqdm(subjects):\n",
    "        ############################################################\n",
    "        # Fill in the section to calculate the cer and wer for a\n",
    "        # single subject. Look up at other sections to see how it's\n",
    "        # done.\n",
    "        raise NotImplementedError(\"Student exercise\")\n",
    "        ############################################################\n",
    "\n",
    "        # Load images and labels for a given subject\n",
    "        images, true_transcripts = ...\n",
    "\n",
    "        # Transcribe the images to text\n",
    "        transcribed_text = ...\n",
    "\n",
    "        # Calculate the CER and WER\n",
    "        cer, wer = ...\n",
    "\n",
    "        results.append({\n",
    "            'subject': subject,\n",
    "            'cer': cer,\n",
    "            'wer': wer,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b1cb3-cc7b-415f-a790-06054cfd53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def calculate_all_mismatch(df, model, processor):\n",
    "    \"\"\"\n",
    "    Calculate CER and WER for all subjects in a dataset\n",
    "\n",
    "    Args:\n",
    "        df: a dataframe containing information about images and transcripts\n",
    "        model: an image-to-text model\n",
    "        processor: a processor object\n",
    "\n",
    "    Returns:\n",
    "        a list of dictionaries containing a per-subject breakdown of the\n",
    "        results\n",
    "    \"\"\"\n",
    "    subjects = df.subject.unique().tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Calculate CER and WER for all subjects\n",
    "    for subject in tqdm.tqdm(subjects):\n",
    "        # Load images and labels for a given subject\n",
    "        images, true_transcripts = get_images_and_transcripts(df, subject)\n",
    "\n",
    "        # Transcribe the images to text\n",
    "        transcribed_text = transcribe_images(images, model, processor)\n",
    "\n",
    "        # Calculate the CER and WER\n",
    "        cer, wer = calculate_mismatch(transcribed_text, true_transcripts)\n",
    "\n",
    "        results.append({\n",
    "            'subject': subject,\n",
    "            'cer': cer,\n",
    "            'wer': wer,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdede9c-1958-476f-94da-de6483cab32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_all_mismatch(df, model, processor)\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1ed00-87ef-4dc4-993e-0ff0118b9baa",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Not all subjects are as easy to transcribe as subject 52! Let's check out subject 57, who has high CER and WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678275e-d2b3-4046-9898-59fa92f85e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A subject that's harder to read\")\n",
    "images, true_transcripts = get_images_and_transcripts(df, 57)\n",
    "visualize_images_and_transcripts(images, true_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aec500-3b20-4738-90a0-185f8e427c4e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Indeed, this text seems harder to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369b40f-f00c-4c14-9ac5-fa14c73ff301",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Code exercise 1.3: measure OOD generalization\n",
    "\n",
    "What we've done thus far is to measure the empirical loss–the character error rate–for each subject. The empirical loss is defined as:\n",
    "\n",
    "$$R^e(\\theta) = \\mathbb{E}^e[ L(y, f(x, \\theta)) ] $$\n",
    "\n",
    "Here:\n",
    "\n",
    "* The environment $e$ is the training distribution\n",
    "* $R^e(\\theta)$ is the empirical risk in an environment\n",
    "* $\\theta$ are the learned parameters of the TrOCR model\n",
    "* $x$ is the conditioning data, that is, the images\n",
    "* $f$ is the function approximated by the TrOCR model, which maps images to probabilities of certain tokens\n",
    "* $L$ is the loss (or metric–not necessarily differentiable) for a single line of text, the character error rate (CER)\n",
    "* $\\mathbb{E}^e$ is the expectation taken over all the samples\n",
    "\n",
    "A single environment $e$ corresponds to a single subject. The out-of-distribution generalization is instead given by:\n",
    "\n",
    "$$R^{OOD} = \\max_{e \\in \\mathcal{E}_{all}} R^e(\\theta) $$\n",
    "\n",
    "It's the worst-case empirical loss over the out-of-distribution environments ${e \\in \\mathcal{E}_{all}}$ we wish to deploy on. In other words, the character error rate for the subject with the most difficult-to-read handwriting.\n",
    "\n",
    "Intuitively, our AI developer's vision of robustness might be: my note transcription app is robust and generalizes if it works well even when someone has illegible handwriting. The app is only as good as how well it works in the worst-case scenario. Let's measure that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca28eb2-40ba-4b3b-8b89-adddcbe63a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_max_cer(df_results):\n",
    "    \"\"\"\n",
    "    Calculate the mean character-error-rate across subjects as\n",
    "    well as the maximum (that is, the OOD risk).\n",
    "\n",
    "    Args:\n",
    "        df_results: a dataframe containing results\n",
    "\n",
    "    Returns:\n",
    "        A tuple, (mean_cer, max_cer)\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # Fill in the section to calculate the mean and max cer\n",
    "    # across subjects.\n",
    "    raise NotImplementedError(\"Student exercise\")\n",
    "    ############################################################\n",
    "\n",
    "    # Calculate the mean CER across test subjects.\n",
    "    mean_subjects = ...\n",
    "\n",
    "    # Calculate the max CER across test subjects.\n",
    "    max_subjects = ...\n",
    "    return mean_subjects, max_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f1759-816f-4af7-ab6e-cd1cb14e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def calculate_mean_max_cer(df_results):\n",
    "    \"\"\"\n",
    "    Calculate the mean character-error-rate across subjects as\n",
    "    well as the maximum (that is, the OOD risk).\n",
    "\n",
    "    Args:\n",
    "        df_results: a dataframe containing results\n",
    "\n",
    "    Returns:\n",
    "        A tuple, (mean_cer, max_cer)\n",
    "    \"\"\"\n",
    "    # Calculate the mean CER across test subjects.\n",
    "    mean_subjects = df_results.cer.mean()\n",
    "\n",
    "    # Calculate the max CER across test subjects.\n",
    "    max_subjects = df_results.cer.max()\n",
    "    return mean_subjects, max_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee896b4a-181b-4b49-9439-ad44373af99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_subjects, max_subjects = calculate_mean_max_cer(df_results)\n",
    "mean_subjects, max_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c20d9b-5ebb-4e86-8c4b-2c16bab3477c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We see that:\n",
    "\n",
    "* when measured on this (admittedly small) out-of-distribution dataset, the average character error rate is about 5.8%, larger than the 3.4% reported for IAM\n",
    "* the out-of-distribution character error rate is 12%\n",
    "\n",
    "Whether that's good enough for our AI developer depends on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37296654-60d7-4a58-b93d-2e27458bcc7b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Discussion\n",
    "\n",
    "Numbers in tables filled with benchmarks don't tell the whole story: often, we care about OOD robustness. Our developer benchmarked the TrOCR model for their use case and found a worst-case character error rate above 10%. Whether or not that's acceptable is a judgment call, and it's not the only metric the developer might care about. They might also need to meet other constraints:\n",
    "\n",
    "- Memory, FLOPs, latency, cost of inference: the deployment environment might not be able to support very large-scale models because of memory or compute constraints, or those would run too slowly for the use case. Cloud inference might not be practical with limited internet access.\n",
    "- SWaP-C: if the model is embodied in a physical device, the Size, Weight, Power and Cost of that device will ultimately be important.\n",
    "- Latency of development: a bespoke model developed from scratch might take a long time to develop; our busy developer might prefer to adapt a pretrained, sub-optimal architecture than using a custom architecture\n",
    "- Cost of upkeep: machine learning systems can be notoriously difficult to keep running. Our developer might prefer to use a suboptimal system managed by somebody else than taking on the burden of dealing with the upkeep themselves.\n",
    "\n",
    "Our intrepid developer wants to ship this app soon! They decide on a strategy: the model is good enough to get started. They'll deploy the model as is, but they'll have an option in the app to report errors. They'll then label *those* errors and fine-tune the model. Before that though, they want to understand what's inside the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3c4e0-7f5b-4518-9a01-2f930f8e81e2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 3: Dissecting TrOCR\n",
    "\n",
    "TrOCR (transformer-based optical character recognition) is a model that performs printed optical character recognition and handwriting transcription on the basis of two transformers. But what's inside of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e68dd4-b3dc-40ac-92af-8ad949fc0764",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "![Picture which shows trocr architecture.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/trocr_architecture.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef6729-0f0b-4f4c-87b2-415cd792d865",
   "metadata": {
    "execution": {}
   },
   "source": [
    "TrOCR uses two transformers in an encoder-decoder architecture:\n",
    "\n",
    "1. An encoder, a vision transformer (ViT), maps 16x16 patches of the image to individual tokens\n",
    "2. A decoder, a text transformer, maps previously decoded text and the encoder hidden state to the next token in the sequence to be decoded. This is known as causal language modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3ee14-f0cc-4639-bcb4-0ca4e0167a2c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: A recap of transformers\n",
    "\n",
    "[We covered transformers in W2D5 of the DL course](https://deeplearning.neuromatch.io/tutorials/W2D5_AttentionAndTransformers/student/W2D5_Tutorial1.html). Let's quickly recap transformers. Transformers are a class of deep learning architectures that have become dominant in natural language processing (NLP) since their introduction in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. Their success in natural language processing has led to their application across other domains, including computer vision, which is the case with TrOCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454f061-01c1-4e06-b878-405b4f270099",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "![Picture which shows one layer transformer.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/transformer_one_layer.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AL2Wr4LNQPc6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "*Illustration from Alammar, J (2018). The Illustrated Transformer. Retrieved from https://jalammar.github.io/illustrated-transformer/*\n",
    "\n",
    "Transformers are built on self-attention, allowing them to weigh the importance of different parts of the input data differently. This has proven useful for tasks that require an understanding of context, such as language translation, text summarization, and, as we will see, optical character recognition. Some key components of transformers are:\n",
    "\n",
    "- Tokenization: the input sequence (e.g. sentence) is split into different components (e.g. word pieces). Each component, or token, is embedded into a fixed dimensional space. In natural language processing, tokenization is done via a lookup table: every word piece is mapped to a fixed-dimensional vector. [See W3D1 of the DL course for a refresher on tokenization](https://deeplearning.neuromatch.io/tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/student/W3D1_Tutorial2.html?highlight=word2vec#tokenizers).\n",
    "\n",
    "- Self-attention: A self-attention mechanism allows the tokens in the sequence to interact to form new representations. Specifically, queries and keys are derived from tokens; an inner product between queries and keys, followed by a softmax, forms the attention matrix. The attention matrix is multiplied by the value matrix to obtain a new representation.\n",
    "\n",
    "- Positional encoding: Positional encoding is added to the input to give the model information about the position of each token within the sequence. Unlike RNNs or CNNs, transformers do not process data in order–without position encoding, they are permutation invariant. We'll dig deeper into what this implies in the section on the inductive biases of transformers.\n",
    "\n",
    "- Layer Normalization and Residual Connections are used within the transformer architecture to stabilize the learning process and improve the model's ability to learn deep representations.\n",
    "\n",
    "One of the key advantages of transformers over previous architectures is a high degree of parallelism, which allows one to train larger, more capable models. Let's inspect the training data of TrOCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8292e-1509-4f08-a2fc-fe444211669a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: The encoder and decoder\n",
    "\n",
    "Let's dig in more specifically into the **encoder** inside of TrOCR. It's a visual transformer (ViT), an adaptation of transformers for problems in vision. It proceeds as follows:\n",
    "\n",
    "1. It takes a raw image and resizes it to 384x384\n",
    "2. It chops it up into 16x16 patches\n",
    "3. It embeds each patch inside a fixed dimensional space, adding positional embeddings\n",
    "4. It passes the patches through self-attention layers.\n",
    "5. It ends up $577=(384/16)^2+1$ total embedded tokens. For the base model, the tokens have an embedding size of 768.\n",
    "\n",
    "Let's see the structure of the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70729ebc-9d99-4d06-8f71-b22656138065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fe34c-c68c-468d-8afe-e27f87a6ebc4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Code exercise 3.1: Understanding the inputs and outputs of the decoder\n",
    "\n",
    "Let's make sure we understand how the encoder operates by giving it a sample input and checking that its output is the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3b9aa-0b35-42a7-adb8-a817526ed5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_decoder(model):\n",
    "    \"\"\"\n",
    "    Inspect decoder to verify that it processes inputs in the expected way.\n",
    "\n",
    "    Args:\n",
    "        model: the TrOCR model\n",
    "    \"\"\"\n",
    "    ##################################################################\n",
    "    # Feed the encoder an input and measure the output to understand\n",
    "    # the role of the vision encoder.\n",
    "    raise NotImplementedError(\"Student exercise\")\n",
    "    #\n",
    "    ##################################################################\n",
    "    # Create an empty tensor (batch size of 1) to feed it to the encoder.\n",
    "    # Remember that images should have 3 channels and have size 384x384\n",
    "    # Recall that images are fed in pytorch with tensors of shape\n",
    "    # batch x channels x height x width\n",
    "    single_input = ...\n",
    "\n",
    "    # Run the input through the encoder.\n",
    "    output = ...\n",
    "\n",
    "    # Measure the number of hidden tokens which are the output of the encoder\n",
    "    hidden_shape = output['last_hidden_state'].shape\n",
    "\n",
    "    assert hidden_shape[0] == 1\n",
    "    assert hidden_shape[1] == 577\n",
    "    assert hidden_shape[2] == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6177ce-7005-48ae-8faa-d4008ae5942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def inspect_decoder(model):\n",
    "    \"\"\"\n",
    "    Inspect decoder to verify that it processes inputs in the expected way.\n",
    "\n",
    "    Args:\n",
    "        model: the TrOCR model\n",
    "    \"\"\"\n",
    "    # Create an empty tensor (batch size of 1) to feed it to the encoder.\n",
    "    # Remember that images should have 3 channels and have size 384x384\n",
    "    # Recall that images are fed in pytorch with tensors of shape\n",
    "    # batch x channels x height x width\n",
    "    single_input = torch.zeros(1, 3, 384, 384).to(device)\n",
    "\n",
    "    # Run the input through the encoder.\n",
    "    output = model.encoder(single_input)\n",
    "\n",
    "    # Measure the number of hidden tokens which are the output of the encoder\n",
    "    hidden_shape = output['last_hidden_state'].shape\n",
    "\n",
    "    assert hidden_shape[0] == 1\n",
    "    assert hidden_shape[1] == 577\n",
    "    assert hidden_shape[2] == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2eec7c-6173-4113-b303-8722e6b356d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_decoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046051f-f33a-48bf-bf1b-a82de33ead17",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The vision transformer acts much like a conventional encoder transformer in sequence-to-sequence tasks: it maps the input sequence to a hidden representation. This hidden representation is then attended during decoding using cross-attention.\n",
    "\n",
    "We can locate the cross-attention in the decoder, as its keys and values have dimensionality 768 (same as the encoder) and its queries are of dimension 1024 (like the rest of the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f93ae-1d5d-4b9d-9d76-fad1685ac923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f666c65-047f-439c-9e5c-0c82b39e21b9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "TL;DR: there's nothing magic going on: there are two relatively large-scale transformers which are wired in the conventional encoder-decoder architecture. The transformers themselves are generic and have relatively weak built-in inductive biases. What allows the model to generalize beyond its training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee81e82-ff71-4a2d-8451-77b59cd71966",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 4: The magic in the data\n",
    "\n",
    "It's straightforward to write down the encoder-decoder transformer used by TrOCR–it's conceptually quite similar to the original transformer as outlined in Vaswani et al. (2017). What makes the model tick (and potentially break) is a good training pipeline to ensure good OOD generalization. It's worth taking a look at the TrOCR paper to see the many different sources of data which are used to train the model:\n",
    "\n",
    "1. [The encoder is pretrained on masked image modelling on ImageNet-22k](https://huggingface.co/docs/transformers/en/model_doc/beit)\n",
    "2. [The decoder is pretrained on masked language modelling on 160GB of raw text](https://arxiv.org/abs/1907.11692)\n",
    "3. The entire model is trained end-to-end on 648M text lines found in 2M PDF pages on the internet, with the fonts randomly swapped\n",
    "4. The model is then fine-tuned end-to-end on the IAM handwriting dataset, with heavy augmentation\n",
    "\n",
    "Let's look at a few of these pieces in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da9d1a-f84a-4a6c-99ba-5a3d2dad2823",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.1: text and video replacing hours of text measurement exercise\n",
    "\n",
    "In this section, we take a look at ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c49ad2-af76-4741-bc61-1cb96bd844a7",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "#content_review(f\"{feedback_prefix}_Calculate_Writing_Time_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce0081-636c-4f40-bc2f-fc8f2d22dee7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.2: Generalization via augmentation\n",
    "\n",
    "Another important part of the training recipe for this model is the use of multiple augmentations of the data. When data is not abundant, this can improve generalization. Thus, we take an expressive model with few built-in inductive biases, and through demonstrations, let it learn invariances and equivariances in the data, encouraging generalization.\n",
    "\n",
    "By applying various transformations to images and displaying the results, you can visually understand how augmentation works and its impact on model performance. Let's look at parts of the TrOCR recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dcd8-f11a-447a-bf33-564f90239456",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's start with loading and visualizing our chosen image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327840a-93df-45c7-9435-ea9d4269b9eb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Picture which shows neuroai_hello_world.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D1_Generalization/static/neuroai_hello_world.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adf7a5-9cf0-457d-b645-38a26cddec10",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we will apply a few transformations to this image. You can play around with the input values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0a320-46f3-4f84-8c51-c8cd07cea776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL Image to Tensor\n",
    "image = IMG.open(\"neuroai_hello_world.png\")\n",
    "image = transforms.ToTensor()(image)\n",
    "\n",
    "# Define each transformation separately\n",
    "# RandomAffine: applies rotations, translations, scaling. Here, rotates by up to ±15 degrees,\n",
    "affine = transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "\n",
    "# ElasticTransform: applies elastic distortions to the image. The 'alpha' parameter controls\n",
    "# the intensity of the distortion.\n",
    "elastic = transforms.ElasticTransform(alpha=25.0)\n",
    "\n",
    "# RandomPerspective: applies random perspective transformations with a specified distortion scale.\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "# RandomErasing: randomly erases a rectangle area in the image.\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "\n",
    "# GaussianBlur: applies gaussian blur with specified kernel size and sigma range.\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ee77d-988c-474a-bc76-582b7315e436",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's now combine them in a single list and display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e691d06-e1bc-45fc-b289-89827eaf0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all transformations for iteration\n",
    "transformations = [affine, elastic, perspective, erasing, gaussian_blur]\n",
    "\n",
    "# Display\n",
    "display_transformed_images(image, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a5322-0b23-49be-9a2c-a2c1a023b873",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The transformations applied to the model include:\n",
    "\n",
    "1. Original: the baseline image without any modifications.\n",
    "2. RandomAffine: applies random affine transformations to the image, which include translation, scaling, rotation, and shearing. This helps the model become invariant to such transformations in the input data.\n",
    "3. ElasticTransform: introduces random elastic deformations, simulating non-linear transformations that might occur naturally. It is useful for tasks where we expect such distortions, like medical image analysis.\n",
    "4. RandomPerspective: changes the perspective from which the image is viewed, simulating the effect of viewing the object from different angles.\n",
    "5. RandomErasing: randomly removes parts of the image and fills it with some arbitrary pixel values. It can make the model robust against occlusions in the input data.\n",
    "6. GaussianBlur: applies a Gaussian blur to the image, smoothing it. This can help the model be better with out-of-focus images.\n",
    "\n",
    "All of these augmentations, which are part of this models' training recipe, help prevent overfitting and improving the generalization of the model to new, unseen images. We can compose these to create new challenging training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c000dc-2c1e-41e6-a988-ab75819b0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the transformations\n",
    "all_transforms = transforms.Compose([\n",
    "    affine,\n",
    "    elastic,\n",
    "    perspective,\n",
    "    erasing,\n",
    "    gaussian_blur\n",
    "])\n",
    "\n",
    "# Apply combined transformation\n",
    "augmented_image_tensor = all_transforms(image)\n",
    "\n",
    "display_original_and_transformed_images(image, augmented_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13dff2-5e74-46d3-be3d-deafe27a7383",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, all those trasnformations are being applied simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03601c-741d-4c59-82eb-3ee0690afa3c",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "#content_review(f\"{feedback_prefix}_Augmentation_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb05cb5-9d0c-427d-a246-f80c09bea0ec",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.3: Generalization via synthetic data\n",
    "\n",
    "When augmentation is not enough, we can further improve generalization by training on synthetic data. This allows us to stretch our data even further. Data augmentation creates variations of existing data without changing its inherent properties, while synthetic data generation creates entirely new data that mimics the characteristics of real data.\n",
    "\n",
    "As it turns out, generating new text is tractable–text can be rendered in a wide range of cursive fonts to simulate real data. Here, we'll showcase this idea by defining strings and create a generator to generate a synthetic version of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Yee_i85LExa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strings\n",
    "strings = ['Hello world', 'This is the first tutorial', 'For Neuromatch NeuroAI']\n",
    "\n",
    "# Specify font path\n",
    "font_path = \"DancingScript-VariableFont_wght.ttf\"  # Ensure this path is correct\n",
    "\n",
    "# Example usage\n",
    "strings = ['Hello world', 'This is the first tutorial', 'For Neuromatch NeuroAI']\n",
    "font_path = \"DancingScript-VariableFont_wght.ttf\"  # Ensure this path is correct\n",
    "\n",
    "# Create a generator with the specified parameters\n",
    "generator = image_generator(strings, font_path, space_width=2, skewing_angle=3)\n",
    "\n",
    "i = 1\n",
    "for img in generator:\n",
    "  plt.imshow(img, cmap='gray')\n",
    "  plt.title(f\"Example {i}\")\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e6f2f-672d-41dc-9020-16d9f12259ff",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "What does this type of synthetic data capture that wouldn’t be easy to capture through data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ccec7-3247-4055-9838-ec8f08b0325b",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "#content_review(f\"{feedback_prefix}_Discussion_Synthetic_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7071b-4230-481f-9abd-b2967f53f279",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive demo 4.1: Generating handwriting style data\n",
    "\n",
    "We can take this idea further and generate handwriting style data. We will use an embedded calligrapher.ai model to generate new snippets of writing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca19de-44b0-492d-af5b-8fa78fc5e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.calligrapher.ai/\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592d515-dad1-4db3-a435-e2183422606d",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "#content_review(f\"{feedback_prefix}_Generate_Handwriting_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0c558-a0be-46f2-8e65-9babe22ff4f4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We train models to minimize a loss function. Oftentimes, however, what we care about is something different, like how well the model will generalize when it's deployed. Our intrepid developer got a rude awakening in comparing the OOD robustness of the model to its empirical loss on the train set: the character error rate was several times larger than expected. Motivated by other factors, like engineering complexity, our developer decided to move forward and deploy a handwriting transcription system, hoping it could be fine-tuned based on user data later.\n",
    "\n",
    "There's a lot that goes into the training of robust AI models that generalize well. Generic high-capacity models with weak inductive biases, like transformers, are trained on large-scale data. Pretraining, augmentations and synthetic data can all be part of the recipe for learning good inductive biases that might be hard to express mathematically. Because large-scale models can often require significant compute to train, in practice, models that have been trained for other purposes are adapted and re-used, preventing the need to learn from scratch. These models embody what's known as [\"the bitter lesson\"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html): general methods that leverage computation are ultimately the most effective, and by a large margin."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D1_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
