{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207f0be-cbd8-4b93-a823-61af51421e2a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Generalization in AI\n",
    "\n",
    "**Week 1, Day 1: Generalization**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d71fc6-0728-41bd-b84e-ac78a622ece4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "\n",
    "1. Identify and articulate common objectives pursued by developers of operational AI systems, such as:\n",
    "\n",
    "- Performance, latency, Size, Weight, Power, and Cost (SWaP-C)\n",
    "- Achieving explainability and comprehension\n",
    "\n",
    "2. Explain at least three strategies for enhancing the generalization capabilities of AI systems, including the contemporary trend of training generic models on extensive datasets, commonly referred to as the \"bitter lesson.\"\n",
    "\n",
    "3. Gain practical experience with the fundamentals of deep learning and PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff267ac-6f23-4ef3-bb42-098ce864d0b4",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee444563-8ef0-4b20-b8d7-92eeace85488",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acdfc5-c864-40a0-b648-be385d5c3eb5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "#!pip install numpy Pillow matplotlib torch torchvision transformers gradio sentencepiece protobuf\n",
    "#!pip install git+https://github.com/Belval/TextRecognitionDataGenerator#egg=trdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40270953",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard Libraries for file and operating system operations, security, and web requests\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Core python data science and image processing libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning and model specific libraries\n",
    "import torch\n",
    "import transformers\n",
    "from torchvision import transforms\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import google.protobuf\n",
    "\n",
    "# Utility and interface libraries\n",
    "import gradio as gr\n",
    "from IPython.display import IFrame\n",
    "import trdg\n",
    "from trdg.generators import GeneratorFromStrings\n",
    "import sentencepiece\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa95f5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf34b9a-1dd5-458a-b390-0fa12609d532",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_image(image_path):\n",
    "    \"\"\"Display an image from a given file path.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): The path to the image file.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "def display_transformed_images(image, transformations):\n",
    "    \"\"\"\n",
    "    Apply a list of transformations to an image and display them.\n",
    "\n",
    "    Inputs:\n",
    "    - image (Tensor): The input image as a tensor.\n",
    "    - transformations (list): A list of torchvision transformations to apply.\n",
    "    \"\"\"\n",
    "    # Convert tensor image to PIL Image for display\n",
    "    pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "    fig, axs = plt.subplots(len(transformations) + 1, 1, figsize=(5, 15))\n",
    "    axs[0].imshow(pil_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, transform in enumerate(transformations):\n",
    "        # Apply transformation if it's not the placeholder\n",
    "        if transform != \"Custom ElasticTransform Placeholder\":\n",
    "            transformed_image = transform(image)\n",
    "            # Convert transformed tensor image to PIL Image for display\n",
    "            display_image = transforms.ToPILImage()(transformed_image)\n",
    "            axs[i+1].imshow(display_image)\n",
    "            axs[i+1].set_title(transform.__class__.__name__)\n",
    "            axs[i+1].axis('off')\n",
    "        else:\n",
    "            axs[i+1].text(0.5, 0.5, 'ElasticTransform Placeholder', ha='center')\n",
    "            axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_original_and_transformed_images(original_tensor, transformed_tensor):\n",
    "    \"\"\"\n",
    "    Display the original and transformed images side by side.\n",
    "\n",
    "    Inputs:\n",
    "    - original_tensor (Tensor): The original image as a tensor.\n",
    "    - transformed_tensor (Tensor): The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display original image\n",
    "    original_image = original_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display transformed image\n",
    "    transformed_image = transformed_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[1].imshow(transformed_image)\n",
    "    axs[1].set_title('Transformed')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_generated_images(generator):\n",
    "    \"\"\"\n",
    "    Display images generated from strings.\n",
    "\n",
    "    Inputs:\n",
    "    - generator (GeneratorFromStrings): A generator that produces images from strings.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, (text_img, lbl) in enumerate(generator, 1):\n",
    "        ax = plt.subplot(1, len(generator.strings) * generator.count // len(generator.strings), i)\n",
    "        plt.imshow(text_img)\n",
    "        plt.title(f\"Example {i}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a34ac-fa41-4e90-ab45-82c14384a83e",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "def download_file(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded file.\n",
    "    - url (str): The URL from which to download the file.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url)  # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content)  # Write the downloaded content to a file\n",
    "                print(f\"{fname} has been downloaded successfully.\")\n",
    "\n",
    "# Variables for the font file and download URL\n",
    "fname = \"Dancing_Script.zip\"\n",
    "url = \"https://osf.io/32yed/download\"\n",
    "expected_md5 = \"d59bd3201b58a37d0d3b4cd0b0ec7400\"\n",
    "\n",
    "# Download the font file\n",
    "download_file(fname, url, expected_md5)\n",
    "\n",
    "def extract_zip(zip_fname):\n",
    "    \"\"\"\n",
    "    Extracts a ZIP file to the current directory.\n",
    "\n",
    "    Inputs:\n",
    "    - zip_fname (str): The filename/path of the ZIP file to be extracted.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_fname, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "extract_zip(fname)\n",
    "\n",
    "def download_image(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads an image file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded image.\n",
    "    - url (str): The URL from which to download the image.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"img_1235.jpg\", \"image_augmentation.png\"]  # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/kv5bx/download\", \"https://osf.io/fqwsr/download\"]  # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"920ae567f707bfee0be29dc854f804ed\", \"f4f1ebee1470a7e2d7662eec1d193ba2\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    download_image(fname, url, expected_md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ca8c3-c1a3-45e2-8100-d636f8921030",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 1: Introducing the TrOCR (Transformer-based Optical Character Recognition) Model\n",
    "\n",
    "Before diving into the specifics of the TrOCR model, it's crucial to have a solid understanding of the underlying technology that powers it: transformers. Transformers are a class of deep learning models that have revolutionized the field of natural language processing (NLP) since their introduction in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. Their success in NLP has led to their application across a variety of domains, including computer vision, which is the case with TrOCR.\n",
    "\n",
    "### Key Concepts of Transformers\n",
    "\n",
    "Transformers are built on the principle of self-attention, allowing them to weigh the importance of different parts of the input data differently. This is particularly beneficial for tasks that require an understanding of the context, such as language translation, text summarization, and, as we will see, optical character recognition. The key components of transformers are:\n",
    "\n",
    "- A self-Attention mechanism allows the model to focus on different parts of the input sequence when producing a specific part of the output sequence. For OCR, this means the model can learn to focus on specific parts of an image when trying to recognize characters or words.\n",
    "\n",
    "- Positional Encoding is added to the input to give the model information about the position of each part of the sequence (or each pixel/region in the case of images), because transformers do not process data in order (like RNNs or CNNs).\n",
    "\n",
    "- An Encoder-Decoder architecture is followed. The encoder processes the input data and captures its context, while the decoder generates the output sequence, one element at a time. In the context of TrOCR, the encoder would process the image, and the decoder would generate the text.\n",
    "\n",
    "- Layer Normalization and Residual Connections are used within the transformer architecture to stabilize the learning process and improve the model's ability to learn deep representations without degradation.\n",
    "\n",
    "### TrOCR and Transformers\n",
    "\n",
    "TrOCR leverages these transformer principles by integrating them into a model tailored for optical character recognition. It features an encoder for image processing (vision component) and a decoder for text generation (language component). This amalgamation enables TrOCR to bridge the gap between visual and textual data effectively, rendering it a potent tool for transmuting images of text into machine-readable text.\n",
    "\n",
    "The initial step involves loading the pre-trained TrOCR model and its corresponding processor. The model VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\") specializes in recognizing handwritten text, while the TrOCRProcessor prepares input images and decodes model predictions into human-readable text. This showcases the seamless accessibility and utilization of advanced machine learning models with minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00326ca-807d-460f-adf4-767e94bc0ccc",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained TrOCR model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-handwritten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f33da0-8340-431b-913a-fa1f38ff3da6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With the model loaded, the next step involves defining a function utilizing the pre-trained model for Optical Character Recognition (OCR) to convert an image to text. This function preprocesses the image to meet the model's input requirements, predicts text as token IDs, decodes these tokens into human-readable text while excluding special tokens, and returns the recognized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1debc6-b3b1-4173-839c-f2a4240efb2d",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the function to recognize text from an image\n",
    "def recognize_text(image):\n",
    "    \"\"\"\n",
    "    This function takes an image as input and uses a pre-trained language model to generate text from the image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (PIL Image or Tensor): The input image containing text to be recognized.\n",
    "\n",
    "    Outputs:\n",
    "    - text (str): The recognized text extracted from the input image.\n",
    "    \"\"\"\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba7aed-2887-49c8-ac28-c6f704e89cb8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Subsequently, Gradio is employed to craft an interactive web interface for the TrOCR demonstration. Gradio, a Python library, simplifies the creation of UIs for machine learning models. This interface facilitates image uploads and displays recognized text, allowing users to interactively explore the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46ff0-958b-41f1-b00a-2fd0464b3d28",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=recognize_text,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Interactive demo: TrOCR\",\n",
    "    description=\"Demo for Microsoft’s TrOCR, an encoder-decoder model for OCR on single-text line images.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479e8f3-33ae-4ce5-87f6-a2c34b25152a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "How effective is the model's performance? Does it exhibit generalization beyond its training vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4250c8b-45da-45bf-b634-c92c3c6aed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_TrOCR_Interface_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bc209-67df-410d-a2c4-d5855a1421c8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 2: Inspecting the Model's Encoder and Decoder\n",
    "\n",
    "By inspecting the model's encoder and decoder, we gain insight into the inner workings of the TrOCR model. The encoder processes the input images, and the decoder generates text predictions. This step is crucial for understanding the model architecture and the roles of its components in the text recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cd9c9-95fb-4541-9ece-78620745ee1f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Inspect the encoder of the model\n",
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afead452-953b-4114-8525-09071dcbe85b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding exercise\n",
    "\n",
    "In the cell below, inspect the decoder of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1706d-16d6-41a1-a685-a23741c0b6a5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: fill in the missing variables ##\n",
    "# Fill out function and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b13d7-333f-43dc-b1fd-7cd4c92b7ef8",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to remove solution\n",
    "\n",
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29b09e-5300-4212-b0d9-dfa3f9f7c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_TrOCR_Decoder_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f50d0-4fc9-4162-83e9-5e7a83168aec",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Follow-up coding exercise \n",
    "\n",
    "Now, let's create a count_parameters function to compute the total number of parameters in the model, offering an understanding of its complexity. Analyzing parameters separately in the encoder and decoder aids in comprehending the distribution of model complexity and its impact on training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df898317-31e6-4b28-9175-c1206b255a98",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Function to count the parameters of the model\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    This function calculates the total number of parameters in a given PyTorch model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The PyTorch model for which parameters are to be counted.\n",
    "\n",
    "    Outputs:\n",
    "    - num_parameters (int): The total number of parameters in the specified model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count parameters in the encoder\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "\n",
    "# Count parameters in the decoder\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "\n",
    "encoder_params, decoder_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a68484-c156-4cee-8b00-d0baae23b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Count_Parameters_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d7a60-21cd-4556-a0dc-43bc3ba9f478",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Written exercise\n",
    "\n",
    "Look at the available variants of this model (i.e. small, base, large). How do these models trade off in terms of accuracy vs. inference latency? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c09eea-44f6-4448-b2c2-3babcb2e67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Written_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50088f7-6e0a-4dc9-bfd5-6072ed94a7b7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "Models like TrOCR embed various inductive biases that influence their learning and decision-making processes. Some of these biases include:\n",
    "\n",
    "- Spatial bias: given the nature of OCR tasks, TrOCR is biased towards recognizing spatial patterns in images, such as the arrangement of characters and words.\n",
    "\n",
    "- Linguistic bias: the model is predisposed to leveraging linguistic structures and semantics when generating text predictions, reflecting the inherent properties of language.\n",
    "\n",
    "- Contextual bias: TrOCR incorporates contextual information from both the input image and previously generated text tokens to make accurate predictions, reflecting the importance of context in text recognition tasks.\n",
    "\n",
    "- Attention mechanism bias: the self-attention mechanism in transformers biases the model towards attending to relevant parts of the input data, facilitating effective feature extraction and information integration.\n",
    "\n",
    "Understanding these inductive biases is crucial for interpreting the model's behavior and performance across different tasks and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef84cf-3686-43a2-836e-2566a351c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_TrOCR_Discussion_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa316df-4675-44be-a3b8-49c60c469ed5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 3: generalization, part I: transfer learning \n",
    "\n",
    "In this section, we take a look at how much data is distilled inside the model, focusing on the decoder. We want to calculate how long it would take to write a certain number of words, comparing human writing capabilities with the model's text generation speed. \n",
    "\n",
    "Try to fill in the blanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ada4c5-ea4f-4feb-917d-1f9b43b11865",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def calculate_writing_time(total_words, words_per_day, days_per_week, weeks_per_year, average_human_lifespan):\n",
    "    \"\"\"\n",
    "    Calculate the time required to write a given number of words in lifetimes.\n",
    "\n",
    "    Inputs:\n",
    "    - total_words (int): total number of words to be written.\n",
    "    - words_per_day (int): number of words written per day.\n",
    "    - days_per_week (int): number of days dedicated to writing per week.\n",
    "    - weeks_per_year (int): number of weeks dedicated to writing per year.\n",
    "    - average_human_lifespan (int): average lifespan of a human in years.\n",
    "\n",
    "    Outpus:\n",
    "    - time_to_write_lifetimes (float): time to write the given words in lifetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: fill in the missing variables ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "    #################################################\n",
    "\n",
    "    words_per_year = words_per_day * days_per_week * weeks_per_year\n",
    "\n",
    "    # Calculate the time to write in years\n",
    "    time_to_write_years = total_words / ...\n",
    "\n",
    "    # Calculate the time to write in lifetimes\n",
    "    time_to_write_lifetimes = time_to_write_years / average_human_lifespan\n",
    "\n",
    "    return time_to_write_lifetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6127df-6dde-445f-8b49-3df6ba9064fb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def calculate_writing_time(total_words, words_per_day, days_per_week, weeks_per_year, average_human_lifespan):\n",
    "    \"\"\"\n",
    "    Calculate the time required to write a given number of words in lifetimes.\n",
    "\n",
    "    Inputs:\n",
    "    - total_words (int): total number of words to be written.\n",
    "    - words_per_day (int): number of words written per day.\n",
    "    - days_per_week (int): number of days dedicated to writing per week.\n",
    "    - weeks_per_year (int): number of weeks dedicated to writing per year.\n",
    "    - average_human_lifespan (int): average lifespan of a human in years.\n",
    "\n",
    "    Outpus:\n",
    "    - time_to_write_lifetimes (float): time to write the given words in lifetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    words_per_year = words_per_day * days_per_week * weeks_per_year\n",
    "\n",
    "    # Calculate the time to write in years\n",
    "    time_to_write_years = total_words / words_per_year\n",
    "\n",
    "    # Calculate the time to write in lifetimes\n",
    "    time_to_write_lifetimes = time_to_write_years / average_human_lifespan\n",
    "\n",
    "    return time_to_write_lifetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a24ae-6ec6-44cc-bb49-3b5dd27fa5eb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Example values\n",
    "total_words = 5e9\n",
    "words_per_day = 1500\n",
    "days_per_week = 6\n",
    "weeks_per_year = 50\n",
    "average_human_lifespan = 80\n",
    "\n",
    "# Test the function\n",
    "time_to_write_lifetimes_roberta = calculate_writing_time(\n",
    "    total_words,\n",
    "    words_per_day,\n",
    "    days_per_week,\n",
    "    weeks_per_year,\n",
    "    average_human_lifespan\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time to write {total_words} words in lifetimes: {time_to_write_lifetimes_roberta} lifetimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c49ad2-af76-4741-bc61-1cb96bd844a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Calculate_Writing_Time_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e55acc-2fab-4df6-8298-f4f41f868205",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exploring LLaMA 2\n",
    "\n",
    "RoBERTa is a pretty tiny model by modern standards. A more modern LLM like Llama 2 is trained on 2 trillion tokens. How much time would it take to generate that much text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19006eb-4600-4aa4-9c99-b85b5e4a87c5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Exploring Llama 2\n",
    "total_tokens_llama2 = 2e12\n",
    "total_words_llama2 = 2e12 / 1.5 #assuming 1.5 words per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785a098-d6ff-48a8-8b57-3e0e9e436ffe",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Time to generate text\n",
    "time_to_write_lifetimes_llama = calculate_writing_time(total_words_llama2, words_per_day, days_per_week, weeks_per_year, average_human_lifespan)\n",
    "print(time_to_write_lifetimes_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc491eab-ca40-46ee-9e66-dadb13240d69",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Assuming 1.5 tokens/word, it would take ~=37,000 lifetimes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3911c18-cf91-4273-9673-6f9e10f276dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_LLaMA2_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce0081-636c-4f40-bc2f-fc8f2d22dee7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 3: Generalization, part II: augmentation\n",
    "\n",
    "This section introduces the concept of image augmentation, a technique used to increase the diversity of training data and improve model generalization. When data is not abundant, we can improve generalization by augmenting the existing dataset with variants of the same data. Thus, we take an expressive model with few built-in inductive biases, and through demonstrations, let it learn invariances and equivariances in the data, encouraging generalization.\n",
    "\n",
    "By applying various transformations to images and displaying the results, you can visually understand how augmentation works and its impact on model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dcd8-f11a-447a-bf33-564f90239456",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's start with loading and visualizing our chosen image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78127c97-955e-46e0-869d-7f7285449f70",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "image_path = 'img_1235.jpg'\n",
    "display_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adf7a5-9cf0-457d-b645-38a26cddec10",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we will apply a few transformations to this image. You can play around with the input values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0a320-46f3-4f84-8c51-c8cd07cea776",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Convert PIL Image to Tensor\n",
    "image = Image.open(image_path)\n",
    "image = transforms.ToTensor()(image)\n",
    "\n",
    "# Define each transformation separately\n",
    "# RandomAffine: applies rotations, translations, scaling. Here, rotates by up to ±15 degrees,\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "\n",
    "# ElasticTransform: applies elastic distortions to the image. The 'alpha' parameter controls\n",
    "# the intensity of the distortion.\n",
    "elastic = transforms.ElasticTransform(alpha=50.0)\n",
    "\n",
    "# RandomPerspective: applies random perspective transformations with a specified distortion scale.\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "# RandomErasing: randomly erases a rectangle area in the image.\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "\n",
    "# GaussianBlur: applies gaussian blur with specified kernel size and sigma range.\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ee77d-988c-474a-bc76-582b7315e436",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's now combine them in a single list and display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e691d06-e1bc-45fc-b289-89827eaf0317",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# A list of all transformations for iteration\n",
    "transformations = [affine, elastic, perspective, erasing, gaussian_blur]\n",
    "\n",
    "# Display\n",
    "display_transformed_images(image, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a5322-0b23-49be-9a2c-a2c1a023b873",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As you can notice, this images showcase various common data augmentation techniques used in machine learning to increase the diversity of a dataset. Each section represents a transformation applied to the original image:\n",
    "\n",
    "1. Original: the baseline image without any modifications.\n",
    "2. RandomAffine: applies random affine transformations to the image, which include translation, scaling, rotation, and shearing. This helps the model become invariant to such transformations in the input data.\n",
    "3. ElasticTransform: introduces random elastic deformations, simulating non-linear transformations that might occur naturally. It is useful for tasks where we expect such distortions, like medical image analysis.\n",
    "4. RandomPerspective: changes the perspective from which the image is viewed, simulating the effect of viewing the object from different angles.\n",
    "5. RandomErasing: randomly removes parts of the image and fills it with some arbitrary pixel values. It can make the model robust against occlusions in the input data.\n",
    "6. GaussianBlur: applies a Gaussian blur to the image, smoothing it. This can help in reducing high-frequency noise in the images.\n",
    "\n",
    "These techniques are essential for training robust models, particularly in computer vision, as they help in preventing overfitting and improving the generalization of the model to new, unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ed9ac-5bb8-4e07-8bb1-6a71442a6244",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also use transforms. Compose from PyTorch to apply all of these transformations simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c000dc-2c1e-41e6-a988-ab75819b0b39",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Path to the image\n",
    "image_path = 'image_augmentation.png'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert PIL Image to Tensor\n",
    "image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# Define transformations here\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "elastic = transforms.ElasticTransform(alpha=90.0)\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "# Combine all the transformations\n",
    "all_transforms = transforms.Compose([\n",
    "    affine,\n",
    "    elastic,\n",
    "    perspective,\n",
    "    erasing,\n",
    "    gaussian_blur\n",
    "])\n",
    "\n",
    "# Apply combined transformation\n",
    "augmented_image_tensor = all_transforms(image_tensor)\n",
    "\n",
    "display_original_and_transformed_images(image_tensor, augmented_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13dff2-5e74-46d3-be3d-deafe27a7383",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, all those trasnformations are being applied simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03601c-741d-4c59-82eb-3ee0690afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Augmentation_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb05cb5-9d0c-427d-a246-f80c09bea0ec",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 3: Generalization, part III: synthetic data\n",
    "\n",
    "When augmentation is not enough, we can further improve generalization by training on synthetic data. This allows us to stretch our data even further. \n",
    "\n",
    "Crucially, data augmentation creates variations of existing data without changing its inherent properties, while synthetic data generation creates entirely new data that mimics the characteristics of real data.\n",
    "\n",
    "Here, we will define strings and create a generator to generate a synthetic version of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1a69c-005a-41c1-8014-8fe9cb4c8ea7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define strings\n",
    "strings = ['Hello', 'This is Patrick', 'From NMA']\n",
    "\n",
    "# Specify font path\n",
    "font_path = \"DancingScript-VariableFont_wght.ttf\"  # Ensure this path is correct\n",
    "\n",
    "# Create a generator with the specified parameters\n",
    "generator = GeneratorFromStrings(\n",
    "    strings=strings,\n",
    "    fonts=[font_path],\n",
    "    space_width=2,\n",
    "    skewing_angle=8,\n",
    "    count=3\n",
    ")\n",
    "\n",
    "# Define the desired size\n",
    "desired_size = (500, 300)  # Width, Height in pixels\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(image, new_size):\n",
    "    return image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "for img, lbl in generator:\n",
    "    # Resize the image before showing it\n",
    "    img = resize_image(img, desired_size)\n",
    "    img.show()\n",
    "\n",
    "# Call the function with the generator\n",
    "display_generated_images(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148975b-1a97-4d28-83e4-429d86e77e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Synthetic_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e6f2f-672d-41dc-9020-16d9f12259ff",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "What does this type of synthetic data capture that wouldn’t be easy to capture through data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ccec7-3247-4055-9838-ec8f08b0325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Discussion_Synthetic_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7071b-4230-481f-9abd-b2967f53f279",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Generating handwriting style data\n",
    "\n",
    "We can take this idea further and generate handwriting style data. We will use an embedded calligrapher.ai model to generate new snippets of writing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca19de-44b0-492d-af5b-8fa78fc5e51e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.calligrapher.ai/\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592d515-dad1-4db3-a435-e2183422606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Generate_Handwriting_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdb822-ef4b-4c82-8ecf-dd1c3119be3a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "What kinds of variation does this synthetic data capture that wouldn’t be easy to reproduce to all our other data generation pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17d357-9228-4f03-96c5-36430fda06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Discussion_Handwriting_Exercise\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial1",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
