{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c900bed-85b4-4e92-a7c5-d705f2d563bc",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "!pip install requests matplotlib numpy Pillow scikit-image scikit-learn torch torchvision scipy ipywidgets tqdm\n",
    "\n",
    "# Install GNS and pyBPL\n",
    "!pip install git+https://github.com/neuromatch/GNS-Modeling\n",
    "!pip install git+https://github.com/neuromatch/pyBPL\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2870-3fd6-4a63-8360-36763673490d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard libraries\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from importlib import reload\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Data handling and visualization\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# Interactive controls in Jupyter notebooks\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Libraries for specific tasks related to gns and pybpl\n",
    "import gns\n",
    "from gns import MODEL_SAVE_PATH\n",
    "from gns.inference.parsing import get_topK_parses\n",
    "from gns.omniglot.classification import ClassificationDataset\n",
    "from gns.rendering import Renderer as DefaultRenderer\n",
    "from gns.type import TypeModel\n",
    "from gns.utils.experiments import mkdir, time_string\n",
    "import pybpl\n",
    "from pybpl import splines, parameters\n",
    "from pybpl.util import nested_map\n",
    "from pybpl.util.stroke import dist_along_traj\n",
    "from pybpl.util.general import fspecial\n",
    "\n",
    "# Utility for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload gns\n",
    "reload(gns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870241d-73fb-455f-a8ef-c92f9b2bdaf3",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452c1b0-d79d-4c17-8738-b0b342180a09",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_images(probe, options):\n",
    "    # Open the probe image and the option images\n",
    "    probe_image = Image.open(probe)\n",
    "    option_images = [Image.open(img_path) for img_path in options]\n",
    "\n",
    "    # Create a figure with the probe and the 3x3 grid for the options directly below\n",
    "    fig = plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "\n",
    "    # Add the probe image to the top of the figure with a red border\n",
    "    ax_probe = fig.add_subplot(4, 3, (1, 3))  # Span the probe across the top 3 columns\n",
    "    ax_probe.imshow(probe_image)\n",
    "    ax_probe.axis('off')\n",
    "    rect = patches.Rectangle((0, 0), probe_image.width-1, probe_image.height-1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax_probe.add_patch(rect)\n",
    "\n",
    "    # Position the 3x3 grid of option images directly below the probe image\n",
    "    for index, img in enumerate(option_images):\n",
    "        row = (index // 3) + 1  # Calculate row in the 3x3 grid, starting directly below the probe\n",
    "        col = (index % 3) + 1   # Calculate column in the 3x3 grid\n",
    "        ax_option = fig.add_subplot(4, 3, row * 3 + col)  # Adjust grid position to directly follow the probe\n",
    "        ax_option.imshow(img)\n",
    "        ax_option.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5deda-c9f0-4733-a01b-3d077a96bafb",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval for zip files\n",
    "# @markdown\n",
    "\n",
    "def handle_file_operations(fname, url, expected_md5, extract_to='data'):\n",
    "    \"\"\"Handles downloading, verifying, and extracting a file.\"\"\"\n",
    "\n",
    "    # Define helper functions for download, verify, and extract operations\n",
    "    def download_file(url, filename):\n",
    "        \"\"\"Downloads file from the given URL and saves it locally.\"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(filename, \"wb\") as fid:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    fid.write(chunk)\n",
    "            print(\"Download successful.\")\n",
    "            return True\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"!!! Failed to download data: {e} !!!\")\n",
    "            return False\n",
    "\n",
    "    def verify_file_md5(filename, expected_md5):\n",
    "        \"\"\"Verifies the file's MD5 checksum.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filename, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        if hash_md5.hexdigest() == expected_md5:\n",
    "            print(\"MD5 checksum verified.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "            return False\n",
    "\n",
    "    def extract_zip_file(filename, extract_to):\n",
    "        \"\"\"Extracts the ZIP file to the specified directory.\"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"File extracted successfully to {extract_to}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"!!! The ZIP file is corrupted or not a zip file !!!\")\n",
    "\n",
    "    # Main operation\n",
    "    if not os.path.isfile(fname) or not verify_file_md5(fname, expected_md5):\n",
    "        if download_file(url, fname) and verify_file_md5(fname, expected_md5):\n",
    "            extract_zip_file(fname, extract_to)\n",
    "    else:\n",
    "        print(f\"File '{fname}' already exists and is verified. Proceeding to extraction.\")\n",
    "        extract_zip_file(fname, extract_to)\n",
    "\n",
    "# Example usage\n",
    "file_info = [\n",
    "    {\"fname\": \"one-shot-classification.zip\", \"url\": \"https://osf.io/aw6eq/download\", \"expected_md5\": \"9376412e7fb74d64f045644b51e641a6\"},\n",
    "    {\"fname\": \"omniglot-py.zip\", \"url\": \"https://osf.io/bazxp/download\", \"expected_md5\": \"f7a4011f5c25460c6d95ee1428e377ed\"},\n",
    "    {\"fname\": \"parses.zip\", \"url\": \"https://osf.io/97bpq/download\", \"expected_md5\": \"f9e42660d860d1f95296a3729e4bd2e3\"},\n",
    "    {\"fname\": \"targets.zip\", \"url\": \"https://osf.io/stk9f/download\", \"expected_md5\": \"38bbf4a87910fe56a02ee7a77a9ded3e\"}\n",
    "]\n",
    "\n",
    "for file in file_info:\n",
    "    handle_file_operations(**file)\n",
    "\n",
    "#Current directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3609707-5107-4b50-8b0a-a607890adadb",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def check_float_tesnor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def drawings_to_cpu(drawings):\n",
    "    if isinstance(drawings[0], list):\n",
    "        if drawings[0][0].is_cuda:\n",
    "            drawings = [[stk.cpu() for stk in drawing] for drawing in drawings]\n",
    "    elif drawings[0].is_cuda:\n",
    "        drawings = [stk.cpu() for stk in drawings]\n",
    "    return drawings\n",
    "\n",
    "def broaden_filter(a, b, device=None):\n",
    "    H = b*torch.tensor(\n",
    "        [[a/12, a/6, a/12],\n",
    "         [a/6, 1-a, a/6],\n",
    "         [a/12, a/6, a/12]],\n",
    "        dtype=torch.get_default_dtype(),\n",
    "        device=device\n",
    "    )\n",
    "    H = H[None, None]\n",
    "    return H\n",
    "\n",
    "def blur_filter(fsize, sigma, device=None):\n",
    "    H = fspecial(fsize, sigma, ftype='gaussian', device=device)\n",
    "    H = H[None,None]\n",
    "    return H\n",
    "\n",
    "def check_float_tensor(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        assert x.shape == ()\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        assert isinstance(x, float)\n",
    "    return x\n",
    "\n",
    "def select_random_images_within_alphabet(base_path, alphabet_path, exclude_character_path, num_images=8):\n",
    "    chosen_images = []\n",
    "    all_characters = [char for char in os.listdir(alphabet_path) if os.path.isdir(os.path.join(alphabet_path, char)) and os.path.join(alphabet_path, char) != exclude_character_path]\n",
    "    while len(chosen_images) < num_images:\n",
    "        if not all_characters:\n",
    "            break\n",
    "        character = random.choice(all_characters)\n",
    "        character_path = os.path.join(alphabet_path, character)\n",
    "        all_images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        if not all_images:\n",
    "            continue\n",
    "        image_file = random.choice(all_images)\n",
    "        image_path = os.path.join(character_path, image_file)\n",
    "        chosen_images.append(image_path)\n",
    "    return chosen_images\n",
    "\n",
    "def run_trial(base_path, num_trials):\n",
    "    for _ in range(num_trials):\n",
    "        languages = [lang for lang in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, lang))]\n",
    "        selected_language = random.choice(languages)\n",
    "        language_path = os.path.join(base_path, selected_language)\n",
    "        characters = [char for char in os.listdir(language_path) if os.path.isdir(os.path.join(language_path, char))]\n",
    "        selected_character = random.choice(characters)\n",
    "        character_path = os.path.join(language_path, selected_character)\n",
    "        images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        probe_image_path, correct_answer_image_path = random.sample(images, 2)\n",
    "        probe_image_path = os.path.join(character_path, probe_image_path)\n",
    "        correct_answer_image_path = os.path.join(character_path, correct_answer_image_path)\n",
    "        wrong_answers = select_random_images_within_alphabet(base_path, language_path, character_path, num_images=8)\n",
    "        options = wrong_answers\n",
    "        options.insert(random.randint(0, len(options)), correct_answer_image_path)\n",
    "        display_images(probe_image_path, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cf0b6-954a-414b-80b9-e356349bc934",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL and save it in the specified directory.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, filename)  # Ensure the file is saved in base_dir\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "\n",
    "def verify_checksum(filename, expected_checksum):\n",
    "    \"\"\"\n",
    "    Verify the MD5 checksum of a file\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the file\n",
    "    expected_checksum (str): Expected MD5 checksum\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the checksum matches, False otherwise\n",
    "    \"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            md5.update(chunk)\n",
    "\n",
    "    return md5.hexdigest() == expected_checksum\n",
    "\n",
    "def load_models(model_files, directory, map_location='cpu'):\n",
    "    \"\"\"\n",
    "    Load multiple models from a specified directory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for model_file in model_files:\n",
    "        full_path = os.path.join(directory, model_file)  # Correctly join paths\n",
    "        models[model_file] = torch.load(full_path, map_location=map_location)\n",
    "    return models\n",
    "\n",
    "def verify_models_in_destination(model_files, destination_directory):\n",
    "    \"\"\"\n",
    "    Verify the presence of model files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to verify.\n",
    "    destination_directory (str): The directory where the models are supposed to be.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all models are found in the directory, False otherwise.\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    for model_file in model_files:\n",
    "        # Construct the full path to where the model should be\n",
    "        full_path = os.path.join(destination_directory, model_file)\n",
    "        # Check if the model exists at the location\n",
    "        if not os.path.exists(full_path):\n",
    "            missing_files.append(model_file)\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"Missing model files in destination: {missing_files}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"All models are correctly located in the destination directory.\")\n",
    "        return True\n",
    "\n",
    "# URLs and checksums for the models\n",
    "models_info = {\n",
    "    'location_model.pt': ('https://osf.io/zmd7y/download', 'dfd51cf7c3a277777ad941c4fcc23813'),\n",
    "    'stroke_model.pt': ('https://osf.io/m6yc7/download', '511ea7bd12566245d5d11a85d5a0abb0'),\n",
    "    'terminate_model.pt': ('https://osf.io/dsmhc/download', '2f3e26cfcf36ce9f9172c15d8b1079d1')\n",
    "}\n",
    "\n",
    "destination_directory = base_dir\n",
    "\n",
    "# Define model_files based on the keys of models_info to ensure we have the filenames\n",
    "model_files = list(models_info.keys())\n",
    "\n",
    "# Iterate over the models to download and verify\n",
    "for model_name, (url, checksum) in models_info.items():\n",
    "    download_file(url, model_name)  # Downloads directly into base_dir\n",
    "    if verify_checksum(os.path.join(base_dir, model_name), checksum):\n",
    "        print(f\"Successfully verified {model_name}\")\n",
    "    else:\n",
    "        print(f\"Checksum does not match for {model_name}. Download might be corrupted.\")\n",
    "\n",
    "# Verify the presence of the models in the destination directory\n",
    "if verify_models_in_destination(model_files, destination_directory):\n",
    "    print(\"Verification successful: All models are in the correct directory.\")\n",
    "else:\n",
    "    print(\"Verification failed: Some models are missing from the destination directory.\")\n",
    "\n",
    "# Load the models from the destination directory\n",
    "models = load_models(model_files, destination_directory, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad64c08-fca7-492e-99d7-c614f9824bd2",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Activity 1\n",
    "\n",
    "#Students interact with a simple widget to perform the categorization task in Lake et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf54d-c1b2-4f10-8f70-a96e12620dee",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = \"data/omniglot-py/images_background\"\n",
    "\n",
    "# Running the trial\n",
    "run_trial(base_path, num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed022acf-0a4a-428b-af0a-8fa41c1787a6",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Reflection activity: How do you think you, as a human, are performing a task like Omniglot?\n",
    "# One-shot learning\n",
    "# Segmentation\n",
    "# Conditioned generation\n",
    "# Unconditioned generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346989d-6ac7-4d3d-ae37-67f92c36873a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Activity 2\n",
    "\n",
    "# We build a simple widget that uses the sample bottom up parsing script in example/classification/\n",
    "# to build alternative bottom up parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f7499-c4b4-47e6-8dfe-137333c33c7e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_score_fn(model, parses):\n",
    "    drawings = nested_map(lambda x: splines.get_stk_from_bspline(x), parses)\n",
    "    if torch.cuda.is_available():\n",
    "        drawings = nested_map(lambda x: x.cuda(), drawings)\n",
    "        parses = nested_map(lambda x: x.cuda(), parses)\n",
    "    losses = model.losses_fn(parses, drawings, filter_small=False, denormalize=True)\n",
    "    return -losses.cpu()\n",
    "\n",
    "def collect_img_results(img_id, parses, log_probs, reverse):\n",
    "    \"\"\"Collects parses and log probabilities without saving to disk.\"\"\"\n",
    "    appendix = 'test' if reverse else 'train'\n",
    "    data = {\n",
    "        'img_id': img_id,\n",
    "        'appendix': appendix,\n",
    "        'parses': parses,\n",
    "        'log_probs': log_probs,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_base_parses_in_memory(run_id, trials_per=800, reverse=False, dry_run=False):\n",
    "    print('run_id: %i' % run_id)\n",
    "    print('Loading model...')\n",
    "    # Correctly identify the base directory of the `gns` package\n",
    "    gns_base_dir = os.path.dirname(gns.__file__)\n",
    "\n",
    "    # Specify the model save directory within the `gns` package\n",
    "    model_save_path = os.path.join(gns_base_dir, 'model_saves')\n",
    "\n",
    "    # Initialize TypeModel\n",
    "    type_model = TypeModel().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        type_model = type_model.cuda()\n",
    "    score_fn = lambda parses: model_score_fn(type_model, parses)\n",
    "\n",
    "    # Use the dynamically determined path\n",
    "    osc_path = os.path.join(os.getcwd(), 'data/one-shot-classification')\n",
    "    print('Loading classification dataset...')\n",
    "    dataset = ClassificationDataset(osc_folder=osc_path)\n",
    "    run = dataset.runs[run_id]\n",
    "    imgs = run.test_imgs if reverse else run.train_imgs\n",
    "\n",
    "    collected_data = []  # In-memory storage for parses and log_probs\n",
    "\n",
    "    print('Collecting top-K parses for each train image...')\n",
    "    nimg = len(imgs)\n",
    "    for i in range(nimg):\n",
    "        start_time = time.time()\n",
    "        parses, log_probs = get_topK_parses(\n",
    "            imgs[i], k=5, score_fn=score_fn, configs_per=1,\n",
    "            trials_per=trials_per)\n",
    "        total_time = time.time() - start_time\n",
    "        print(f'image {i+1}/{nimg} took {time_string(total_time)}')\n",
    "        if dry_run:\n",
    "            continue\n",
    "        img_results = collect_img_results(i, parses, log_probs, reverse)\n",
    "        collected_data.append(img_results)\n",
    "\n",
    "    return collected_data\n",
    "\n",
    "# Example usage\n",
    "run_id = 10\n",
    "trials_per = 800\n",
    "reverse = False\n",
    "dry_run = False\n",
    "\n",
    "# Call the function\n",
    "collected_data = get_base_parses_in_memory(run_id=run_id, trials_per=trials_per, reverse=reverse, dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c406-e512-466e-b8ff-34acda11fc86",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Use retina mode for matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Function to visualize parses\n",
    "def visualize_parses(renderer, collected_data, item, run):\n",
    "    # Load the classification labels to map test images to their corresponding training images\n",
    "    with open(f'./data/one-shot-classification/all_runs/run{run+1:02}/class_labels.txt') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    test_to_train = {}\n",
    "    for line in data:\n",
    "        left, right = line.split(' ')\n",
    "        test_to_train[left.split('/')[-1]] = right.split('/')[-1].strip()\n",
    "\n",
    "    plt.figure(figsize=(12.5, 2.5))\n",
    "    train_im = test_to_train[f'item{item:02}.png']\n",
    "\n",
    "    # Reference image visualization\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(1 - plt.imread(f'./data/one-shot-classification/all_runs/run{run+1:02}/training/{train_im}'), cmap='gray')\n",
    "    plt.title('Reference image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Load parses from the collected data\n",
    "    for img_data in collected_data:\n",
    "        if img_data['img_id'] == item - 1:\n",
    "            parses = img_data['parses']\n",
    "            break\n",
    "\n",
    "    r = renderer()\n",
    "    # Visualization of parses\n",
    "    for i, parse in enumerate(parses):\n",
    "        drawings = [splines.get_stk_from_bspline(x) for x in parse]\n",
    "        plt.subplot(1, 6, i+2)\n",
    "        plt.imshow(r(drawings), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Parse {i+1}')\n",
    "\n",
    "        colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
    "        for i, d in enumerate(drawings):\n",
    "            plt.plot(d[:, 0], -d[:, 1], colors[i], linewidth=1)\n",
    "            plt.plot(d[0, 0], -d[0, 1], 'w.', markersize=20, fillstyle='none')\n",
    "            plt.plot(d[-1, 0], -d[-1, 1], 'ws', markersize=10, fillstyle='none')\n",
    "\n",
    "# Example of visualizing parses for a specific item and run\n",
    "visualize_parses(DefaultRenderer, collected_data, item=9, run=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae5308-6cd4-4f98-a5a7-6fcbb336ba14",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Activity 3\n",
    "\n",
    "# Build parts of the forward model necessary to build a differentiable renderer for a single stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6c02b-77a4-4676-b1d7-fc42f1150716",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Parameters\n",
    "# @markdown\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # Library to use\n",
    "        self.libname = 'library'\n",
    "        self.set_rendering_params()\n",
    "        self.set_spline_params()\n",
    "        self.set_image_model_params()\n",
    "        self.set_mcmc_params()\n",
    "        self.set_search_params()\n",
    "\n",
    "    def set_rendering_params(self):\n",
    "        self.imsize = torch.Size([105, 105]) # image size\n",
    "\n",
    "        ## ink-add parameters\n",
    "        self.ink_pp = 2. # amount of ink per point\n",
    "        self.ink_max_dist = 2. # distance between points to which you get full ink\n",
    "\n",
    "        ## broadening parameters\n",
    "        self.ink_ncon = 2 # number of convolutions\n",
    "        self.ink_a = 0.5 # parameter 1\n",
    "        self.ink_b = 6. # parameter 2\n",
    "        self.broaden_mode = 'Lake' # broadening version (must be either \"Lake\" or \"Hinton\")\n",
    "\n",
    "        ## blurring parameters\n",
    "        self.fsize = 11 # convolution size for blurring\n",
    "\n",
    "    def set_spline_params(self):\n",
    "        \"\"\"\n",
    "        Parameters for creating a trajectory from a spline\n",
    "        \"\"\"\n",
    "        self.spline_max_neval = 200 # maxmium number of evaluations\n",
    "        self.spline_min_neval = 10 # minimum number of evaluations\n",
    "        self.spline_grain = 1.5 # 1 trajectory point for every this many units pixel distance\n",
    "\n",
    "    def set_image_model_params(self):\n",
    "        \"\"\"\n",
    "        Max/min noise parameters for image model\n",
    "        \"\"\"\n",
    "        self.max_blur_sigma = torch.tensor(16, dtype=torch.float) # min/max blur sigma\n",
    "        self.min_blur_sigma = torch.tensor(0.5, dtype=torch.float) # min/max blur sigma\n",
    "        self.max_epsilon = torch.tensor(0.5, dtype=torch.float) # min/max pixel epsilon\n",
    "        self.min_epsilon = torch.tensor(1e-4, dtype=torch.float) # min/max pixel epsilon\n",
    "\n",
    "    def set_mcmc_params(self):\n",
    "        \"\"\"\n",
    "        MCMC parameters\n",
    "        \"\"\"\n",
    "        ## chain parameters\n",
    "        self.mcmc_nsamp_type_chain = 200 # number of samples to take in the MCMC chain (for classif.)\n",
    "        self.mcmc_nsamp_type_store = 10 # number of samples to store from this chain (for classif.)\n",
    "        self.mcmc_nsamp_token_chain = 25 # for completion (we take last sample in this chain)\n",
    "\n",
    "        ## mcmc proposal parameters\n",
    "        self.mcmc_prop_gpos_sd = 1 # global position move\n",
    "        self.mcmc_prop_shape_sd = 3/2 # shape move\n",
    "        self.mcmc_prop_scale_sd = 0.0235 # scale move\n",
    "        self.mcmc_prop_relmid_sd = 0.2168 # attach relation move\n",
    "        self.mcmc_prop_relpos_mlty = 2 # multiply the sd of the standard position noise by this to propose new positions from prior\n",
    "\n",
    "    def set_search_params(self):\n",
    "        \"\"\"\n",
    "        Parameters of search algorithm (part of inference)\n",
    "        \"\"\"\n",
    "        self.K = 5 # number of particles to use in search algorithm\n",
    "        self.max_affine_scale_change = 2 # scale changes must be less than a factor of 2\n",
    "        self.max_affine_shift_change = 50 # shift changes must less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82efd4d-06eb-43ce-a67d-0f1d1afb73e1",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Broaden and Blur\n",
    "# @markdown\n",
    "\n",
    "class BroadenAndBlur(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        if blur_fsize is None:\n",
    "            blur_fsize = PM.fsize\n",
    "        assert blur_fsize % 2 == 1, 'blur conv filter size must be odd'\n",
    "        self.register_buffer('H_broaden', broaden_filter(PM.ink_a, PM.ink_b))\n",
    "        self.register_buffer('H_blur', blur_filter(blur_fsize, blur_sigma))\n",
    "        self.nbroad = PM.ink_ncon\n",
    "        self.blur_pad = blur_fsize//2\n",
    "        self.blur_sigma = blur_sigma\n",
    "        self.blur_fsize = blur_fsize\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.H_broaden.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.H_broaden.is_cuda\n",
    "\n",
    "    def forward(self, x, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] pre-conv image probabilities\n",
    "        blur_sigma : float | None\n",
    "            amount of blur. 'None' means use value from __init__ call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor\n",
    "            [n,H,W] post-conv image probabilities\n",
    "        \"\"\"\n",
    "        if self.is_cuda:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if blur_sigma is None:\n",
    "            H_blur = self.H_blur\n",
    "            blur_sigma = self.blur_sigma\n",
    "        else:\n",
    "            blur_sigma = check_float_tesnor(blur_sigma, self.device)\n",
    "            H_blur = blur_filter(self.blur_fsize, blur_sigma, device=self.device)\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        else:\n",
    "            epsilon = check_float_tesnor(epsilon, self.device)\n",
    "\n",
    "        # unsqueeze\n",
    "        x = x.unsqueeze(1)\n",
    "        # apply broaden\n",
    "        for i in range(self.nbroad):\n",
    "            x = F.conv2d(x, self.H_broaden, padding=1)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # return if no blur\n",
    "        if blur_sigma == 0:\n",
    "            x = x.squeeze(1)\n",
    "            return x\n",
    "        # apply blur\n",
    "        for i in range(2):\n",
    "            x = F.conv2d(x, H_blur, padding=self.blur_pad)\n",
    "        x = F.hardtanh(x, 0., 1.)\n",
    "        # apply pixel noise\n",
    "        if epsilon > 0:\n",
    "            x = (1-epsilon)*x + epsilon*(1-x)\n",
    "        # squeeze\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9321991-a37a-4173-bab0-44175b7e13ba",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Hidden methods for Renderer and Painter\n",
    "# @markdown\n",
    "\n",
    "def cuda(self, device=None):\n",
    "    self.painter = self.painter.cpu()\n",
    "    self.broaden_and_blur = self.broaden_and_blur.cuda(device)\n",
    "    return self\n",
    "\n",
    "def forward_partial_r(self, drawing, blur_sigma=None, epsilon=None, concat=False):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    if isinstance(drawing[0], list):\n",
    "        pimgs = [self.painter.forward_partial(d) for d in drawing]\n",
    "        lengths = [len(p) for p in pimgs]\n",
    "        pimgs = torch.cat(pimgs)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        if concat:\n",
    "            return pimgs\n",
    "        pimgs = torch.split(pimgs, lengths, 0)\n",
    "        return list(pimgs)\n",
    "    else:\n",
    "        pimgs = self.painter.forward_partial(drawing)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon)\n",
    "        return pimgs\n",
    "\n",
    "def forward_partial_p(self, drawing):\n",
    "    \"\"\"\n",
    "    In this version, we include all partial canvas renders in addition\n",
    "    to the final renders\n",
    "    \"\"\"\n",
    "    assert not self.is_cuda\n",
    "    drawing = drawings_to_cpu(drawing)\n",
    "    ns = len(drawing)\n",
    "    pimgs = torch.zeros(ns+1, *self.imsize)\n",
    "    canvas = torch.zeros(*self.imsize)\n",
    "    for i, stk in enumerate(drawing):\n",
    "        canvas, _ = self.add_stroke(canvas, stk)\n",
    "        pimgs[i+1] = canvas\n",
    "    return pimgs\n",
    "\n",
    "def check_bounds(self, myt):\n",
    "    xt = myt[:,0]\n",
    "    yt = myt[:,1]\n",
    "    x_out = (torch.floor(xt) < 0) | (torch.ceil(xt) >= self.imsize[0])\n",
    "    y_out = (torch.floor(yt) < 0) | (torch.ceil(yt) >= self.imsize[1])\n",
    "    out = x_out | y_out\n",
    "    return out\n",
    "\n",
    "def seqadd(self, D, lind_x, lind_y, inkval):\n",
    "    lind = self.index_mat[lind_x.long(), lind_y.long()]\n",
    "    D = D.view(-1)\n",
    "    D = D.scatter_add(0, lind, inkval)\n",
    "    D = D.view(self.imsize)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae763d6-dc78-4e5e-9ae9-233dd53d384b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        pimgs = ...\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = ...\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            pimgs[i] = ...\n",
    "        return pimgs\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "# Example of visualizing parses for a specific item and run\n",
    "\n",
    "# Add hidden methods (otherwise code would have been too long)\n",
    "#Renderer.cuda = cuda\n",
    "#Renderer.forward_partial_r = forward_partial_r\n",
    "#Painter.forward_partial_p = forward_partial_p\n",
    "#Painter.check_bounds = check_bounds\n",
    "#Painter.seq_add = seq_add\n",
    "\n",
    "# Now create instances of Renderer and Painter\n",
    "#renderer_instance = Renderer()\n",
    "#painter_instance = Painter()\n",
    "\n",
    "#visualize_parses(Renderer, collected_data, item=9, run=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60529a18-f91c-446a-8926-70e16ec7ed8c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to remove solution\n",
    "\n",
    "class Renderer(nn.Module):\n",
    "    def __init__(self, blur_sigma=0.5, epsilon=0., blur_fsize=None, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "\n",
    "        self.painter = Painter(PM)\n",
    "        self.broaden_and_blur = BroadenAndBlur(blur_sigma, epsilon, blur_fsize, PM)\n",
    "\n",
    "    def forward(self, drawings, blur_sigma=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Render each drawing by converting the drawing to image ink\n",
    "        and then applying broaden & blur filters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drawings : list[list[torch.Tensor]] | list[torch.Tensor]\n",
    "            Input drawings. Each drawing is a list of tensors\n",
    "        blur_sigma : float | None\n",
    "            Sigma parameter for blurring. Only used for adaptive blurring.\n",
    "            Default 'None' means use the blur_sigma from __init__() call\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pimgs : torch.Tensor\n",
    "            [n,H,W] Pre-conv image probabilities\n",
    "        \"\"\"\n",
    "        # draw the strokes (this part on cpu)\n",
    "        if not isinstance(drawings[0], list):\n",
    "            single = True\n",
    "            drawings = [drawings]\n",
    "        else:\n",
    "            single = False\n",
    "\n",
    "        pimgs = self.painter(drawings)\n",
    "        pimgs = self.broaden_and_blur(pimgs, blur_sigma, epsilon) # (n,H,W)\n",
    "\n",
    "        if single:\n",
    "            pimgs = pimgs[0]\n",
    "\n",
    "        return pimgs\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, PM=None):\n",
    "        super().__init__()\n",
    "        if PM is None:\n",
    "            PM = Parameters()\n",
    "        self.ink_pp = PM.ink_pp\n",
    "        self.ink_max_dist = PM.ink_max_dist\n",
    "        self.register_buffer('index_mat',\n",
    "                             torch.arange(PM.imsize[0]*PM.imsize[1]).view(PM.imsize))\n",
    "        self.register_buffer('space_flip', torch.tensor([-1.,1.]))\n",
    "        self.imsize = PM.imsize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.index_mat.device\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return self.index_mat.is_cuda\n",
    "\n",
    "    def space_motor_to_img(self, stk):\n",
    "        return torch.flip(stk, dims=[-1])*self.space_flip\n",
    "\n",
    "    def add_stroke(self, pimg, stk):\n",
    "        stk = self.space_motor_to_img(stk)\n",
    "        # reduce trajectory to only those points that are in bounds\n",
    "        out = self.check_bounds(stk) # boolean; shape (neval,)\n",
    "        ink_off_page = out.any()\n",
    "        if out.all():\n",
    "            return pimg, ink_off_page\n",
    "        stk = stk[~out]\n",
    "\n",
    "        # compute distance between each trajectory point and the next one\n",
    "        if stk.shape[0] == 1:\n",
    "            myink = stk.new_tensor(self.ink_pp)\n",
    "        else:\n",
    "            dist = torch.norm(stk[1:] - stk[:-1], dim=-1) # shape (k,)\n",
    "            dist = dist.clamp(None, self.ink_max_dist)\n",
    "            dist = torch.cat([dist[:1], dist])\n",
    "            myink = (self.ink_pp/self.ink_max_dist)*dist # shape (k,)\n",
    "\n",
    "        # make sure we have the minimum amount of ink, if a particular\n",
    "        # trajectory is very small\n",
    "        sumink = torch.sum(myink)\n",
    "        if sumink < 2.22e-6:\n",
    "            nink = myink.shape[0]\n",
    "            myink = (self.ink_pp/nink)*torch.ones_like(myink)\n",
    "        elif sumink < self.ink_pp:\n",
    "            myink = (self.ink_pp/sumink)*myink\n",
    "        assert torch.sum(myink) > (self.ink_pp - 1e-4)\n",
    "\n",
    "        # share ink with the neighboring 4 pixels\n",
    "        x = stk[:,0]\n",
    "        y = stk[:,1]\n",
    "        xfloor = torch.floor(x).detach()\n",
    "        yfloor = torch.floor(y).detach()\n",
    "        xceil = torch.ceil(x).detach()\n",
    "        yceil = torch.ceil(y).detach()\n",
    "        x_c_ratio = x - xfloor\n",
    "        y_c_ratio = y - yfloor\n",
    "        x_f_ratio = 1 - x_c_ratio\n",
    "        y_f_ratio = 1 - y_c_ratio\n",
    "        lind_x = torch.cat([xfloor, xceil, xfloor, xceil])\n",
    "        lind_y = torch.cat([yfloor, yfloor, yceil, yceil])\n",
    "        inkval = torch.cat([\n",
    "            myink*x_f_ratio*y_f_ratio,\n",
    "            myink*x_c_ratio*y_f_ratio,\n",
    "            myink*x_f_ratio*y_c_ratio,\n",
    "            myink*x_c_ratio*y_c_ratio\n",
    "        ])\n",
    "        # paint the image\n",
    "        pimg = self.seqadd(pimg, lind_x, lind_y, inkval)\n",
    "        return pimg, ink_off_page\n",
    "\n",
    "    def draw(self, pimg, strokes):\n",
    "        for stk in strokes:\n",
    "            pimg, _ = self.add_stroke(pimg, stk)\n",
    "        return pimg\n",
    "\n",
    "    def forward(self, drawings):\n",
    "        assert not self.is_cuda\n",
    "        drawings = drawings_to_cpu(drawings)\n",
    "        n = len(drawings)\n",
    "        pimgs = torch.zeros(n, *self.imsize)\n",
    "        for i in range(n):\n",
    "            pimgs[i] = self.draw(pimgs[i], drawings[i])\n",
    "        return pimgs\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "# Example of visualizing parses for a specific item and run\n",
    "\n",
    "# Add hidden methods (otherwise code would have been too long)\n",
    "#Renderer.cuda = cuda\n",
    "#Renderer.forward_partial_r = forward_partial_r\n",
    "#Painter.forward_partial_p = forward_partial_p\n",
    "#Painter.check_bounds = check_bounds\n",
    "#Painter.seqadd = seqadd\n",
    "\n",
    "# Now create instances of Renderer and Painter\n",
    "#renderer_instance = Renderer()\n",
    "#painter_instance = Painter()\n",
    "\n",
    "#visualize_parses(Renderer, collected_data, item=9, run=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial3",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
