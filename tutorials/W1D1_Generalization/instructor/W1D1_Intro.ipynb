{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1764495c-fa85-46f1-a72b-1ba805d48e8a",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "source": [
    "# W1D1 Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e229713-7d0c-4ac9-afd1-b935808fec00",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835db789-8d58-4cff-b212-4c26726380c5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Prerequisite knowledge\n",
    "\n",
    "This is the first day, so we assume you have prerequisites equivalent to having taken the CN and DL classes. This day’s tutorials delve into next-token prediction of a neural network with transformers in PyTorch. We will also talk about attention in this context. You should be familiar from DL W2D5, W3D1. You will work with neural data (actually EMG) in the second tutorial. You should be broadly familiar from the CN class. Please review these precourse materials if necessary!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef995ca-7561-4221-8c56-b1ea07bd288c",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_cn\",\n",
    "            \"user_key\": \"y1x3mpx5\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D1_Intro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab3846-0223-4914-af4a-f347920ee4c6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Video\n",
    "\n",
    "Hi everyone, and welcome to the opening session of our tutorial series at the Neuromatch Academy. My name is Samuele Bolotta, I am from Italy and I am the curriculum specialist in the NeuroAI Course. I have a master's in neuroscience and a master's in artificial intelligence. In terms of my personal life, my main hobbies right now are powerlifting and hiking. I am excited to be one of the educators guiding you through an exploration of NeuroAI. Importantly, I want to highlight that this was a collective effort. Our team of content creators, reviewers, production editors, and collaborators has worked hard to put together the educational content we'll be covering.\n",
    "\n",
    "Today, we'll explore the concept of Generalization in Artificial Intelligence (AI). Generalization is the ability of AI systems to perform well on new, unseen data across various real-world contexts. Industrial applications of machine learning often use flexible architectures with weak inductive biases which are trained on large, diverse datasets.\n",
    "\n",
    "During the Deep Learning Course, specifically on Week 2, Day 5 (W2D5), we discussed Transformers. Introduced by Vaswani et al. in 2017, Transformers are particularly noted for their high parallelizability, which facilitates efficient training on a large scale. This efficiency, in turn, has encouraged the development of modern accelerators like GPUs and TPUs which are tuned for transformers, thus creating a prime example of the so-called “hardware lottery”. Transformers are used for a wide range of AI tasks, from natural language processing, to vision, and to robotics. They start by modeling the input as a set of tokens; in natural language processing, tokens can be parts of words; in vision, image patches. These tokens then interact in multiple stages through an attention mechanism. Finally, the output of the network can be read in a variety of ways to perform classification, regression, or autoregressive generation.\n",
    "\n",
    "Let’s talk about the inductive biases in transformers. Transformers display permutation equivariance, meaning the model's output does not change if the order of the input data is altered. That might seem surprising, since transformers are often used for modelling sequences, like sentences, where matters order! Sensitivity to position is recovered by using position encoding, which tags each token with its position in a sequence or grid. Moreover, central to the Transformer architecture is the attention mechanism. This feature allows the model to selectively concentrate on various parts of the input data. This capability is critical for the model's overall performance, a topic we will delve deeper into on Day 4. High-capacity transformers have a remarkable ability to 'learn' inductive biases from data exposure and augmentations, enhancing their generalization capabilities. In practice, achieving generalization often involves training on a broad spectrum of data. The extensive datasets available today enable models like Transformers to be exposed to far more data than single humans or animals in their lifetimes.\n",
    "\n",
    "To contextualize these concepts, we will examine the problem of recognizing and generating handwritten text. This example illuminates how the architecture, training dataset, and data augmentations converge to empower a model to accomplish a complex task. Handwriting recognition serves as a case study for discussing how AI, neuroscience, and cognitive science conceptualize and tackle problems. We will dissect the transformer architecture, focusing on its tokenization, attention mechanisms, and classification heads. Additionally, we'll introduce scaling laws—crucial for grasping how model performance improves with larger datasets and increased computational resources. Expanding further, we'll explore transfer learning, augmentation techniques, and the use of synthetic data.\n",
    "By the end of this session, you will have gotten a broad overview of generalization in the context of AI and practical experience in applying these concepts. Let's dive in!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c633817-98cd-4817-aa74-cce6792f5f2e",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'W5o_HTsef0I'), ('Bilibili', 'BV1ho4y1C7Eo')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6df3b-e096-40cc-ad84-f83de132bbfa",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d94cf-5a3e-44ac-b6bc-0c350fe66533",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "link_id = \"rbx2a\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dbc70-f36b-446b-acfa-5b7df84c1f10",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Intro_Video\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Intro",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
