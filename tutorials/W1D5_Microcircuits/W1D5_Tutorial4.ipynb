{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Bonus Material\n",
    "\n",
    "**Week 1, Day 5: Microcircuits**\n",
    "\n",
    "By Neuromatch Academy\n",
    "\n",
    "**Content creators**: Noga Mudrik, Xaq Pitkow\n",
    "\n",
    "**Content reviewers**: Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk, Patrick Mineault, Alex Murphy\n",
    "\n",
    "**Production editors**: Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk, Alex Murphy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "We will continue to explore the notion of sparsity, but this time in relation to the spatial domain. If you recall from Tutorial 1, there we were examining the temporal dimension primarily. Our exercises on spatial sparsity and spatial differentiation will guide you to view the notion of sparsity through a slightly different lens. We hope this alternate perspective can further reinforce how sparsity is a key component driving generalization and how this works in both the brain and in AI models.\n",
    "\n",
    "The sections below contain bonus exercises for the following tutorials you have seen today.\n",
    "\n",
    "* Section 1: Sparsity\n",
    "* Section 2: Sparsity\n",
    "* Section 3: Normalization\n",
    "\n",
    "\n",
    "With all that said, let's get to work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D5_T4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "#working with data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "#interactive display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "#modeling\n",
    "from sklearn.datasets import make_sparse_coded_signal, make_regression\n",
    "from sklearn.decomposition import DictionaryLearning, PCA\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "import tensorflow as tf\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "sns.set_context('talk')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus Section 1: Sparsity in space\n",
    "\n",
    "In this section, we will focus on sparsity not from the perspective of the temporal domain (which we saw in the main tutorial) but rather from the spatial domain, how sparsity between units can be quantified and measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Bonus Video 1: Sparsity in space\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'bGH4NiD1Ye4'), ('Bilibili', 'BV1sT421a7MX')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_sparsity_in_space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Coding Excercise 1: Hard thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the 1st frame, which we will focus on for the spatial differentiation (i.e., differentiation in space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization of the frame\n",
    "\n",
    "frame = np.load('frame1.npy')\n",
    "plot_images([frame])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have focused on applying ReLU, which can also be considered as a form of **'soft thresholding'**. Now, we will apply a different kind of thresholding called '**hard thresholding**'.\n",
    "\n",
    "Here, you will have to write a function called **hard_thres** that receives 2 inputs:\n",
    "\n",
    "1) 'frame' - 2d array as input of $p \\times p$ floats.\n",
    "\n",
    "2) 'theta' - threshold scalar values.\n",
    "\n",
    "The function has to return a 2d array called **frame_HT** with the same dimensions as of **frame**, however, all values smaller than $\\theta$ need to be set to 0, i.e.:\n",
    "\n",
    " $$\n",
    "\\text{frame-HT}_{[i,j]} =\n",
    "\\begin{cases}\n",
    "frame_{[i,j]} & \\text{if } frame_{[i,j]} \\geq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for every pixel [i,j] in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_thres(frame, theta):\n",
    "    \"\"\"\n",
    "    Return a hard thresholded array of values based on the parameter value theta.\n",
    "\n",
    "    Inputs:\n",
    "    - frame (np.array): 2D signal.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    ## Fill out the following then remove.\n",
    "    raise NotImplementedError(\"Student exercise: complete hard thresholding function.\")\n",
    "    ###################################################################\n",
    "    frame_HT = frame.copy()\n",
    "    frame_HT[...] = 0\n",
    "    return frame_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "def hard_thres(frame, theta):\n",
    "    \"\"\"\n",
    "    Return a hard thresholded array of values based on the parameter value theta.\n",
    "\n",
    "    Inputs:\n",
    "    - frame (np.array): 2D signal.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "    \"\"\"\n",
    "    frame_HT = frame.copy()\n",
    "    frame_HT[frame_HT < theta] = 0\n",
    "    return frame_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test your function\n",
    "\n",
    "frame_HT = hard_thres(frame, 150)\n",
    "plot_images([frame, frame_HT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how many pixels became of the same **violet** color. Above, the violet color corresponds to 0. Hence, these pixels are the **thresholded** ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define the variable **frame_HT** as the first frame after the hard threshold operator, using the above **hard_thres** function with a threshold of 80% of the frame values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Observe 80% of thresholded signal\n",
    "\n",
    "low_perc = np.percentile(frame, 80)\n",
    "frame_HT = hard_thres(frame, low_perc)\n",
    "plot_images([frame, frame_HT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even more pixels become violet as we threshold the vast majority of the signal (80%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore the differences before and after applying hard thresholding using the histogram. Generate a histogram for the `frame_HT`. It might take a while as the image is of high quality and it contains a lot of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Histogram comparison\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(1,2,figsize = (15,5), sharey = True)\n",
    "    axs[0].hist(frame.flatten(), bins = 100);\n",
    "    axs[1].hist(frame_HT.flatten(), bins = 100);\n",
    "\n",
    "    #utils\n",
    "    [ax.set_yscale('log') for ax in axs]\n",
    "    [remove_edges(ax) for ax in axs]\n",
    "    [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "    [ax.set_title(title) for title, ax in zip(['Before Thresholding', 'After Thresholding'], axs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also take a look at the kurtosis to assess sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Kurtosis values comparison\n",
    "\n",
    "plot_labeled_kurtosis(frame, frame_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_hard_thresholding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus Section 2: Spatial differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will apply differentiation operations to spatial data. Spatial differentiation is common in identifying image patterns and extracting meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Bonus Video 2: Spatial differentiation\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'IpMDGo7WYpM'), ('Bilibili', 'BV1nS411N7fK')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_spatial_differentiation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Coding Exercise 2: Spatial filtering\n",
    "\n",
    "Let's apply spatial differentiation on the 1st frame to look at the result.\n",
    "\n",
    "Below, apply 1-frame spatial differentiation on **frame**.\n",
    "\n",
    "Define a variable **diff_x** that stores the x-axis differentiation and **diff_y** that stores the y-axis differentiation. Compare the results by presenting the differentiated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `diff_x` and `diff_y`.\")\n",
    "###################################################################\n",
    "diff_x = np.diff(frame, axis = ...)\n",
    "diff_y = np.diff(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "diff_x = np.diff(frame, axis = 1)\n",
    "diff_y = np.diff(frame, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Observe the result\n",
    "plot_spatial_diff(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as usual, let's look at the histogram and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Observe the histogram\n",
    "plot_spatial_histogram(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Observe the kurtosis values\n",
    "plot_spatial_kurtosis(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Analysis in Spatial Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than focusing on differences between adjacent pixels, we'll employ more comprehensive filters for spatial differencing. Specifically, we will apply the differentiation using the `diff_box` method, which is similar to our approach in the temporal case. Define the `diff_box_values_y` variable for the specified windows for the first frame in both `x` and `y' directions.\n",
    "\n",
    "In contrast to the temporal differencing, we aim for the `diff_box` to be symmetric in the spatial case. This is important because, in spatial analysis, we need to consider the context from both sides of a given point, as opposed to time-series data, where there is a clear directional flow from past to present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison of filters.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/static/filters.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **symmetric** box filter in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_box_spatial(data, window, pad_size = 4):\n",
    "    \"\"\"\n",
    "    Implement & apply spatial filter on the signal.\n",
    "\n",
    "    Inputs:\n",
    "    - data (np.ndarray): input signal.\n",
    "    - window (int): size of the window.\n",
    "    - pad_size (int, default = 4): size of pad around data.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    ## Fill out the following then remove\n",
    "    raise NotImplementedError(\"Student exercise: add pad around to complete filter operation.\")\n",
    "    ###################################################################\n",
    "    filter = np.concatenate([np.repeat(0, ...), np.repeat(-1, window), np.array([1]), np.repeat(-1, window), np.repeat(0, ...)]).astype(float)\n",
    "\n",
    "    #normalize\n",
    "    filter /= np.sum(filter**2)**0.5\n",
    "\n",
    "    #make sure the filter sums to 0\n",
    "    filter_plus_sum =  filter[filter > 0].sum()\n",
    "    filter_min_sum = np.abs(filter[filter < 0]).sum()\n",
    "    filter[filter > 0] *= filter_min_sum/filter_plus_sum\n",
    "\n",
    "    #convolution of the signal with the filter\n",
    "    diff_box = np.convolve(data, filter, mode='full')[:len(data)]\n",
    "    diff_box[:window] = diff_box[window]\n",
    "    return diff_box, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "def diff_box_spatial(data, window, pad_size = 4):\n",
    "    \"\"\"\n",
    "    Implement & apply spatial filter on the signal.\n",
    "\n",
    "    Inputs:\n",
    "    - data (np.ndarray): input signal.\n",
    "    - window (int): size of the window.\n",
    "    - pad_size (int, default = 4): size of pad around data.\n",
    "    \"\"\"\n",
    "    filter = np.concatenate([np.repeat(0, pad_size), np.repeat(-1, window), np.array([1]), np.repeat(-1, window), np.repeat(0, pad_size)]).astype(float)\n",
    "\n",
    "    #normalize\n",
    "    filter /= np.sum(filter**2)**0.5\n",
    "\n",
    "    #make sure the filter sums to 0\n",
    "    filter_plus_sum =  filter[filter > 0].sum()\n",
    "    filter_min_sum = np.abs(filter[filter < 0]).sum()\n",
    "    filter[filter > 0] *= filter_min_sum/filter_plus_sum\n",
    "\n",
    "    #convolution of the signal with the filter\n",
    "    diff_box = np.convolve(data, filter, mode='full')[:len(data)]\n",
    "    diff_box[:window] = diff_box[window]\n",
    "    return diff_box, filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the filter. How it differs from the temporal one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization of the filter\n",
    "window = 10\n",
    "diff_box_signal, filter = diff_box_spatial(sig, window)\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_e1 = np.arange(len(filter))\n",
    "    plot_e2 = np.arange(len(filter)) + 1\n",
    "    plot_edge_mean = 0.5*(plot_e1 + plot_e2)\n",
    "    plot_edge = lists2list( [[e1 , e2] for e1 , e2 in zip(plot_e1, plot_e2)])\n",
    "    ax.scatter(plot_edge_mean, filter, color = 'purple')\n",
    "    ax.plot(plot_edge, np.repeat(filter, 2), alpha = 0.3, color = 'purple')\n",
    "    add_labels(ax,ylabel = 'Filter value', title = 'Box Filter', xlabel = 'Space (pixel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the filter to the data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove.\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `diff_box_values_y`.\")\n",
    "###################################################################\n",
    "\n",
    "num_winds = 10\n",
    "windows = np.linspace(2,92,num_winds)\n",
    "rows = frame.shape[0]\n",
    "cols = frame.shape[1]\n",
    "\n",
    "diff_box_values_x = [np.array([diff_box_spatial(frame[row], int(window))[0]  for row in range(rows)]) for window in windows]\n",
    "\n",
    "diff_box_values_y = [np.array([diff_box_spatial(frame[:, ...], int(window))[0] for col in range(cols)]) for window in windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "num_winds = 10\n",
    "windows = np.linspace(2,92,num_winds)\n",
    "rows = frame.shape[0]\n",
    "cols = frame.shape[1]\n",
    "\n",
    "diff_box_values_x = [np.array([diff_box_spatial(frame[row], int(window))[0]  for row in range(rows)]) for window in windows]\n",
    "\n",
    "diff_box_values_y = [np.array([diff_box_spatial(frame[:, col], int(window))[0] for col in range(cols)]) for window in windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Observe the results\n",
    "\n",
    "visualize_images_diff_box(frame, diff_box_values_x, diff_box_values_y, num_winds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Kurtosis values comparison\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(figsize = (17,7))\n",
    "    tauskur = [kurtosis(np.abs(frame - diff_box_values_i).flatten()) for diff_box_values_i in diff_box_values_x]\n",
    "    tauskur_y = [kurtosis(np.abs(frame.T - diff_box_values_i).flatten()) for diff_box_values_i in diff_box_values_y]\n",
    "\n",
    "    df1 = pd.DataFrame([kurtosis(sig)] + tauskur, index = ['Signal']+ ['$window = {%d}$'%tau for tau in windows], columns = ['kurtosis x'])\n",
    "    df2 = pd.DataFrame([kurtosis(sig)] + tauskur_y, index = ['Signal']+ ['$window = {%d}$'%tau for tau in windows], columns = ['kurtosis y'])\n",
    "    dfs = pd.concat([df1,df2], axis = 1)\n",
    "    dfs.plot.barh(ax = ax, alpha = 0.5, color =['purple', 'pink'])\n",
    "    remove_edges(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_spatial_filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus Section 3: Benefits of using normalization - Efficient Coding\n",
    "\n",
    "This bonus material explores some of the ideas that were introduced in Tutorial 2 on **Normalization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linearities are critical for computation, but they can also lose information. We propose you look at a very simple example of how normalization can help preserve information through a network. In the exercise below, complete `HardTanh` and functions `LeakyHardTanh` (observe that the inverse of the latter is already here for you).\n",
    "\n",
    "`HardTanh` is the function $f(x)$ which is defined as following:\n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "1, & \\text{if } x > 1\\\\\n",
    "x, & \\text{if } -1 \\leq x \\leq 1\\\\\n",
    "-1, & \\text{if } x < -1\n",
    "\\end{cases}$$\n",
    "\n",
    "while `LeakyHardTanh` is $f(x) = \\text{HardTanh}(x) + \\text{leak-slope}* x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO: Implement the normalization example equation ##\n",
    "# Fill remove the following line of code once you have completed the exercise:\n",
    "raise NotImplementedError(\"Student exercise: complete missing calculations in `HardTanh` and `LeakyHardTanh` functions.\")\n",
    "#################################################\n",
    "\n",
    "def HardTanh(x):\n",
    "  \"\"\"\n",
    "  Calculate `tanh` output for the given input data.\n",
    "\n",
    "  Inputs:\n",
    "  - x (np.ndarray): input data.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.ndarray): `tanh(x)`.\n",
    "  \"\"\"\n",
    "  min_val = -1\n",
    "  max_val = 1\n",
    "  output = np.copy(x)\n",
    "  output[output>...] = ...\n",
    "  output[output<...] = ...\n",
    "  return output\n",
    "\n",
    "def LeakyHardTanh(x, leak_slope=0.03):\n",
    "  \"\"\"\n",
    "  Calculate `tanh` output for the given input data with the leaky term.\n",
    "\n",
    "  Inputs:\n",
    "  - x (np.ndarray): input data.\n",
    "  - leak_slope (float, default = 0.03): leaky term.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.ndarray): `tanh(x)`.\n",
    "  \"\"\"\n",
    "  output = np.copy(x)\n",
    "  output = HardTanh(output) + ...*...\n",
    "  return output\n",
    "\n",
    "def InverseLeakyHardTanh(y, leak_slope=0.03):\n",
    "  \"\"\"\n",
    "  Calculate input into the `tanh` function with the leaky term for the given output.\n",
    "\n",
    "  Inputs:\n",
    "  - y (np.array): output of leaky tanh function.\n",
    "  - leak_slope (float, default = 0.03): leaky term.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.array): input into leaky tanh function.\n",
    "  \"\"\"\n",
    "  ycopy = np.copy(y)\n",
    "  output = np.where(\n",
    "      np.abs(ycopy) >= 1+leak_slope, \\\n",
    "      (ycopy - np.sign(ycopy))/leak_slope, \\\n",
    "      ycopy/(1+leak_slope)\n",
    "  )\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "def HardTanh(x):\n",
    "  \"\"\"\n",
    "  Calculates `tanh` output for the given input data.\n",
    "\n",
    "  Inputs:\n",
    "  - x (np.ndarray): input data.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.ndarray): `tanh(x)`.\n",
    "  \"\"\"\n",
    "  min_val = -1\n",
    "  max_val = 1\n",
    "  output = np.copy(x)\n",
    "  output[output>max_val] = max_val\n",
    "  output[output<min_val] = min_val\n",
    "  return output\n",
    "\n",
    "def LeakyHardTanh(x, leak_slope=0.03):\n",
    "  \"\"\"\n",
    "  Calculate `tanh` output for the given input data with the leaky term.\n",
    "\n",
    "  Inputs:\n",
    "  - x (np.ndarray): input data.\n",
    "  - leak_slope (float, default = 0.03): leaky term.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.ndarray): `tanh(x)`.\n",
    "  \"\"\"\n",
    "  output = np.copy(x)\n",
    "  output = HardTanh(output) + leak_slope*output\n",
    "  return output\n",
    "\n",
    "def InverseLeakyHardTanh(y, leak_slope=0.03):\n",
    "  \"\"\"\n",
    "  Calculate input into the `tanh` function with the leaky term for the given output.\n",
    "\n",
    "  Inputs:\n",
    "  - y (np.array): output of leaky tanh function.\n",
    "  - leak_slope (float, default = 0.03): leaky term.\n",
    "\n",
    "  Outputs:\n",
    "  - output (np.array): input into leaky tanh function.\n",
    "  \"\"\"\n",
    "  ycopy = np.copy(y)\n",
    "  output = np.where(\n",
    "      np.abs(ycopy) >= 1+leak_slope, \\\n",
    "      (ycopy - np.sign(ycopy))/leak_slope, \\\n",
    "      ycopy/(1+leak_slope)\n",
    "  )\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize the functions\n",
    "\n",
    "# with plt.xkcd():\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "plot_vals = np.arange(-4, 4, 0.01)\n",
    "leak_slope = 0.03\n",
    "for i in range(3):\n",
    "  # Set the spines (axes lines) to intersect at the center\n",
    "  ax[i].spines['left'].set_position('zero')\n",
    "  ax[i].spines['bottom'].set_position('zero')\n",
    "  ax[i].spines['right'].set_color('none')\n",
    "  ax[i].spines['top'].set_color('none')\n",
    "  ax[i].set_xlabel('x', loc='right', fontsize=20)\n",
    "ax[0].plot(plot_vals, LeakyHardTanh(plot_vals, leak_slope), '-k')\n",
    "ax[0].set_title('LeakyHardTanh(x)', fontsize=14)\n",
    "ax[1].plot(plot_vals, InverseLeakyHardTanh(plot_vals, leak_slope), '-k')\n",
    "ax[1].set_title('InverseLeakyHardTanh(x)', fontsize=14)\n",
    "ax[2].plot(plot_vals, InverseLeakyHardTanh(LeakyHardTanh(plot_vals, leak_slope), leak_slope), '-k')\n",
    "ax[2].set_title('InverseLeakyHardTanh( LeakyHardTanh(x) )', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define an $n$-dimensional vector $\\mathbf{x}$. This is our target latent variable, and we would like to preserve the information about it. However, $\\mathbf{x}$ is corrupted by a multiplicative scalar nuisance variable, g: $\\mathbf{y}$=g$\\mathbf{x}$.\n",
    "\n",
    "Downstream computation will use $\\mathbf{y}$ by passing it through an element-wise non-linearity $f$ (that saturates beyond a certain input range) and adding noise. By doing so, we lose information -- potentially a lot of information if $g$ is large and pushes the inputs into the saturating part of the non-linearity.\n",
    "\n",
    "If we knew $g$, then we could remove it by division and reduce the problem. Although we don't know $g$, we can still use Normalization as an estimate of $g$, divide by that estimate, and invert the non-linearity to recover an approximation of the original $\\mathbf{x}$. Here we use a `LeakyHardTanh`, which almost saturates but is technically invertible.\n",
    "\n",
    "Let's see if Normalization helps. We will compute the correlation between x and the estimate $\\hat{x}$ and compare this correlation with and without the usage of the Normalization function.\n",
    "\n",
    "Our information ($\\mathbf{X}$) is a collection of 10-dimensional vectors, having 400 samples in total. $\\mathbf{X} \\in \\mathbb{R}^{400 \\times 10}$, each of the components are drawn from $ \\mathcal{N}(0, 1)$. For each component for each vector in $\\mathbf{X}$, we have a nuisance scaling factor $s \\in \\mathbb{R}^{400}$, $s \\sim Exp(0.2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data\n",
    "\n",
    "def normalize(x, sigma, p, g):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "  - x(np.ndarray): Input array (n_samples * n_dim)\n",
    "  - sigma(float): Smoothing factor\n",
    "  - p(int): p-norm\n",
    "  - g(int): scaling factor\n",
    "\n",
    "  Outputs:\n",
    "  - xnorm (np.ndarray): normalized values.\n",
    "  \"\"\"\n",
    "  # Raise the absolute value of x to the power p\n",
    "  xp = np.power(np.abs(x), p)\n",
    "  # Sum the x over the dimensions (n_dim) axis\n",
    "  xp_sum = np.sum(np.power(np.abs(x), p), axis=1)\n",
    "  # Correct the dimensions of xp_sum, and taking the average reduces the dimensions\n",
    "  # Making xp_sum a row vector of shape (1, n_dim)\n",
    "  xp_sum = np.expand_dims(xp_sum, axis=1)\n",
    "  # Raise the sum to the power 1/p and add the smoothing factor (sigma)\n",
    "  denominator = sigma + np.power(xp_sum, 1/p)\n",
    "  # Scale the input data with a factor of g\n",
    "  numerator = x*g\n",
    "  # Calculate normalized x\n",
    "  xnorm = numerator/denominator\n",
    "  return xnorm\n",
    "\n",
    "# data\n",
    "n_samples = 400 # number of samples\n",
    "n_dim = 10 # dimensions of each sample\n",
    "latent_std = 1 # width of latent distribution\n",
    "\n",
    "# nuisance\n",
    "nuisance_scale = 5 # distribution width for nuisance scaling factor\n",
    "\n",
    "# normalization\n",
    "smoothing_factor = 0.1 # normalization smoothness - sigma\n",
    "norm_p = 2 # Lp norm\n",
    "norm_scale = 1 # normalization scale\n",
    "\n",
    "# noise\n",
    "noise_std = 0.05 # added noise standard deviation\n",
    "\n",
    "# Non-Linearity\n",
    "leak_slope = 0.001 # slope after leaky saturation\n",
    "\n",
    "# random nuisance scaling for each example vector\n",
    "nuisance = np.random.exponential(nuisance_scale, size=(n_samples, 1))\n",
    "x_sec31 = np.random.normal(loc=0.0, scale=latent_std, size=(n_samples, n_dim))\n",
    "y_sec31 = x_sec31 * nuisance # input vectors scaled by random nuisance\n",
    "ynorm_sec31 = normalize(y_sec31, smoothing_factor, norm_p, norm_scale) * norm_scale # normalized vectors\n",
    "noise = np.random.normal(loc=0.0, scale=noise_std, size=(n_samples, n_dim))\n",
    "\n",
    "# without normalization\n",
    "transmit_noisy_x = LeakyHardTanh(y_sec31, leak_slope) + noise\n",
    "estimate_x = InverseLeakyHardTanh(transmit_noisy_x, leak_slope)\n",
    "# with normalization\n",
    "transmitNormalized_noisy_x = LeakyHardTanh(ynorm_sec31, leak_slope) + noise\n",
    "estimateNormalized_x = InverseLeakyHardTanh(transmitNormalized_noisy_x, leak_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the dimensions of $\\mathbf{x}$ and visualize it after nuisance scaling as well as after normalization.\n",
    "\n",
    "$$\\mathbf{x}_{norm} = \\frac{g \\mathbf{x}}{\\sigma + \\sqrt[p]{\\Sigma_{i = 1}^{N} |x_{i}|^{p}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize input\n",
    "with plt.xkcd():\n",
    "    sns.kdeplot(ynorm_sec31[:, 0], color='r', label='$(s \\mathbf{x})_{norm}$')\n",
    "    sns.kdeplot(x_sec31[:, 0], color='k', label='$\\mathbf{x}$')\n",
    "    sns.kdeplot(y_sec31[:, 0], color='b', label='$s \\mathbf{x}$')\n",
    "    plt.xlabel('Information (x)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's transmit this observable information through a network. In this example, the network is an element-wise `LeakyHardTanh`. Additionally, the transmission is noisy with transmission noise $n \\sim \\mathcal{N}(0, 0.05)$.\n",
    "\n",
    "Hence, the transmitted signal is `LeakyHardTanh`($s\\mathbf{x}$) + $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize noisy transmitted signal\n",
    "with plt.xkcd():\n",
    "    plt.figure(figsize=(7.5, 7.5))\n",
    "    sns.kdeplot(LeakyHardTanh(y_sec31, leak_slope)[:, 0], linestyle='--', color='b', label=r'LeakyHardTanh$(s \\mathbf{x})$')\n",
    "    sns.kdeplot(transmit_noisy_x[:, 0],color='b', label=r'LeakyHardTanh$(s \\mathbf{x})$+noise')\n",
    "    sns.kdeplot(LeakyHardTanh(ynorm_sec31, leak_slope)[:, 0], linestyle='--', color='r', label='LeakyHardTanh$(s \\mathbf{x})_{norm}$')\n",
    "    sns.kdeplot(transmitNormalized_noisy_x[:, 0], color='r', label='LeakyHardTanh$(s \\mathbf{x})_{norm}$+noise')\n",
    "    plt.xlabel('Transmitted Signal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the true information by calculating the inverse of the network (`InverseLeakyHardTanh`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize estimated information\n",
    "with plt.xkcd():\n",
    "    sns.kdeplot(estimateNormalized_x[:, 0], color='r', label='$\\mathbf{\\hat{x}}_{norm}$')\n",
    "    sns.kdeplot(x_sec31[:, 0], color='k', label='$\\mathbf{x}$')\n",
    "    sns.kdeplot(estimate_x[:, 0], color='b', label='$\\mathbf{\\hat{x}}$')\n",
    "    plt.xlabel('Estimated information (x)')\n",
    "    plt.xlim(-50, 50)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quantify how well we can estimate the true information by calculating R-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot correlation between estimated information and true information\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot x vs. estimated x\n",
    "    x_ = x_sec31.reshape((-1, 1)).squeeze(-1)\n",
    "    y_ = estimate_x.reshape((-1, 1)).squeeze(-1)\n",
    "    sns.regplot(x=x_, y=y_, ax=ax[0], fit_reg=False)\n",
    "    ax[0].set_xlabel('x')\n",
    "    ax[0].set_ylabel(r'$\\hat{x}$')\n",
    "    # Calculate R-squared and p-value\n",
    "    result = scipy.stats.linregress(x_, y_)\n",
    "    ax[0].set_title(r'$\\hat{x} \\enspace vs. \\enspace x, \\enspace R^{2} = $' + \\\n",
    "                    f'{(result.rvalue**2):.2f}')\n",
    "    ax[0].set_ylabel('$\\hat{x}$', loc='bottom', fontsize=20)\n",
    "    ax[0].set_ylim((-5, 5))\n",
    "\n",
    "    # Plot x vs. estimated normalized x\n",
    "    x_ = x_sec31.reshape((-1, 1)).squeeze(-1)\n",
    "    y_ = estimateNormalized_x.reshape((-1, 1)).squeeze(-1)\n",
    "    sns.regplot(x=x_, y=y_, ax=ax[1], fit_reg=False)\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel(r'$\\hat{x_{norm}}$')\n",
    "    # ax[1].set_ylim((-1.05, 1.05))\n",
    "    ax[1].set_ylim((-5, 5))\n",
    "    # Calculate R-squared and p-value\n",
    "    result = scipy.stats.linregress(x_, y_)\n",
    "    ax[1].set_title(r'$\\hat{x_{norm}} \\enspace vs. \\enspace x, \\enspace R^{2} = $' + \\\n",
    "                    f'{(result.rvalue**2):.2f}')\n",
    "    ax[1].set_ylabel('$\\hat{x_{norm}}$', loc='bottom', fontsize=20)\n",
    "\n",
    "    for i in range(2):\n",
    "      # Set the spines (axes lines) to intersect at the center\n",
    "      ax[i].spines['left'].set_position('zero')\n",
    "      ax[i].spines['bottom'].set_position('zero')\n",
    "      ax[i].spines['right'].set_color('none')\n",
    "      ax[i].spines['top'].set_color('none')\n",
    "      ax[i].set_xlabel('x', loc='right', fontsize=20)\n",
    "      ax[i].set_xlim((-4, 4))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that normalization helped us preserve the information through transmission by preventing saturation (constraining the information within a limited dynamic range)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Think 1.1\n",
    "\n",
    "1. We control the dynamic range for normalization. Does there exist an optimum range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's control the range by manipulating the scaling factor ($g$).\n",
    "\n",
    "$$\\mathbf{x}_{norm} = \\frac{g \\mathbf{x}}{\\sigma + \\sqrt[p]{\\Sigma_{1}^{N} |x_{i}|^{p}}}$$\n",
    "\n",
    "We will plot the improvement in the correlation versus the range that the normalization produces (via scaling factor $g$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Effect of scaling normalization ($g$)\n",
    "norm_scales = np.arange(0.01, 5, 0.01)\n",
    "improvements = []\n",
    "\n",
    "x_ = x_sec31.reshape((-1, 1)).squeeze(-1)\n",
    "y_ = estimate_x.reshape((-1, 1)).squeeze(-1)\n",
    "result = scipy.stats.linregress(x_, y_)\n",
    "nonnorm_r2 = result.rvalue\n",
    "\n",
    "for norm_scale in norm_scales:\n",
    "  ynorm_ = normalize(y_sec31, smoothing_factor, norm_p, norm_scale) # normalized vectors\n",
    "  transmitNormalized_noisy_x = LeakyHardTanh(ynorm_, leak_slope) + noise\n",
    "  estimateNormalized_x = InverseLeakyHardTanh(transmitNormalized_noisy_x, leak_slope)\n",
    "  x_ = x_sec31.reshape((-1, 1)).squeeze(-1)\n",
    "  y_ = estimateNormalized_x.reshape((-1, 1)).squeeze(-1)\n",
    "  result = scipy.stats.linregress(x_, y_)\n",
    "  norm_r2 = result.rvalue\n",
    "  improvement = norm_r2/nonnorm_r2\n",
    "  improvements.append(improvement)\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(norm_scales, improvements, '.')\n",
    "    plt.ylim((-0.05, 2.05))\n",
    "    plt.xlabel('Normalization scaling factor ($g$)')\n",
    "    plt.ylabel(r'Improvement')\n",
    "    plt.title(r'Improvement = $\\frac{R^{2}(x, \\hat{x}_\\mathrm{scaled norm})}{R^{2}(x, \\hat{x})}$')\n",
    "    ax = plt.gca()\n",
    "    for line in ax.get_lines():\n",
    "      line.set_path_effects([path_effects.Normal()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an optimal normalization range: while being too narrow - the noise dominates, and with too wide - the saturation destroys information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Think 1.2\n",
    "\n",
    "1. Thinking deeper: here, we have used normalization only to preserve the information, essentially by avoiding most of the non-linearity. Do you think the computation can gain an advantage by using saturation? How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_efficient_coding\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
