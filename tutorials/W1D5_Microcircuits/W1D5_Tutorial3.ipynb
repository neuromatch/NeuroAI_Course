{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYV2Ls9DUWuM"
   },
   "source": [
    "# Tutorial 3: Attention\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "**Content creators:** Saaketh Medepalli, Aditya Singh, Saeed Salehi, Xaq Pitkow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or7_s4jQUWuO"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "By the end of this tutorial, we aim to:\n",
    "\n",
    "\n",
    "1. Learn how the brain and AI systems implemention attention\n",
    "\n",
    "2. Understand how multiplicative interactions allow flexible gating of information\n",
    "\n",
    "3. Demonstrate the inductive bias of the self-attention mechanism towards learning functions of sparse subsets of variables\n",
    "\n",
    "A key microarchitectural operation in brains and machines is **attention**. The essence of this operation is multiplication. Unlike the vanilla neural network operation, a nonlinear function of a weighted sum of inputs $f(\\sum_j w_{j}x_j)$, the attention operation allows responses to *multiply* inputs. This enable computations like modulating weights by other inputs.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "In brains, the theory of the Spotlight of Attention (Posner et al 1980) posited that gain modulation allowed brain computations to highlight or select certain information, prioritizing it for subsequent computation. In machines, the seminal book Parallel Distributed Processing (Rumelhart, Hinton, McClelland 1986) described Sigma-Pi networks that included both sums ($\\Sigma$) and products ($\\Pi$) as fundamental operations, and mentioned. The Transformer network introduced in (Vaswani et al 2017) used a specific architecture featuring layers of multiplication and normalization. Many machine learning systems since then have fruitfully applied this architecture to language, vision, and many more modalities. In this tutorial we will isolate the central properties and generalization benefits of multiplicative attention shared by all of these applications.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Exercises include simple attentional modulation of inputs, coding the self-attention mechanism, demonstrating its inductive bias, and interpreting the consequences of attention.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "References:\n",
    "- Posner MI, Snyder CR, Davidson BJ (1980). Attention and the detection of signals. *Journal of experimental psychology: General*. 109(2):160.\n",
    "\n",
    "- Rumelhart DE, McClelland JL, PDP Research Group (1986). *Parallel Distributed Processing*. MIT press.\n",
    "\n",
    "- Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser ≈Å, Polosukhin I (2017). [Attention is all you need.](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) *Advances in Neural Information Processing Systems*.\n",
    "\n",
    "- Thomas Viehmann, [toroidal - a lightweight transformer library for PyTorch](https://github.com/MathInf/toroidal)\n",
    "\n",
    "- Edelman et al. 2022, *Inductive biases and variable creation in self-attention mechanisms*\n",
    "\n",
    "- Deep Graph Library Tutorials, [Transformer as a Graph Neural Network](https://docs.dgl.ai/en/0.8.x/tutorials/models/4_old_wines/7_transformer.html)\n",
    "\n",
    "- Lilian Weng, [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkKObs0rUWuO"
   },
   "source": [
    "---\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prSi55e7UWuP"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xgig_EQRUWuP"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")\n",
    "fig_w, fig_h = plt.rcParams['figure.figsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OQbwi6yLeFzK"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "def plot_loss_accuracy(t_loss, t_acc, v_loss = None, v_acc = None):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.suptitle(\"Training and Validation for the Transformer Model\")\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(t_loss, label=\"Training loss\", color=\"red\")\n",
    "    if v_loss is not None:\n",
    "        # plt.plot(v_loss, label=\"Valididation loss\", color=\"blue\")\n",
    "        plt.scatter(len(t_loss)-1, v_loss, label=\"Validation loss\", color=\"blue\", marker=\"*\")\n",
    "        # plt.text(len(t_loss)-1, v_loss, f\"{v_loss:.3f}\", va=\"bottom\", ha=\"right\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(t_acc, label=\"Training accuracy\", color=\"red\", linestyle=\"dotted\")\n",
    "    if v_acc is not None:\n",
    "        # plt.plot(v_acc, label=\"Validation accuracy\", color=\"blue\", linestyle=\"--\")\n",
    "        plt.scatter(len(t_acc)-1, v_acc, label=\"Validation accuracy\", color=\"blue\", marker=\"*\")\n",
    "        # plt.text(len(t_acc)-1, v_acc, f\"{v_acc:.3f}\", va=\"bottom\", ha=\"right\")\n",
    "    plt.xticks([])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples(X_plot, y_plot, correct_ids, title=None):\n",
    "    n_samples, seq_length = X_plot.shape\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 2.5), sharey=True)\n",
    "    rects = []\n",
    "    for ri in correct_ids:\n",
    "        rects.append(plt.Rectangle((ri-0.5, -0.5), 1, n_samples, edgecolor=\"red\", alpha=1.0, fill=False, linewidth=2))\n",
    "    axs[0].imshow(X_plot, cmap=\"binary\")\n",
    "    for rect in rects:\n",
    "        axs[0].add_patch(rect)\n",
    "    # axs[0].axis(\"off\")\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_ylabel(\"Context\")\n",
    "    axs[0].set_xlabel(\"Samples\")\n",
    "    if title is not None:\n",
    "        axs[0].set_title(title)\n",
    "    axs[1].imshow(y_plot, cmap=\"binary\")\n",
    "    axs[1].add_patch(plt.Rectangle((-0.5, -0.5), 1, n_samples, edgecolor=\"black\", alpha=1.0, fill=False, linewidth=2))\n",
    "    axs[1].yaxis.set_label_position(\"right\")\n",
    "    axs[1].set_ylabel(\"Labels\")\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_xticks([])\n",
    "    plt.subplots_adjust(wspace=-1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_weights(att_weights, correct_ids, context_length, labels):\n",
    "    from matplotlib.lines import Line2D\n",
    "    B = att_weights.size(0)\n",
    "    aw_flatten = att_weights[:, -1, :-1].view(-1, context_length)\n",
    "    x_axis = torch.arange(context_length).repeat(B, 1)\n",
    "    y_axis = labels.view(-1, 1).repeat(1, context_length)\n",
    "    aw_ravel = aw_flatten.ravel()\n",
    "    x_ravel = x_axis.ravel()\n",
    "    y_ravel = y_axis.ravel()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    colors = [\"red\", \"green\"]\n",
    "    labels_legened = [\"True\", \"False\"]\n",
    "    ax.scatter(x_ravel, aw_ravel, alpha=0.1, c=[colors[int(y)] for y in y_ravel])\n",
    "    rects = []\n",
    "    for ri in correct_ids:\n",
    "        rects.append(plt.Rectangle((ri-0.5, 1e-6), 1.0, 2.0, edgecolor=\"blue\", alpha=1.0, fill=False, linewidth=2))\n",
    "    for rect in rects:\n",
    "        ax.add_patch(rect)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(1e-6, 2)\n",
    "    plt.title(\"Attention weights for the whole batch\")\n",
    "    plt.xlabel(\"Boolean input index t\")\n",
    "    plt.ylabel(\"Attention weight\")\n",
    "    legend_elements = [Line2D([0], [0], linestyle='None', marker='o', color='green', label='True', markerfacecolor='g', markersize=7),\n",
    "                       Line2D([0], [0], linestyle='None', marker='o', color='red', label='False', markerfacecolor='r', markersize=7)]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_compare(results_sat_d, results_sat_s, results_mlp_d, results_mlp_s,\n",
    "                 B_t_sat_s, B_t_sat_d, B_t_mlp_s, B_t_mlp_d):\n",
    "    from  matplotlib.colors import LinearSegmentedColormap\n",
    "    cmap=LinearSegmentedColormap.from_list('rg',[\"w\", \"w\", \"r\", \"y\", \"g\"], N=256)\n",
    "\n",
    "    t_loss_sat_d, t_acc_sat_d, v_loss_sat_d, v_acc_sat_d, model_np_sat_d = results_sat_d\n",
    "    t_loss_sat_s, t_acc_sat_s, v_loss_sat_s, v_acc_sat_s, model_np_sat_s = results_sat_s\n",
    "    t_loss_mlp_d, t_acc_mlp_d, v_loss_mlp_d, v_acc_mlp_d, model_np_mlp_d = results_mlp_d\n",
    "    t_loss_mlp_s, t_acc_mlp_s, v_loss_mlp_s, v_acc_mlp_s, model_np_mlp_s = results_mlp_s\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title(\"sparse\", fontsize=16)\n",
    "    plt.ylabel(\"Attention\", fontsize=16)\n",
    "    plt.barh(3, 0, color=\"blue\")\n",
    "    plt.barh(4, 0, color=\"green\")\n",
    "    plt.text(0.05, 3.5, f\"# Parameters: {model_np_sat_s}\")\n",
    "    plt.text(0.05, 3.0, f\"# samples: {B_t_sat_s}\")\n",
    "    plt.barh(2, t_acc_sat_s, color=cmap(t_acc_sat_s))\n",
    "    plt.barh(1, v_acc_sat_s, color=cmap(v_acc_sat_s))\n",
    "    plt.text(0.05, 2, f\"# training acc: {t_acc_sat_s:.0%}\")\n",
    "    plt.text(0.05, 1, f\"# validation acc: {v_acc_sat_s:.0%}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title(\"dense\", fontsize=16)\n",
    "    plt.barh(3, 0, color=\"blue\")\n",
    "    plt.barh(4, 0, color=\"green\")\n",
    "    plt.text(0.05, 3.5, f\"# Parameters: {model_np_sat_d}\")\n",
    "    plt.text(0.05, 3.0, f\"# samples: {B_t_sat_d}\")\n",
    "    plt.barh(2, t_acc_sat_d, color=cmap(t_acc_sat_d))\n",
    "    plt.barh(1, v_acc_sat_d, color=cmap(v_acc_sat_d))\n",
    "    plt.text(0.05, 2, f\"# training acc: {t_acc_sat_d:.0%}\")\n",
    "    plt.text(0.05, 1, f\"# validation acc: {v_acc_sat_d:.0%}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.barh(3, 0, color=\"blue\")\n",
    "    plt.barh(4, 0, color=\"green\")\n",
    "    plt.text(0.05, 3.5, f\"# Parameters: {model_np_mlp_s}\")\n",
    "    plt.text(0.05, 3.0, f\"# samples: {B_t_mlp_s}\")\n",
    "    plt.barh(2, t_acc_mlp_s, color=cmap(t_acc_mlp_s))\n",
    "    plt.barh(1, v_acc_mlp_s, color=cmap(v_acc_mlp_s))\n",
    "    plt.text(0.05, 2, f\"# training acc: {t_acc_mlp_s:.0%}\")\n",
    "    plt.text(0.05, 1, f\"# validation acc: {v_acc_mlp_s:.0%}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.ylabel(\"MLP\", fontsize=16)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.barh(3, 0, color=\"blue\")\n",
    "    plt.barh(4, 0, color=\"green\")\n",
    "    plt.text(0.05, 3.5, f\"# Parameters: {model_np_mlp_d}\")\n",
    "    plt.text(0.05, 3.0, f\"# samples: {B_t_mlp_d}\")\n",
    "    plt.barh(2, t_acc_mlp_d, color=cmap(t_acc_mlp_d))\n",
    "    plt.barh(1, v_acc_mlp_d, color=cmap(v_acc_mlp_d))\n",
    "    plt.text(0.05, 2, f\"# training acc: {t_acc_mlp_d:.0%}\")\n",
    "    plt.text(0.05, 1, f\"# validation acc: {v_acc_mlp_d:.0%}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ogciQHHGeHhn"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "class s_Sparse_AND:  # 1-Dimensional AND\n",
    "    def __init__(self, T: int, s: int):\n",
    "        self.T = T # context length\n",
    "        self.s = s # sparsity\n",
    "        self.p = 0.5**(1.0/3.0)  # probability chosen for balanced data\n",
    "        self.f_i = None\n",
    "\n",
    "    def pick_an_f(self):\n",
    "        self.f_i = sorted(random.sample(range(self.T), self.s))\n",
    "        self.others = list(i for i in range(self.T) if i not in self.f_i)\n",
    "\n",
    "    def generate(self, m: int, verbose: bool = False):\n",
    "        if self.f_i is None:\n",
    "            self.pick_an_f()\n",
    "        max_try = 100\n",
    "        i_try = 0\n",
    "        while i_try < max_try:\n",
    "            i_try += 1\n",
    "            X, y = torch.zeros(m, self.T), torch.zeros(m, 1)\n",
    "            X[torch.rand(m, self.T) < self.p] = 1\n",
    "            y[X[:, self.f_i].sum(dim=1) == self.s] = 1\n",
    "            if y.sum()/m < 0.4 or y.sum()/m > 0.6:\n",
    "                verbose and print(f\"Large imbalance in the training set {y.sum()/m}, retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                verbose and print(f\"Data-label balance: {y.sum()/m}\")\n",
    "            bad_batch = False\n",
    "            for i in self.f_i:\n",
    "                for o in self.others:\n",
    "                    if (X[:, i] == X[:, o]).all():\n",
    "                        verbose and print(f\"Found at least another compatible hypothesis {i} and {o}\")\n",
    "                        bad_batch = True\n",
    "                        break\n",
    "            if bad_batch:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            verbose and print(\"Could not find a compatible hypothesis\")\n",
    "        return X.long(), y.float()\n",
    "\n",
    "\n",
    "# From notebook\n",
    "# Load an example image from the imagenet-sample-images repository\n",
    "def load_image(path):\n",
    "  \"\"\"\n",
    "  Load an image from a given path\n",
    "\n",
    "  Args:\n",
    "    path: String\n",
    "      Path to the image\n",
    "\n",
    "  Returns:\n",
    "    img: PIL Image\n",
    "      Image loaded from the path\n",
    "  \"\"\"\n",
    "  img = Image.open(path)\n",
    "  return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-yvHFzsXeLZw"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "class BinaryMLP(torch.nn.Module):\n",
    "    def __init__(self, in_dims, h_dims, out_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.h_dims = h_dims\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(in_dims, h_dims[0]))\n",
    "        torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "        self.layers.append(torch.nn.GELU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout))\n",
    "        for i in range(len(h_dims) - 1):\n",
    "            self.layers.append(torch.nn.Linear(h_dims[i], h_dims[i+1]))\n",
    "            torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "            torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "            self.layers.append(torch.nn.GELU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout))\n",
    "        self.layers.append(torch.nn.Linear(h_dims[-1], out_dims))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class BinarySAT(torch.nn.Module):\n",
    "    \"\"\"Binary Self-Attention Transformer\n",
    "    The code is adopted from \"https://github.com/MathInf/toroidal\" by Thomas Viehmann\n",
    "    \"\"\"\n",
    "    def __init__(self, T: int, d: int, n_heads: int, n: int):\n",
    "        super().__init__()\n",
    "        self.T = T  # context length\n",
    "        self.E = T + 1  # effective length (including cls token)\n",
    "        self.d = d  # embedding size\n",
    "        self.n_heads = n_heads  # number of heads\n",
    "        self.scale = (d // n_heads) ** -0.5  # scaling factor (1 / sqrt(d_k))\n",
    "        self.n = n  # number of hidden units\n",
    "        assert d % n_heads == 0, \"embedding size must be divisible by number of heads\"\n",
    "        self.v = 2  # vocabulary size (binary input, 0 or 1)\n",
    "        att_drop=0.1\n",
    "        out_drop=0.1\n",
    "        mlp_drop=0.1\n",
    "        ln_eps=1e-6\n",
    "\n",
    "        # embedding layers\n",
    "        self.toke = torch.nn.Embedding(2, d)  # token embedding\n",
    "        self.cls = torch.nn.Parameter(torch.randn(1, 1, d))  # \"cls / class / global\" learnable token\n",
    "        self.pose = torch.nn.Parameter(torch.randn(1, T + 1, d))  # positional embedding\n",
    "        self.norm1 = torch.nn.LayerNorm(d, eps=ln_eps)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "\n",
    "        # self-attention layers\n",
    "        self.qkv = torch.nn.Linear(d, 3 * d)  # query, key, value layers\n",
    "        self.dropout_attn = torch.nn.Dropout(att_drop)\n",
    "        self.proj = torch.nn.Linear(d, d)  # projection layer\n",
    "        self.dropout_out = torch.nn.Dropout(out_drop)\n",
    "        self.norm2 = torch.nn.LayerNorm(d, eps=ln_eps)\n",
    "\n",
    "        # MLP layers\n",
    "        self.mlp_l1 = torch.nn.Linear(d, n)\n",
    "        self.mlp_l2 = torch.nn.Linear(n, d)\n",
    "        self.dropout_mlp = torch.nn.Dropout(mlp_drop)\n",
    "        self.norm3 = torch.nn.LayerNorm(d, eps=1e-6)\n",
    "\n",
    "        # clasification layer\n",
    "        self.head = torch.nn.Linear(d, 1)\n",
    "\n",
    "        # initialize weights and biases (per description in the paper)\n",
    "        torch.nn.init.normal_(self.toke.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.pose, std=0.02)\n",
    "        torch.nn.init.normal_(self.cls, std=0.02)\n",
    "        torch.nn.init.ones_(self.norm1.weight)\n",
    "        torch.nn.init.zeros_(self.norm1.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.qkv.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.qkv.bias)\n",
    "        torch.nn.init.normal_(self.proj.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.ones_(self.norm2.weight)\n",
    "        torch.nn.init.zeros_(self.norm2.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.mlp_l1.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l1.bias)\n",
    "        torch.nn.init.normal_(self.mlp_l2.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l2.bias)\n",
    "        torch.nn.init.ones_(self.norm3.weight)\n",
    "        torch.nn.init.zeros_(self.norm3.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.head.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        B = x.size(0)  # batch size\n",
    "        x = self.toke(x)\n",
    "        x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)\n",
    "        x = x + self.pose\n",
    "\n",
    "        # Transformer Block\n",
    "        norm_x = self.norm1(x)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "        q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.n_heads, -1).unbind(dim=2)\n",
    "        # # (Scaled Dot-Product Attention)\n",
    "        logits = torch.einsum(\"bthc,bshc->bhts\", q, k)  # query key product\n",
    "        logits *= self.scale  # normalize against staturation\n",
    "        attn = torch.softmax(logits, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "        output = torch.einsum(\"bhts,bshc->bthc\", attn, v)  # weighted attention\n",
    "        # # concat and linear projection with residual connection\n",
    "        output = output.reshape(B, self.E, self.d)  # recombine\n",
    "        output = self.proj(output)  # linear layer projection\n",
    "        output = self.dropout_out(output)\n",
    "        x = self.norm2(x + output)  # normalization and residual connection\n",
    "\n",
    "        # MLP with residual connection\n",
    "        output = torch.relu(self.mlp_l1(x))  # nonlinear layer\n",
    "        output = self.dropout_mlp(output)\n",
    "        output = self.mlp_l2(output)  # linear layer\n",
    "        x = self.norm3(x + output)  # normalization and residual connection\n",
    "\n",
    "        # projection\n",
    "        x = self.head(x[:, -1])\n",
    "        x = torch.sigmoid(x)  # binary classification task\n",
    "        return x\n",
    "\n",
    "\n",
    "def bin_acc(y_hat, y):\n",
    "    \"\"\"\n",
    "    Compute the binary accuracy\n",
    "    \"\"\"\n",
    "    y_ = y_hat.round()\n",
    "    TP_TN = (y_ == y).float().sum().item()\n",
    "    FP_FN = (y_ != y).float().sum().item()\n",
    "    assert TP_TN + FP_FN == y.numel(), f\"{TP_TN + FP_FN} != {y.numel()}\"\n",
    "    return TP_TN / y.numel()\n",
    "\n",
    "\n",
    "def get_n_parameters(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get the number of learnable parameters in a model\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for par in model.parameters():\n",
    "        i += par.numel()\n",
    "    return i\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), 'model_states.pt')\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model_states = torch.load('model_states.pt')\n",
    "    model.load_state_dict(model_states)\n",
    "\n",
    "def evaluator(model, criterion, X_v, y_v, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X_v, y_v = X_v.to(device), y_v.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_v)\n",
    "        loss = criterion(y_hat.squeeze(), y_v.squeeze())\n",
    "        acc = bin_acc(y_hat, y_v)\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def trainer(model, optimizer, criterion, n_epochs, X_t, y_t, device=\"cpu\", verbose=False):\n",
    "    train_loss, train_acc = [], []\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "    for i in range(n_epochs):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        y_hat = model(X_t)\n",
    "        loss_t = criterion(y_hat.squeeze(), y_t.squeeze())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            train_loss.append(loss_t.item())\n",
    "            train_acc.append(bin_acc(y_hat, y_t))\n",
    "    model.eval()\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention_solution(Q, K, V):\n",
    "    \"\"\" Scaled dot product attention\n",
    "    Args:\n",
    "        Q: queries (B, H, d, n)\n",
    "        K: keys (B, H, d, n)\n",
    "        V: values (B, H, d, n)\n",
    "    Returns:\n",
    "        Attention tensor (B, H, d, n), Scores (B, H, d, d)\n",
    "    Notes:\n",
    "        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim\n",
    "    \"\"\"\n",
    "\n",
    "    assert K.shape == Q.shape and K.shape == V.shape, \"Queries, Keys and Values must have the same shape\"\n",
    "    B, H, d, n = K.shape  # batch_size, num_heads, key-query dim, embedding dim\n",
    "    scale = math.sqrt(d)\n",
    "    Q_mm_K = torch.einsum(\"bhdn,bhen->bhde\", Q, K)  # dot-product reducing the n dimension\n",
    "    S = Q_mm_K / scale  # score or scaled dot product\n",
    "    S_sm = torch.softmax(S, dim=-1)  # softmax\n",
    "    A = torch.einsum(\"bhde,bhen->bhdn\", S_sm, V)  # Attention\n",
    "    return A, S\n",
    "\n",
    "\n",
    "def make_train(model, T, s, B_t, B_v, n_epochs, device=\"cpu\", kind=\"MLP\", results=True):\n",
    "    etta = 1e-3\n",
    "    data_gen = s_Sparse_AND(T, s)\n",
    "    X_train, y_train = data_gen.generate(B_t, verbose=False)\n",
    "    X_valid, y_valid = data_gen.generate(B_v, verbose=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=etta)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    model_np = get_n_parameters(model)  # number of learnable parameters\n",
    "    results and print(f\"Number of model's learnable parameters: {model_np}\")\n",
    "    if kind == \"MLP\":\n",
    "        t_loss, t_acc = trainer(model, optimizer, criterion, n_epochs, X_train.float(), y_train, device=device, verbose=False)\n",
    "        if results:\n",
    "            v_loss, v_acc = evaluator(model, criterion, X_valid.float(), y_valid, device=device)\n",
    "    else:\n",
    "        t_loss, t_acc = trainer(model, optimizer, criterion, n_epochs, X_train, y_train, device=device, verbose=False)\n",
    "        if results:\n",
    "            v_loss, v_acc = evaluator(model, criterion, X_valid, y_valid, device=device)\n",
    "    results and print(f\"Training loss: {t_loss[-1]:.3f}, accuracy: {t_acc[-1]:.3f}\")\n",
    "    results and print(f\"Validation loss: {v_loss:.3f}, accuracy: {v_acc:.3f}\")\n",
    "    if results:\n",
    "        return t_loss[-1], t_acc[-1], v_loss, v_acc, model_np\n",
    "    else:\n",
    "        return data_gen\n",
    "\n",
    "\n",
    "def weighted_attention(self, x):\n",
    "    \"\"\"This function computes the weighted attention as a method for the BinarySAT class\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): An array of shape (B:batch_size, T:context length) containing the input data\n",
    "\n",
    "    Returns:\n",
    "        Tensor: weighted attention of shape (B, E, E) where E = T + 1\n",
    "    \"\"\"\n",
    "    assert self.n_heads == 1, \"This function is only implemented for a single head!\"\n",
    "    # Embedding\n",
    "    B = x.size(0)  # batch size\n",
    "    x = self.toke(x)  # token embedding\n",
    "    x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)  # concatenate cls token\n",
    "    x = x + self.pose  # positional embedding\n",
    "    norm_x = self.norm1(x)  # normalization\n",
    "\n",
    "    # Scaled Dot-Product Attention (partially implemented)\n",
    "    q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.d).unbind(dim=2)\n",
    "    # q, k, v all have shape (B, E, h, d) where h is the number of heads (1 in this case)\n",
    "    W_qk = q @ k.transpose(-2, -1)\n",
    "    W_qk = W_qk * self.scale\n",
    "    W_qk = torch.softmax(W_qk, dim=-1)\n",
    "    return W_qk\n",
    "\n",
    "\n",
    "def results_dict(results_sat_d, results_sat_s, results_mlp_d, results_mlp_s):\n",
    "    from collections import OrderedDict\n",
    "    t_loss_sat_d, t_acc_sat_d, v_loss_sat_d, v_acc_sat_d, model_np_sat_d = results_sat_d\n",
    "    t_loss_sat_s, t_acc_sat_s, v_loss_sat_s, v_acc_sat_s, model_np_sat_s = results_sat_s\n",
    "    t_loss_mlp_d, t_acc_mlp_d, v_loss_mlp_d, v_acc_mlp_d, model_np_mlp_d = results_mlp_d\n",
    "    t_loss_mlp_s, t_acc_mlp_s, v_loss_mlp_s, v_acc_mlp_s, model_np_mlp_s = results_mlp_s\n",
    "    output = OrderedDict()\n",
    "    # output[\"Training Loss SAT dense\"] = t_loss_sat_d\n",
    "    # output[\"Training Accuracy SAT dense\"] = t_acc_sat_d\n",
    "    output[\"Validation Loss SAT dense\"] = round(v_loss_sat_d, 3)\n",
    "    output[\"Validation Accuracy SAT dense\"] = v_acc_sat_d\n",
    "    output[\"Number of Parameters SAT dense\"] = model_np_sat_d\n",
    "    # output[\"Training Loss SAT sparse\"] = t_loss_sat_s\n",
    "    # output[\"Training Accuracy SAT sparse\"] = t_acc_sat_s\n",
    "    output[\"Validation Loss SAT sparse\"] = round(v_loss_sat_s, 3)\n",
    "    output[\"Validation Accuracy SAT sparse\"] = v_acc_sat_s\n",
    "    output[\"Number of Parameters SAT sparse\"] = model_np_sat_s\n",
    "    # output[\"Training Loss MLP dense\"] = t_loss_mlp_d\n",
    "    # output[\"Training Accuracy MLP dense\"] = t_acc_mlp_d\n",
    "    output[\"Validation Loss MLP dense\"] = round(v_loss_mlp_d, 3)\n",
    "    output[\"Validation Accuracy MLP dense\"] = v_acc_mlp_d\n",
    "    output[\"Number of Parameters MLP dense\"] = model_np_mlp_d\n",
    "    # output[\"Training Loss MLP sparse\"] = t_loss_mlp_s\n",
    "    # output[\"Training Accuracy MLP sparse\"] = t_acc_mlp_s\n",
    "    output[\"Validation Loss MLP sparse\"] = round(v_loss_mlp_s, 3)\n",
    "    output[\"Validation Accuracy MLP sparse\"] = v_acc_mlp_s\n",
    "    output[\"Number of Parameters MLP sparse\"] = model_np_mlp_s\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cizRlxC-w4OA"
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Handles variability by controlling sources of randomness\n",
    "  through set seed values\n",
    "\n",
    "  Args:\n",
    "    seed: Integer\n",
    "      Set the seed value to given integer.\n",
    "      If no seed, set seed value to random integer in the range 2^32\n",
    "    seed_torch: Bool\n",
    "      Seeds the random number generator for all devices to\n",
    "      offer some guarantees on reproducibility\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tGxUD-WJw4OB"
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rri-bmKKw4OC",
    "outputId": "d64235dd-f7fb-4514-e178-7ea72d6441ef"
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63zDRofrfuMr"
   },
   "source": [
    "---\n",
    "# Section 1. Intro to multiplication\n",
    "\n",
    "In this exercise we will show how signals can be used to selectively gate inputs. This is the essence of attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-RXXKi-gB0m"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "We'll implement simple dot product attention for input $\\mathbf{x}$ and scalar output $y$ given by a weighted combination of the inputs,\n",
    "$$y = \\mathbf{w}\\cdot\\mathbf{x}$$\n",
    "for weights $\\mathbf{w}$. Unlike usual readouts, however, attention adjusts the weights based on other signals $\\mathbf{z}$. Here we will use just a two dimensional attentional gain, $z_1$ and $z_2$, each with a corresponding key vector $\\mathbf{q}$ that determines what is attended:\n",
    "\n",
    "$$\\mathbf{w}(z_1,z_2) = \\text{softmax}(z_1 \\mathbf{q}_1 + z_2 \\mathbf{q}_2)$$\n",
    "\n",
    "You should code up this function to return the attentional weights and output.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/ssnio/nma_neuroai_d4_t4/main/static/Fig_0_ref2.png\"/>\n",
    "<figcaption>Figure from Inductive Biases and Variable Creation in Self-Attention Mechanisms</figcaption>\n",
    "</div>\n",
    "\n",
    "**Just include first panel above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amdVCAFXfwSd"
   },
   "outputs": [],
   "source": [
    "def gained_dot_product_attention(x: torch.Tensor,  # input vector\n",
    "                                 q_1: torch.Tensor,  # query vector 1\n",
    "                                 q_2: torch.Tensor,  # query vector 2\n",
    "                                 z_1: float,  # key gain 1\n",
    "                                 z_2: float,  # key gain 2\n",
    "                                 ):\n",
    "    \"\"\"This function computes the gained dot product attention\n",
    "    Args:\n",
    "        x (Tensor): input vector\n",
    "        q_1 (Tensor): query vector 1\n",
    "        q_2 (Tensor): query vector 2\n",
    "        z_1 (float): query gain 1\n",
    "        z_2 (float): query gain 2\n",
    "    Returns:\n",
    "        w (Tensor): attention weights\n",
    "        y (float): gained dot product attention\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO Implement the `gained_dot_product_attention`\n",
    "    # Fill remove the following line of code one you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: complete calculation of gained dot product attention.)\n",
    "    #################################################\n",
    "    w = ...\n",
    "    y = ...\n",
    "    return w, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WICNAY5om_5G"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def gained_dot_product_attention(x: torch.Tensor,  # input vector\n",
    "                                 q_1: torch.Tensor,  # query vector 1\n",
    "                                 q_2: torch.Tensor,  # query vector 2\n",
    "                                 z_1: float,  # query gain 1\n",
    "                                 z_2: float,  # query gain 2\n",
    "                                 ):\n",
    "    \"\"\"This function computes the gained dot product attention\n",
    "    Args:\n",
    "        x (Tensor): input vector\n",
    "        q_1 (Tensor): query vector 1\n",
    "        q_2 (Tensor): query vector 2\n",
    "        z_1 (float): query gain 1\n",
    "        z_2 (float): query gain 2\n",
    "    Returns:\n",
    "        w (Tensor): attention weights\n",
    "        y (float): gained dot product attention\n",
    "    \"\"\"\n",
    "    w = torch.softmax(z_1 * q_1 + z_2 * q_2, dim=0)\n",
    "    y = torch.dot(w, x)\n",
    "    return w, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cfPJPyqRLo-"
   },
   "source": [
    "Now we plot the input weights. Manipulate the sliders to control the two attention gains $z_k$, which we've pre-assigned to two specific sigmoidal pattern vectors $\\mathbf{q}_k$. (Feel free to change those functions and see how the sliders change what they attend.) Observe how these sliders change which part of the input is amplified and attenuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "5JPG7AiDk-ks",
    "outputId": "4724e7fd-387b-4de4-875e-8b8c2cdab490"
   },
   "outputs": [],
   "source": [
    "#@title { run: \"auto\" }\n",
    "gain_1 = 61.8 # @param {type:\"slider\", min:0.1, max:100.0, step:0.1}\n",
    "gain_2 = 0.1 # @param {type:\"slider\", min:0.1, max:100.0, step:0.1}\n",
    "T = 17  # context length\n",
    "# pre-defined signals\n",
    "x = torch.sin(torch.linspace(0, 2*3.1415, T)) + 0.1 * torch.randn(T)\n",
    "q_1 = 1.0 - torch.sigmoid(torch.linspace(-3, 7, T))\n",
    "q_1 = q_1 / q_1.sum()\n",
    "q_2 = torch.sigmoid(torch.linspace(-7, 3, T))\n",
    "q_2 = q_2 / q_2.sum()\n",
    "\n",
    "w, y = gained_dot_product_attention(x, q_1, q_2, gain_1, gain_2)\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "with plt.xkcd():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(q_1, label=\"$\\mathbf{q_1}$\", c=\"m\")\n",
    "    plt.plot(q_2, label=\"$\\mathbf{q_2}$\", c=\"y\")\n",
    "    plt.plot(w, label=\"$\\mathbf{w}$\", c=\"r\")\n",
    "    plt.ylim(-0.1, max(q_1.max(),q_2.max()))\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, label=\"$\\mathbf{x}$\", c=\"blue\")\n",
    "    plt.plot(5*x*w, label=\"$5\\mathbf{w}*\\mathbf{x}$\", c=\"red\")\n",
    "    plt.ylim(-x.abs().max(), x.abs().max())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5XzqdCgx17D"
   },
   "source": [
    "The provided plot illustrates the impact of the gain factor on the attention weights for different input dimensions. The key observations and interpretations are as follows:\n",
    "\n",
    "1. Sparsity and Selectivity:\n",
    "   - The softmax includes both an exponentiation that exaggerates large values, and a normalization which ensures that the weights sum to 1. Thus, other weights decrease as the biggest weights increase.\n",
    "   - This increased selectivity can focus on the most relevant features.\n",
    "   \n",
    "2. Gain Factor Influence:\n",
    "   - For small gains, the attention weights are evenly distributed across the input dimensions.\n",
    "   - As the gain factor increases, the attention weights become sparser: more concentrated and peaked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LR3HMwmyBN7"
   },
   "source": [
    "# Section 2. Self-attention: math\n",
    "\n",
    "Where do these gain factors come from?\n",
    "\n",
    "In dot product attention, the weights were $\\mathbf{w}=\\text{softmax}(\\mathbf{z}\\cdot Q)$, a function of an external $\\mathbf{z}$ and fixed matrix $Q$. (Above we used a 2-dimensional $\\mathbf{z}$.)\n",
    "\n",
    "In the *self-attention* mechanism used in Transformers, the attentional weights are instead functions of the same input $\\mathbf{x}$ that is later modulated by these weights.\n",
    "\n",
    "A very simple version would simply set $\\mathbf{z}=\\mathbf{x}$, so\n",
    "$$\\mathbf{w}=\\text{softmax}(\\mathbf{x}\\cdot Q)$$\n",
    "\n",
    "However, to provide more flexibility, we allow $Q$ to *also* depend on the input: $Q=W_q\\mathbf{x}$. For even more flexibility, we select only specific aspects of the input to serve as gain modulation,\n",
    "$$\\mathbf{z}=W_k\\mathbf{x}$$\n",
    "so\n",
    "$$\\mathbf{w}=\\text{softmax}(W_k \\mathbf{x}\\,\\mathbf{x}^\\top W_q)$$\n",
    "Notice that the weights are themselves products of the input!\n",
    "\n",
    "One final specialization is to allow multiple outputs, each being different projections of the weighted inputs: $y=\\mathbf{w}\\cdot W_v\\mathbf{x}$. This use of multiple features is known as *multi-head* attention.\n",
    "\n",
    "This is commonly described as a query-key-value framework. These various learnable projections of the input are given names:\n",
    "$$\n",
    "Q=W_q\\mathbf{x}\\ \\ \\ \\ \\text{query}\\\\\n",
    "K=W_k\\mathbf{x}\\ \\ \\ \\ \\ \\ \\text{key}\\\\\n",
    "V=W_v\\mathbf{x}\\ \\ \\ \\ \\text{value}\n",
    "$$\n",
    "\n",
    "The self-attention mechanism is then usually written as:\n",
    "\n",
    "$$\\mathbf{y}=\\text{softmax}\\,(\\frac{QK^\\top}{\\sqrt{d}})V$$\n",
    "\n",
    "where we get a big weight on inputs with a strong match between Query and Key, and that weight is applied to a particular set of Values. (For different dimensions $d$ of attended features, there's also a scaling by $\\sqrt{d}$ in the softmax for convenience.)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Putting all this together, the self-attention is then\n",
    "$$\n",
    "\\mathbf{y}=\\text{softmax}(\\tfrac{1}{\\sqrt{d}}W_q\\mathbf{x}\\mathbf{x}^\\top W_k^\\top)W_v\\mathbf{x}\n",
    "$$\n",
    "This equation shows that **the attention mechanism in transformers is a composition of multiplication, sparsification, normalization, and another multiplication** --- all special microarchitecture operations!\n",
    "\n",
    "|![alt](https://thegradient.pub/content/images/2020/09/attention-block.jpg) |![alt](https://thegradient.pub/content/images/2020/09/gnn-block.jpg)|\n",
    "|-|-|\n",
    "\n",
    "**Revision Note: update notation in figure, use only left panel**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vFAYt_Jlpae"
   },
   "source": [
    "### Exercise 2. Implement self-attention\n",
    "\n",
    "In this exercise, you will implement the Attention operation from the transformer architecture, and test it against pre-written code.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Assume that you are given the queries $Q$, keys $K$, and values $V$, each as projections of the inputs $W\\mathbf{x}$.\n",
    "\n",
    "These objects will have multiple features (heads) for each element. Denoting $B$ as the batch size, $H$ as the number of heads, $n$ as the embedding dimension, and $d$ as the dimensionality of the queries, keys, and values (for simplicity, we assume they have the same dimensionality), the objects we need to combine are\n",
    "\n",
    "$$\\text{Query} \\ Q\\in \\mathcal{R}^{B \\times H \\times d \\times n}\\\\\n",
    "\\text{Key} \\ K\\in \\mathcal{R}^{B \\times H \\times d \\times n}\\\\\n",
    "\\text{Value} \\ V\\in \\mathcal{R}^{B \\times H \\times d \\times n}\n",
    "$$\n",
    "The input to the softmax, $Q K^T/\\sqrt{d}$, is sometimes called the attention Score,\n",
    "$$\n",
    "\\text{Score}\\ S\\in \\mathcal{R}^{B \\times H \\times d \\times d}\\\\\n",
    "\\text{Attention}\\ \\mathbf{y}\\in \\mathcal{R}^{B \\times H \\times d \\times n}\n",
    "$$\n",
    "\n",
    "**NOTES:**\n",
    "$K$, $Q$, $V$ are multi-dimensional tensors. Since `torch.matmul`'s matrix multiplication does not support tensors, we can either use Einstein-Sum `einsum` or Batch-MatMul `bmm` for the MatMul operation. It is very important to keep in mind the notation and that we are first performing the dot-product (reduction) on the embedding dimension! The softmax is also applied along the context ($d$) dimension.\n",
    "\n",
    "\n",
    "\n",
    "**Revision note: explain embedding dimension $n$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSJh8zUcqHJA"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\" Scaled dot product attention\n",
    "    Args:\n",
    "        Q: queries (B, H, d, n)\n",
    "        K: keys (B, H, d, n)\n",
    "        V: values (B, H, d, n)\n",
    "    Returns:\n",
    "        Attention tensor (B, H, d, n), Scores (B, H, d, d)\n",
    "    Notes:\n",
    "        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim\n",
    "    \"\"\"\n",
    "\n",
    "    assert K.shape == Q.shape and K.shape == V.shape, \"Queries, Keys and Values must have the same shape\"\n",
    "    B, H, d, n = K.shape  # batch_size, num_heads, key-query dim, embedding dim\n",
    "    scale = math.sqrt(d)\n",
    "\n",
    "    #################################################\n",
    "    # # TODO Scaled dot product attention\n",
    "    # # You should choose between torch.bmm or torch.einsum\n",
    "    # # Remove or comment out the lines you don't need\n",
    "    # Fill remove the following line of code one you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: implement full version of scaled dot product attention.\")\n",
    "    #################################################\n",
    "    # # START using torch.bmm #######################\n",
    "    Q_ = Q.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    K_ = K.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    V_ = V.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    Q_mm_K = ...  # dot-product reducing the n dimension\n",
    "    S = ...  # score or scaled dot product\n",
    "    S_sm = ...  # softmax\n",
    "    A = ...  # Attention\n",
    "    S = S.view(B, H, d, d) # necessary only if using torch.bmm\n",
    "    A = A.view(B, H, d, n) # necessary only if using torch.bmm\n",
    "    # # END using torch.bmm #########################\n",
    "    # # START using torch.einsum ####################\n",
    "    Q_mm_K = ...  # dot-product reducing the n dimension\n",
    "    S = ...  # score or scaled dot product\n",
    "    S_sm = ...  # softmax\n",
    "    A = ...  # Attention\n",
    "    # # END using torch.einsum ######################\n",
    "\n",
    "    assert S.shape == (B, H, d, d), \"Score tensor does not have the correct shape\"\n",
    "    assert A.shape == (B, H, d, n), \"Attention tensor does not have the correct shape\"\n",
    "    return A, S  # Attention, Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAE_350bhb6Y"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\" Scaled dot product attention\n",
    "    Args:\n",
    "        Q: queries (B, H, d, n)\n",
    "        K: keys (B, H, d, n)\n",
    "        V: values (B, H, d, n)\n",
    "    Returns:\n",
    "        Attention tensor (B, H, d, n), Scores (B, H, d, d)\n",
    "    Notes:\n",
    "        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim\n",
    "    \"\"\"\n",
    "\n",
    "    assert K.shape == Q.shape and K.shape == V.shape, \"Queries, Keys and Values must have the same shape\"\n",
    "    B, H, d, n = K.shape  # batch_size, num_heads, key-query dim, embedding dim\n",
    "    scale = math.sqrt(d)\n",
    "\n",
    "    # # START using torch.bmm #######################\n",
    "    Q_ = Q.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    K_ = K.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    V_ = V.view(B * H, d, n) # necessary only if using torch.bmm\n",
    "    Q_mm_K = torch.bmm(Q_, K_.transpose(1, 2))  # dot-product reducing the n dimension\n",
    "    S = Q_mm_K / scale  # score or scaled dot product\n",
    "    S_sm = torch.softmax(S, dim=2)  # softmax\n",
    "    A = torch.bmm(S_sm, V_)  # Attention\n",
    "    S = S.view(B, H, d, d) # necessary only if using torch.bmm\n",
    "    A = A.view(B, H, d, n) # necessary only if using torch.bmm\n",
    "    # # END using torch.bmm #########################\n",
    "    # # START using torch.einsum ####################\n",
    "    Q_mm_K = torch.einsum(\"bhdn,bhen->bhde\", Q, K)  # dot-product reducing the n dimension\n",
    "    S = Q_mm_K / scale  # score or scaled dot product\n",
    "    S_sm = torch.softmax(S, dim=-1)  # softmax\n",
    "    A = torch.einsum(\"bhde,bhen->bhdn\", S_sm, V)  # Attention\n",
    "    # # END using torch.einsum ######################\n",
    "\n",
    "    assert S.shape == (B, H, d, d), \"Score tensor does not have the correct shape\"\n",
    "    assert A.shape == (B, H, d, n), \"Attention tensor does not have the correct shape\"\n",
    "    return A, S  # Attention, Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phoKtu2Hk4-N"
   },
   "source": [
    "Here is a function to test whether your function matches the correct output for self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XOdHLW57Thn"
   },
   "outputs": [],
   "source": [
    "# Testing the function scaled_dot_product\n",
    "batch_size = 7\n",
    "n_heads = 4\n",
    "key_query_value_dim = 3\n",
    "embed_dim = 5\n",
    "tensor_shape = (batch_size, n_heads, key_query_value_dim, embed_dim)\n",
    "queries, keys, values = torch.rand(tensor_shape), torch.rand(tensor_shape), torch.rand(tensor_shape)\n",
    "your_attention, your_score = scaled_dot_product_attention(queries, keys, values)\n",
    "our_attention, our_score = scaled_dot_product_attention_solution(queries, keys, values)\n",
    "assert (your_attention == our_attention).all(), \"The two implementations should produce the same result\"\n",
    "assert (your_score == our_score).all(), \"The two implementations should produce the same result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AroMXZBYdg2H"
   },
   "source": [
    "---\n",
    "# Section 3. Inductive bias of self-attention: Sparse variable creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uu-UdtfLtKo"
   },
   "source": [
    "What is the self-attention mechanism especially good at learning?\n",
    "\n",
    "In the next exercise, we will redo some results from the paper, [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/abs/2110.10090). They show that a single self-attention head can successfully learn to represent a sparse function with low sample complexity.\n",
    "\n",
    "The sparse functions we will use in this tutorial are s-sparse AND functions: For $T$ binary innputs ('context length') and *s* pre-selected unique indices, the sequence is labeled as *True* if the value of $T$ at *all* of the chosen indices is $1$, otherwise *False*. This means that the sequence label only depends on *s* elements of the whole input.\n",
    "\n",
    "We will compare DL architectures (MLP *vs* Self-Attention) in learning this sparse boolean function $f$ with few training samples and good generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhSaqRbPNMUT"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "For this tutorial we have already defined a s-sparse AND dataset generator class `s_Sparse_AND` for you. It generates $m$ sequences with context length $T$ and sparsity $s$. (Note that Each input element is more likely to be 1 than 0 so that the labels have equal probability.) Here we visualize a few input samples and their corresponding labels. The red rectangles show the relevant indicies for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "5rwV6vIpiMEI",
    "outputId": "900863b2-d325-42a8-ab67-3e013bf8e010"
   },
   "outputs": [],
   "source": [
    "context_length = 30  # T: context length\n",
    "s_sparse = 3  # s: sparsity (number of function-relevant indices)\n",
    "n_sequences = 10  # m: number of samples (sequences)\n",
    "data_gen = s_Sparse_AND(context_length, s_sparse)\n",
    "X_, y_ = data_gen.generate(n_sequences, verbose=False)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target (function-relevant indices) indices: {correct_ids}\")\n",
    "\n",
    "plot_samples(X_, y_, correct_ids, f\"{s_sparse}_Sparse_AND samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guHDsoqCNpNg"
   },
   "source": [
    "## Exercise 2. MLP vs. Self-Attention\n",
    "\n",
    "Let's put the MLP and Self-Attention to the test, and see which of the two architectures can generalize better in this problem. We will test both on sparse function (*s* = 3) as well as a denser function (*s* = 15) for the context length of *T* = 30. We will be using a helper-function `make_train` that takes the model and hyper-parameters and will return the trained model and some results.\n",
    "\n",
    "This exercise has 4 parts:\n",
    "\n",
    "1. Create training and validation datasets for MLP, and train an MLP model on the task\n",
    "2. Repeat for the self-attention model\n",
    "3. Plot to compare the results for the two models and datasets\n",
    "4. You finally can change the sample complexity of the MLP dataset, to get to 100% accuracy.\n",
    "5. Plot the attention score (attention weights) for the transformer!\n",
    "\n",
    "**ABBREVIATIONS:**\n",
    "- suffix `_s` = sparse\n",
    "- suffix `_d` = dense\n",
    "- prefix `t_` = training\n",
    "- prefix `v_` = validation\n",
    "- `model_np` = number of parameters\n",
    "- `sat` = Self-Attention Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KUQ1jexfB0J"
   },
   "outputs": [],
   "source": [
    "# Problem hyperparameters\n",
    "context_length = 30  # T: context length\n",
    "sparse_dense = [3, 10]  # s: sparsity (number of function-relevant indices)\n",
    "B_valid = 500  # batch size for validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKnRfupzQTKn"
   },
   "source": [
    "### Part 1: MLP\n",
    "Training an MLP on the \"s-Sparse AND\" task. How does the model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IPbkEXrP1kV",
    "outputId": "b64300b7-8651-42b5-8f6a-053c600e46b6"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters for sparse MLP\n",
    "B_t_mlp = 50  # batch size for training (number of training samples)\n",
    "n_epochs = 500  # number of epochs\n",
    "s_sparse = sparse_dense[0]  # sparse\n",
    "hidden_layers = [512]  # the number of hidden units in each layer [H1, H2, ...]\n",
    "kind = \"MLP\"\n",
    "\n",
    "mlp_model = BinaryMLP(context_length, hidden_layers, 1)  # MLP model\n",
    "results_mlp_s = make_train(mlp_model, context_length, s_sparse, B_t_mlp, B_valid, n_epochs, DEVICE, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dco5-Xgre_Gs",
    "outputId": "ff637400-6a76-41c8-f8e0-f54425143154"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters for dense MLP\n",
    "B_t_mlp = 50  # batch size for training (number of training samples)\n",
    "n_epochs = 500  # number of epochs\n",
    "s_sparse = sparse_dense[1]  # dense\n",
    "hidden_layers = [512]  # the number of hidden units in each layer [H1, H2, ...]\n",
    "kind = \"MLP\"\n",
    "\n",
    "mlp_model = BinaryMLP(context_length, hidden_layers, 1)  # MLP model\n",
    "results_mlp_d = make_train(mlp_model, context_length, s_sparse, B_t_mlp, B_valid, n_epochs, DEVICE, kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOWMvz-SRg8N"
   },
   "source": [
    "### Part 2. Self-Attention\n",
    "\n",
    "Build a Transformer model and train it on the given dataset. How does the training results compare to MLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4C77n77vR6CM",
    "outputId": "8cae8490-662a-42aa-eb9c-2f059a7eee17"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters for sparse SAT\n",
    "B_t_sat = 50  # batch size for training (number of training samples)\n",
    "n_epochs = 500  # number of epochs\n",
    "s_sparse = sparse_dense[0]  # sparse\n",
    "embed_dim = 32  # embedding dimension\n",
    "n_heads = 1  # number of heads\n",
    "hidden_dim = 64  # number of hidden units\n",
    "kind = \"SAT\"\n",
    "\n",
    "sat_model_s = BinarySAT(context_length, embed_dim, n_heads, hidden_dim)  # selt-attention transformer\n",
    "results_sat_s = make_train(sat_model_s, context_length, s_sparse, B_t_sat, B_valid, n_epochs, DEVICE, kind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2o79skih0OO",
    "outputId": "7dee3f21-cbce-4be6-92f9-81fa0bde88b9"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters for dense SAT\n",
    "B_t_sat = 50  # batch size for training (number of training samples)\n",
    "n_epochs = 500  # number of epochs\n",
    "s_sparse = sparse_dense[1]  # dense\n",
    "embed_dim = 32  # embedding dimension\n",
    "n_heads = 1  # number of heads\n",
    "hidden_dim = 64  # number of hidden units\n",
    "kind = \"SAT\"\n",
    "\n",
    "sat_model_d = BinarySAT(context_length, embed_dim, n_heads, hidden_dim)  # selt-attention transformer\n",
    "results_sat_d = make_train(sat_model_d, context_length, s_sparse, B_t_sat, B_valid, n_epochs, DEVICE, kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHYb88XXTsYW"
   },
   "source": [
    "### Part 3. Coding Exercise Comparing results\n",
    "\n",
    "Here, we ask you to plot the results of the trainings above and the hyper-parameters you think are important. The goal is to show in one plot how Self-Attention Transformers and Multi-Layer Perceptons compare for both sparse and dense boolean tasks. You can use any or all of the following information in your plot:\n",
    "- number of parameters\n",
    "- training samples\n",
    "- validation accuracy\n",
    "- validation loss\n",
    "\n",
    "We have provided the results in an ordered dictionary `ordered_results` for your convenience.\n",
    "\n",
    "**Hint:** You can maximise creativity and information-communication :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zstq4cqhPOzt",
    "outputId": "5610772b-32d7-4c38-929a-93e89d82f288"
   },
   "outputs": [],
   "source": [
    "# Validation loss, accuracy, number of parameters and number of training samples\n",
    "ordered_results = results_dict(results_sat_d, results_sat_s, results_mlp_d, results_mlp_s)\n",
    "ordered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvmfOa0PnjY_"
   },
   "source": [
    "### Part 4. Sample complexity\n",
    "One measure of generalization is sample complexity. You can change the number of training samples to check if MLP can do better. Also, only train the MLP model and avoid retraining the transformer to save energy and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnwgVoGooDN-"
   },
   "source": [
    "### Part 5. Attention Weights\n",
    "\n",
    "A common figure in attention literature is the \"Attention visualization\" which shows how the model is attending to different parts of the sequence. You have already implemented the function that returns the attention weight $\\mathbf{w}$, and we will use a similar implementation for our transformer model. The blue rectangles show the target (function-relevant indices) indices. You can see for the `True` sequences (where the target label is `True`), the attention weight would push all the irrelevant values down and increase the attention to the target indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "c_24cVGMpAvK",
    "outputId": "5df6e6d7-8ee1-4900-b7e0-0c2473547fab"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters for sparse SAT\n",
    "context_length = 30  # T: context length\n",
    "B_t_sat = 50  # batch size for training (number of training samples)\n",
    "n_epochs = 500  # number of epochs\n",
    "s_sparse = 3  # sparse\n",
    "embed_dim = 32  # embedding dimension\n",
    "n_heads = 1  # number of heads\n",
    "hidden_dim = 64  # number of hidden units\n",
    "n_sequences = 50  # number of samples for ploting\n",
    "kind = \"SAT\"\n",
    "\n",
    "sat_model_s = BinarySAT(context_length, embed_dim, n_heads, hidden_dim)  # selt-attention transformer\n",
    "data_gen = make_train(sat_model_s, context_length, s_sparse, B_t_sat, B_valid, n_epochs, DEVICE, kind, False)\n",
    "X_, y_ = data_gen.generate(n_sequences, verbose=False)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target (function-relevant indices) indices: {correct_ids}\")\n",
    "with torch.no_grad():\n",
    "    w_att = weighted_attention(sat_model_s, X_.to(DEVICE)).cpu().detach()\n",
    "with plt.xkcd():\n",
    "    plot_attention_weights(w_att, correct_ids, context_length, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYScDjHJrxo1"
   },
   "source": [
    "## Sample Complexity for sparse variable creation\n",
    "\n",
    "So far, we saw that a simple transformer model can effectively learn to represent an s-sparse boolean function. Next, we would like to quantify the sample complexity of self-attention for these functions. The Figure below is from the paper [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://proceedings.mlr.press/v162/edelman22a.html). Given the time and computation limits of our tutorial, we will only show their results here, but you can find the implementation and detailed results on GitHub.\n",
    "\n",
    "They rigorously show that the number of training samples $m$ needed to achieve good performance on an s-sparse function (the 'critical sample size') grows only *logarithmically* with context length $T$.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/ssnio/nma_neuroai_d4_t4/main/static/Fig_2_ref2.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RkKObs0rUWuO"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
