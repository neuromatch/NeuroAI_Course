{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Sparsity and Sparse Coding\n",
    "\n",
    "**Week 1, Day 5: Microcircuits**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Noga Mudrik, Xaq Pitkow\n",
    "\n",
    "__Content reviewers:__ Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk, Patrick Mineault\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 1 hour 20 minutes*\n",
    "\n",
    "In this tutorial, we will discuss the notion of sparsity. In particular, we will:\n",
    "\n",
    "- Recognize various types of sparsity (population, lifetime, interaction).\n",
    "- Relate sparsity to inductive bias, interpretability, and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "link_id = \"eckvr\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D5_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "#working with data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "#interactive display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "#modeling\n",
    "from sklearn.datasets import make_sparse_coded_signal, make_regression\n",
    "from sklearn.decomposition import DictionaryLearning, PCA\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "import tensorflow as tf\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "sns.set_context('talk')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def show_slice(slice_index):\n",
    "    \"\"\"\n",
    "    Plot one slide of sequential data.\n",
    "\n",
    "    Inputs:\n",
    "    - slice_index (int): index of the slide to plot.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(data[slice_index])\n",
    "        ind = (66,133)\n",
    "        plt.scatter([ind[1]], [ind[0]], facecolors='none', edgecolors='r', marker='s', s = 100, lw = 4)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def remove_edges(ax, include_ticks = True, top = False, right = False, bottom = True, left = True):\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    if not include_ticks:\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "\n",
    "def add_labels(ax, xlabel='X', ylabel='Y', zlabel='', title='', xlim = None, ylim = None, zlim = None,xticklabels = np.array([None]),\n",
    "               yticklabels = np.array([None] ), xticks = [], yticks = [], legend = [],\n",
    "               ylabel_params = {'fontsize':19},zlabel_params = {'fontsize':19}, xlabel_params = {'fontsize':19},\n",
    "               title_params = {'fontsize':29}, format_xticks = 0, format_yticks = 0):\n",
    "  \"\"\"\n",
    "  This function add labels, titles, limits, etc. to figures;\n",
    "  Inputs:\n",
    "      ax      = the subplot to edit\n",
    "      xlabel  = xlabel\n",
    "      ylabel  = ylabel\n",
    "      zlabel  = zlabel (if the figure is 2d please define zlabel = None)\n",
    "      etc.\n",
    "  \"\"\"\n",
    "  if xlabel != '' and xlabel != None: ax.set_xlabel(xlabel, **xlabel_params)\n",
    "  if ylabel != '' and ylabel != None:ax.set_ylabel(ylabel, **ylabel_params)\n",
    "  if zlabel != '' and zlabel != None:ax.set_zlabel(zlabel,**zlabel_params)\n",
    "  if title != '' and title != None: ax.set_title(title, **title_params)\n",
    "  if xlim != None: ax.set_xlim(xlim)\n",
    "  if ylim != None: ax.set_ylim(ylim)\n",
    "  if zlim != None: ax.set_zlim(zlim)\n",
    "\n",
    "  if (np.array(xticklabels) != None).any():\n",
    "      if len(xticks) == 0: xticks = np.arange(len(xticklabels))\n",
    "      ax.set_xticks(xticks);\n",
    "      ax.set_xticklabels(xticklabels);\n",
    "  if (np.array(yticklabels) != None).any():\n",
    "      if len(yticks) == 0: yticks = np.arange(len(yticklabels)) +0.5\n",
    "      ax.set_yticks(yticks);\n",
    "      ax.set_yticklabels(yticklabels);\n",
    "  if len(legend)       > 0:  ax.legend(legend)\n",
    "\n",
    "def plot_signal(signal, title = \"Pixel's activity over time\", ylabel = '$pixel_t$'):\n",
    "    \"\"\"\n",
    "    Plot the given signal over time.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - title (str, default = \"Pixel's activity over time\"): title to give to the plot.\n",
    "    - ylabel (str, default = '$pixel_t$'): y-axis label.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots(1,1, figsize = (8,8), sharex = True)\n",
    "        ax.plot(signal, lw = 2)\n",
    "        ax.set_xlim(left = 0)\n",
    "        ax.set_ylim(bottom = 0)\n",
    "        add_labels(ax, xlabel = 'Time (Frames)',ylabel = ylabel, title = title)\n",
    "        remove_edges(ax)\n",
    "        plt.show()\n",
    "\n",
    "def plot_relu_signal(signal, theta = 0):\n",
    "    \"\"\"\n",
    "    Plot the given signal over time and its thresholded value with the given theta.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots(1,1, figsize = (8,8), sharex = True)\n",
    "        thres_x = ReLU(signal, theta)\n",
    "        ax.plot(signal, lw = 2)\n",
    "        ax.plot(thres_x, lw = 2)\n",
    "        ax.set_xlim(left = 0)\n",
    "        ax.legend(['Signal', '$ReLU_{%d}$(signal)'%theta], ncol = 2)\n",
    "        add_labels(ax, xlabel = 'Time', ylabel = 'Signal')\n",
    "        remove_edges(ax)\n",
    "        plt.show()\n",
    "\n",
    "def plot_relu_histogram(signal, theta = 0):\n",
    "    \"\"\"\n",
    "    Plot histogram of the values in the signal before and after applying ReLU operation with the given threshold.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1,2,figsize = (15,10), sharex = True, sharey = True)\n",
    "        thres_x = ReLU(signal, theta)\n",
    "        axs[0].hist(sig, bins = 100)\n",
    "        axs[1].hist(thres_x, bins = 100)\n",
    "        [remove_edges(ax) for ax in axs]\n",
    "        [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "        [ax.set_title(title) for title, ax in zip(['Before Thresholding', 'After Thresholding'], axs)]\n",
    "    plt.show()\n",
    "\n",
    "def plot_relu_signals(signal, theta_values):\n",
    "    \"\"\"\n",
    "    Plot the given signal over time and its thresholded value with the given theta values.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - theta_values (np.array): threshold parameter.\n",
    "    \"\"\"\n",
    "    #define colormap\n",
    "    with plt.xkcd():\n",
    "        cmap_name = 'viridis'\n",
    "        samples = np.linspace(0, 1, theta_values.shape[0])\n",
    "        colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "        fig, ax = plt.subplots(1,1, figsize = (8,8), sharex = True)\n",
    "        for counter, theta in enumerate(theta_values):\n",
    "          ax.plot(ReLU(signal, theta), label = '$\\\\theta = %d$'%theta, color = colors[counter])\n",
    "        ax.set_xlim(left = 0)\n",
    "        ax.legend(ncol = 5)\n",
    "        add_labels(ax, xlabel = 'Time', ylabel = '$ReLU_{\\\\theta}$(Signal)')\n",
    "        remove_edges(ax)\n",
    "\n",
    "def plot_images(images):\n",
    "    \"\"\"\n",
    "    Plot given images.\n",
    "\n",
    "    Inputs:\n",
    "    - images (list): list of 2D np.arrays which represent images.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots(1, len(images), figsize = (15,8))\n",
    "        if len(images) == 1:\n",
    "            ax.imshow(images[0])\n",
    "        else:\n",
    "            for index, image in enumerate(images):\n",
    "                ax[index].imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def plot_labeled_kurtosis(frame, frame_HT, labels = ['Frame', 'HT(frame)']):\n",
    "    \"\"\"\n",
    "    Plot kurtosis value for the frame before and after applying hard threshold operation.\n",
    "\n",
    "    Inputs:\n",
    "    - frame (np.array): given image.\n",
    "    - frame_HT (np.array): thresholded version of the given image.\n",
    "    - labels (list): list of labels to apply for the igven data.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots()\n",
    "        pd.DataFrame([kurtosis(frame.flatten()), kurtosis(frame_HT.flatten())],index = labels, columns = ['kurtosis']).plot.bar(ax = ax, alpha = 0.5, color = 'purple')\n",
    "        remove_edges(ax)\n",
    "    plt.show()\n",
    "\n",
    "def plot_temporal_difference_histogram(signal, temporal_diff):\n",
    "    \"\"\"\n",
    "    Plot histogram for the values of the given signal as well as for its temporal differenced version.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - temporal_diff (np.array): temporal differenced version of the signal.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1,2,figsize = (10,5), sharex = True, sharey = True)\n",
    "        axs[0].hist(signal, bins = 100);\n",
    "        axs[1].hist(temporal_diff, bins = 100);\n",
    "        [remove_edges(ax) for ax in axs]\n",
    "        [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "        [ax.set_title(title) for title, ax in zip(['Pixel \\n Before Diff.', 'Frame \\n After Diff.'], axs)]\n",
    "        for line in axs[0].get_children():\n",
    "            line.set_path_effects([path_effects.Normal()])\n",
    "        for line in axs[1].get_children():\n",
    "            line.set_path_effects([path_effects.Normal()])\n",
    "        plt.show()\n",
    "\n",
    "def plot_temp_diff_histogram(signal, taus, taus_list):\n",
    "    \"\"\"\n",
    "    Plot the histogram for the given signal over time and its temporal differenced versions for different values of lag \\tau.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - taus (np.array): array of tau values (lags).\n",
    "    - taus_list (list): temporal differenced versions of the given signal.\n",
    "    \"\"\"\n",
    "    #define colormap\n",
    "    cmap_name = 'cool'\n",
    "    samples = np.linspace(0, 1, taus.shape[0])\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "    with plt.xkcd():\n",
    "\n",
    "        # histograms\n",
    "        bin_edges = np.arange(0, 256, 5)  # Define bin edges from 0 to 255 with a step of 5\n",
    "\n",
    "        # Compute histogram values using custom bin edges\n",
    "        hist = [np.histogram(tau_list, bins=bin_edges)[0] for tau, tau_list in zip(taus, taus_list)]\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (20,5))\n",
    "        [ax.plot(bin_edges[:-1]*0.5 + bin_edges[1:]*0.5, np.vstack(hist)[j], marker = 'o', color = colors[j], label = '$\\\\tau = %d$'%taus[j]) for j in range(len(taus))]\n",
    "        ax.legend(ncol = 2)\n",
    "        remove_edges(ax)\n",
    "        ax.set_xlim(left = 0, right = 100)\n",
    "        add_labels(ax, xlabel = '$\\\\tau$', ylabel = 'Count')\n",
    "    plt.show()\n",
    "\n",
    "def plot_temp_diff_separate_histograms(signal, lags, lags_list, tau = True):\n",
    "    \"\"\"\n",
    "    Plot the histogram for the given signal over time and its temporal differenced versions for different values of lag \\tau or windows.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - lags (np.array): array of lags (taus or windows).\n",
    "    - lags_list (list): temporal differenced versions of the given signal.\n",
    "    - tau (bool, default = True): which regime to use (tau or window).\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        cmap_name = 'cool'\n",
    "        samples = np.linspace(0, 1, lags.shape[0])\n",
    "        colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "        fig, axs = plt.subplots(2, int(0.5*(2+lags.shape[0])),figsize = (15,10), sharex = True, sharey = False)\n",
    "        axs = axs.flatten()\n",
    "        axs[0].hist(signal, bins = 100, color = 'black');\n",
    "\n",
    "        if tau:\n",
    "            # histograms\n",
    "            bin_edges = np.arange(0, 256, 5)  # Define bin edges from 0 to 255 with a step of 5\n",
    "\n",
    "            # Compute histogram values using custom bin edges\n",
    "            hist = [np.histogram(lag_list, bins=bin_edges)[0] for lag, lag_list in zip(lags, lags_list)]\n",
    "\n",
    "            [axs[j+1].bar(bin_edges[:-1]*0.5 + bin_edges[1:]*0.5, np.abs( np.vstack(hist)[j]), color = colors[j]) for j in range(len(lags))]\n",
    "\n",
    "        else:\n",
    "            [axs[j+1].hist(np.abs(signal - diff_box_values_i), bins = 100, color = colors[j]) for j, diff_box_values_i in enumerate(lags_list)]\n",
    "\n",
    "        [remove_edges(ax) for ax in axs]\n",
    "        [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "        axs[0].set_title('Pixel \\n Before Diff.');\n",
    "        if tau:\n",
    "            [ax.set_title( '$\\\\tau =$ %.2f'%lags[j]) for  j, ax in enumerate(axs[1:]) if j < lags.shape[0]]\n",
    "        else:\n",
    "            [ax.set_title( 'Window %d'%lags[j]) for  j, ax in enumerate(axs[1:]) if j < lags.shape[0]]\n",
    "\n",
    "        for ax in axs:\n",
    "            for line in ax.get_children():\n",
    "                line.set_path_effects([path_effects.Normal()])\n",
    "            for line in ax.get_children():\n",
    "                line.set_path_effects([path_effects.Normal()])\n",
    "    plt.show()\n",
    "\n",
    "def plot_temp_diff_kurtosis(signal, lags, lags_list, tau = True):\n",
    "    \"\"\"\n",
    "    Plot the kurtosis for the given signal over time and its temporal differenced versions for different values of lag \\tau or windows.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - lags (np.array): array of lags (taus or windows).\n",
    "    - lags_list (list): temporal differenced versions of the given signal.\n",
    "    - tau (bool, default = True): which regime to use (tau or window).\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        if tau:\n",
    "            fig, ax = plt.subplots(figsize = (10,3))\n",
    "            tauskur = [kurtosis(tau_i) for tau_i in lags_list]\n",
    "            pd.DataFrame([kurtosis(signal)] + tauskur, index = ['Signal']+ ['$\\\\tau = {%d}$'%tau for tau in lags], columns = ['kurtosis']).plot.barh(ax = ax, alpha = 0.5, color = 'purple')\n",
    "            remove_edges(ax)\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize = (7,7))\n",
    "            tauskur = [kurtosis(np.abs(signal - diff_box_values_i)) for diff_box_values_i in lags_list]\n",
    "            pd.DataFrame([kurtosis(signal)] + tauskur, index = ['Signal']+ ['$window = {%d}$'%tau for tau in lags], columns = ['kurtosis']).plot.bar(ax = ax, alpha = 0.5, color = 'purple')\n",
    "            remove_edges(ax)\n",
    "\n",
    "def plot_diff_box(signal, filter, diff_box_signal):\n",
    "    \"\"\"\n",
    "    Plot signal, the window function and the resulted convolution.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): the given signal.\n",
    "    - filter (int): size of the window function.\n",
    "    - diff_box_signal (np.array): the resulted signal.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "            ax[0].plot(signal)\n",
    "            ax[0].set_title('Signal')\n",
    "            ax[1].plot(filter)\n",
    "            ax[1].set_title('Filter Function')\n",
    "            ax[2].plot(diff_box_signal)\n",
    "            ax[2].set_title('diff_box Signal')\n",
    "            plt.subplots_adjust(wspace=0.3)\n",
    "            [ax_i.set_xlabel('Time') for ax_i in ax]\n",
    "    plt.show()\n",
    "\n",
    "def plot_diff_with_diff_box(signal, windows, diff_box_values):\n",
    "    \"\"\"\n",
    "    Plot difference between given signal and diff_box ones for different windows.\n",
    "\n",
    "    Inputs:\n",
    "    - signal (np.array): given signal.\n",
    "    - windows (np.array): list of window sizes.\n",
    "    - diff_box_values (list): list for diff_box versions of the signal.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        cmap_name = 'cool'\n",
    "        samples = np.linspace(0, 1, windows.shape[0])\n",
    "        colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (15,10))\n",
    "        ax.plot(signal, label = \"Signal\")\n",
    "        [ax.plot(signal - diff_box_values_i, color = colors[j], label = 'window = %d'%windows[j]) for j, diff_box_values_i in enumerate(diff_box_values)]\n",
    "        ax.legend(ncol = 4)\n",
    "        remove_edges(ax)\n",
    "        ax.set_xlim(left = 0, right = 100)\n",
    "        add_labels(ax, xlabel = 'Time (frame)', ylabel = '$\\Delta(pixel)$')\n",
    "    plt.show()\n",
    "\n",
    "def plot_spatial_diff(frame, diff_x,  diff_y):\n",
    "    \"\"\"\n",
    "    Plot spatial differentiation of the given image with lag one.\n",
    "\n",
    "    Inputs:\n",
    "    - frame (np.array): given 2D signal (image).\n",
    "    - diff_x (np.array): spatial difference along x-axis.\n",
    "    - diff_y (np.array): spatial difference along y-axis.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1,3, figsize = (15,10))\n",
    "        diff_x_norm = (diff_x - np.percentile(diff_x, 10))/(np.percentile(diff_x, 90) - np.percentile(diff_x, 10))\n",
    "        diff_x_norm = diff_x_norm*255\n",
    "        diff_x_norm[diff_x_norm > 255] = 255\n",
    "        diff_x_norm[diff_x_norm < 0] = 0\n",
    "        axs[0].imshow(diff_x_norm.astype(np.uint8))\n",
    "        diff_y_norm = (diff_y - np.percentile(diff_y, 10))/(np.percentile(diff_y, 90) - np.percentile(diff_y, 10))\n",
    "        diff_y_norm = diff_y_norm*255\n",
    "        diff_y_norm[diff_y_norm > 255] = 255\n",
    "        diff_y_norm[diff_y_norm < 0] = 0\n",
    "        axs[1].imshow(diff_y_norm.astype(np.uint8))\n",
    "        axs[2].imshow(frame)\n",
    "        [ax.set_xticks([]) for ax in axs]\n",
    "        [ax.set_yticks([]) for ax in axs]\n",
    "        [ax.set_title(title, fontsize = 40) for title, ax in zip(['$\\Delta x$', '$\\Delta y$', 'Original'], axs)];\n",
    "\n",
    "def plot_spatial_diff_histogram(taus, taus_list_x, taus_list_y):\n",
    "    \"\"\"\n",
    "    Plot histograms for each of the spatial differenced version of the signal.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "        cmap_name = 'cool'\n",
    "        samples = np.linspace(0, 1, taus.shape[0])\n",
    "        colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "        bin_edges = np.arange(0, 256, 5)  # Define bin edges from 0 to 255 with a step of 5\n",
    "\n",
    "        hist_x = [np.histogram(tau_list.flatten() ,  bins=bin_edges)[0] for tau, tau_list in zip(taus, taus_list_x )]\n",
    "        hist_y = [np.histogram(tau_list.flatten(),  bins=bin_edges)[0] for tau, tau_list in zip(taus, taus_list_y)]\n",
    "\n",
    "        fig, axs = plt.subplots(2,1,figsize = (15,9))\n",
    "        ax = axs[0]\n",
    "        [ax.plot(bin_edges[:-1]*0.5 + bin_edges[1:]*0.5, np.vstack(hist_x)[j], marker = 'o', color = colors[j], label = '$\\\\tau = %d$'%taus[j]) for j in range(len(taus))]\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(ncol = 2)\n",
    "        remove_edges(ax)\n",
    "        ax.set_xlim(left = 0, right = 100)\n",
    "        add_labels(ax, xlabel = '$\\\\tau_x$', ylabel = 'Count', title = 'Diff. in $x$')\n",
    "\n",
    "        ax = axs[1]\n",
    "        [ax.plot(bin_edges[:-1]*0.5 + bin_edges[1:]*0.5, np.vstack(hist_y)[j], marker = 'o', color = colors[j], label = '$\\\\tau = %d$'%taus[j]) for j in range(len(taus))]\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(ncol = 2)\n",
    "        remove_edges(ax)\n",
    "        ax.set_xlim(left = 0, right = 100)\n",
    "        add_labels(ax, xlabel = '$\\\\tau_y$', ylabel = 'Count', title = 'Diff. in $y$')\n",
    "\n",
    "        fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_spatial_kurtosis(frame, diff_x, diff_y):\n",
    "    \"\"\"\n",
    "    Plot kurtosis for the signal and its spatial differenced version.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots()\n",
    "        pd.DataFrame([kurtosis(frame.flatten()), kurtosis(diff_x.flatten()), kurtosis(diff_y.flatten())],index = ['Signal', 'Diff $x$', 'Diff $y$'], columns = ['kurtosis']).plot.barh(ax = ax, alpha = 0.5, color = 'purple')\n",
    "        remove_edges(ax)\n",
    "    plt.show()\n",
    "\n",
    "def plot_spatial_histogram(frame, diff_x, diff_y):\n",
    "    \"\"\"\n",
    "    Plot histogram for values in frame and differenced versions.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1,3,figsize = (15,10), sharex = True, sharey = True)\n",
    "        axs[0].hist(np.abs(frame.flatten()), bins = 100);\n",
    "        axs[1].hist(np.abs(diff_x.flatten()), bins = 100);\n",
    "        axs[2].hist(np.abs(diff_y.flatten()), bins = 100);\n",
    "        [remove_edges(ax) for ax in axs]\n",
    "        [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "        [ax.set_title(title) for title, ax in zip(['Frame \\n Before Diff.', 'Frame \\n After Diff. x', 'Frame \\n After Diff. y'], axs)]\n",
    "\n",
    "def visualize_images_diff_box(frame, diff_box_values_x, diff_box_values_y, num_winds):\n",
    "    \"\"\"\n",
    "    Plot images with diff_box difference method.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        cmap_name = 'cool'\n",
    "        samples = np.linspace(0, 1, num_winds)\n",
    "        colors = plt.colormaps[cmap_name](samples)\n",
    "        fig, axs = plt.subplots( 2, int(0.5*(2+ len(diff_box_values_x))),  figsize = (15,15))\n",
    "        axs = axs.flatten()\n",
    "        axs[0].imshow(frame)\n",
    "        [axs[j+1].imshow(normalize(frame - diff_box_values_i)) for j, diff_box_values_i in enumerate(diff_box_values_x)]\n",
    "        remove_edges(axs[-1], left = False, bottom = False, include_ticks=  False)\n",
    "        fig.suptitle('x diff')\n",
    "\n",
    "        fig, axs = plt.subplots( 2, int(0.5*(2+ len(diff_box_values_x))),  figsize = (15,15))\n",
    "        axs = axs.flatten()\n",
    "        axs[0].imshow(frame)\n",
    "        [axs[j+1].imshow(normalize(frame.T - diff_box_values_i).T) for j, diff_box_values_i in enumerate(diff_box_values_y)]\n",
    "        remove_edges(axs[-1], left = False, bottom = False, include_ticks=  False)\n",
    "        fig.suptitle('y diff')\n",
    "\n",
    "def create_legend(dict_legend, size = 30, save_formats = ['.png','.svg'],\n",
    "                      save_addi = 'legend' , dict_legend_marker = {},\n",
    "                      marker = '.', style = 'plot', s = 500, plot_params = {'lw':5},\n",
    "                      params_leg = {}):\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "    if style == 'plot':\n",
    "        [ax.plot([],[],\n",
    "                     c = dict_legend[area], label = area, marker = dict_legend_marker.get(area), **plot_params) for area in dict_legend]\n",
    "    else:\n",
    "        if len(dict_legend_marker) == 0:\n",
    "            [ax.scatter([],[], s=s,c = dict_legend.get(area), label = area, marker = marker, **plot_params) for area in dict_legend]\n",
    "        else:\n",
    "            [ax.scatter([],[], s=s,c = dict_legend[area], label = area, marker = dict_legend_marker.get(area), **plot_params) for area in dict_legend]\n",
    "    ax.legend(prop = {'size':size},**params_leg)\n",
    "    remove_edges(ax, left = False, bottom = False, include_ticks = False)\n",
    "\n",
    "def ReLU_implemented(x, theta = 0):\n",
    "    \"\"\"\n",
    "    Calculates ReLU function for the given level of theta (implemented version of first exercise).\n",
    "\n",
    "    Inputs:\n",
    "    - x (np.ndarray): input data.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "\n",
    "    Outputs:\n",
    "    - thres_x (np.ndarray): filtered values.\n",
    "    \"\"\"\n",
    "\n",
    "    thres_x = np.maximum(x - theta, 0)\n",
    "\n",
    "    return thres_x\n",
    "\n",
    "def plot_kurtosis(theta_value):\n",
    "    \"\"\"\n",
    "    Plot kurtosis value for the signal before and after applying ReLU operation with the given threshold value.\n",
    "\n",
    "    Inputs:\n",
    "    - theta_value (int):  threshold parameter value.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        fig, ax = plt.subplots()\n",
    "        relu = kurtosis(ReLU_implemented(sig, theta_value))\n",
    "        pd.DataFrame([kurtosis(sig)] + [relu], index = ['Signal']+ ['$ReLU_{%d}$(signal)'%theta_value], columns = ['kurtosis']).plot.bar(ax = ax, alpha = 0.5, color = 'purple')\n",
    "        remove_edges(ax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def normalize(mat):\n",
    "    \"\"\"\n",
    "    Normalize input matrix from 0 to 255 values (in RGB range).\n",
    "\n",
    "    Inputs:\n",
    "    - mat (np.ndarray): data to normalize.\n",
    "\n",
    "    Outpus:\n",
    "    - (np.ndarray): normalized data.\n",
    "    \"\"\"\n",
    "    mat_norm = (mat - np.percentile(mat, 10))/(np.percentile(mat, 90) - np.percentile(mat, 10))\n",
    "    mat_norm = mat_norm*255\n",
    "    mat_norm[mat_norm > 255] = 255\n",
    "    mat_norm[mat_norm < 0] = 0\n",
    "    return mat_norm\n",
    "\n",
    "def lists2list(xss):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a single list.\n",
    "\n",
    "    Inputs:\n",
    "    - xss (list): list of lists. The list of lists to be flattened.\n",
    "\n",
    "    Outputs:\n",
    "    - (list): The flattened list.\n",
    "    \"\"\"\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"frame1.npy\", \"sig.npy\", \"reweight_digits.npy\", \"model.npy\", \"video_array.npy\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/n652y/download\", \"https://osf.io/c9qxk/download\", \"https://osf.io/ry5am/download\", \"https://osf.io/uebw5/download\", \"https://osf.io/t9g2m/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"6ce619172367742dd148cc5830df908c\", \"f3618e05e39f6df5997f78ea668f2568\", \"1f2f3a5d08e13ed2ec3222dca1e85b60\", \"ae20e6321836783777c132149493ec70\", \"bbd1d73eeb7f5c81768771ceb85c849e\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Introduction to sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Introduction to sparsity\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'Qr9wnQ4iqvQ'), ('Bilibili', 'BV1Vb421n7W3')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_introduction_to_sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Sparsity in Neuroscience and Artificial Intelligence\n",
    "\n",
    "**Sparse** means rare or thinly spread out.\n",
    "\n",
    "Neuroscience and AI both use notions of sparsity to describe efficient representations of the world. The concept of sparsity is usually applied to two things: **sparse activity** and **sparse connections**.\n",
    "\n",
    "**Sparse activity** means that only a small number of neurons or units are active at any given time. Computationally, this helps reduce energy consumption and focuses computational efforts on the most salient features of the data. In modeling the world, it reflects how natural scenes usually contain a small number out of all possible objects or features.\n",
    "\n",
    "**Sparse connections** refers to the selective interaction between neurons or nodes, for example, through a graph that is far from fully connected. Computationally, this can focus processing power where it is most needed. In modeling the world, it reflects that many represented objects or features properties directly relate only to a few others.\n",
    "\n",
    "In the brain, we see sparsity of both types, and researchers have made many theories about its benefits. In AI, regularization often imposes sparsity, providing a variety of performance and generalization benefits.\n",
    "\n",
    "This tutorial will explore a few of these benefits. First, we will calculate how various simple computations affect sparsity, and then we will examine how sparsity can affect inferences.\n",
    "\n",
    "\n",
    "\n",
    "### How can we quantify sparsity?\n",
    "- **$\\ell_0$ pseudo-norm** -- the count of non-zero values: $\\|\\mathbf{x}\\|_{\\ell_0}=\\sum_i \\mathbb{1}_{x_i\\neq0}$ where $\\mathbb{1}$ is an indicator function equal to 1 if and only if the argument is true. This is more difficult to work with than other proxy measures that are convex or differentiable.\n",
    "\n",
    "- **$\\ell_1$ norm** -- the sum of the absolute values: $\\|\\mathbf{x}\\|_{\\ell_1}=\\sum_i |x_i|$\n",
    "\n",
    "- **Kurtosis** -- a fourth-order measure that quantifies the \"tails\" of the distribution: $\\kappa=\\mathbb{E}_x \\left(\\frac{x-\\mu}{\\sigma}\\right)^4$. Higher kurtosis indicates both longer tails and smaller values and, thus, greater sparsity.\n",
    "\n",
    "- **Cardinality** -- in this context, refers to the number of active (non-zero) features in a model, which determines its sparsity and affects its ability to capture and express complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Sparsity notion visualization.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/static/sparsity.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "--------------------------\n",
    "# Section 2: Computing and altering sparsity\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 10 minutes.*\n",
    "\n",
    "Under what scenarios do we encounter sparsity in real life? In this first section, we will explore various contexts and methods through which natural sparsity manifests in real-world signals. We will focus on the effects of nonlinearity and temporal derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 2.1: Sparsity via nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Natural sparsity\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', '_ZHtLgFeiD4'), ('Bilibili', 'BV19f421B77m')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_natural_sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.1: Sparsity as the result of thresholding\n",
    "\n",
    "In this exercise, we will understand how a nonlinearity can increase sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For the first exercise, we will analyze a video of a bird in San Francisco to extract temporal sparsity. You can navigate through the video using the slider provided below.\n",
    "\n",
    "Specifically, to explore temporal sparsity (i.e., sparsity across time, also called lifetime sparsity), we will focus on the activity of a particular pixel, the one marked in red, across various frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Execute the cell to see the interactive widget!\n",
    "\n",
    "data = np.load('video_array.npy')\n",
    "slider = IntSlider(min=0, max=data.shape[0] - 1, step=1, value=0, description= 'Time Point')\n",
    "interact(show_slice, slice_index=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's look at the marked pixel's activity over time and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot of change in pixel's value over time\n",
    "\n",
    "sig = np.load('sig.npy')\n",
    "plot_signal(sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Write a function called \"**ReLU**\" that receives 2 inputs:\n",
    "\n",
    "- $\\mathbf{x}$: a 1d numpy array of $p$ floats.\n",
    "\n",
    "- $\\theta$: a scalar threshold.\n",
    "\n",
    "The function should return a numpy array called `thres_x` such that for each element $j$:\n",
    "\n",
    "$$\n",
    "\\text{thres-x}_j =\n",
    "\\begin{cases}\n",
    "x_j - \\theta & \\text{if } x_j \\geq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Apply the ReLU  function to the signal \"**sig**\" with a threshold of $\\theta =  150$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def ReLU(x, theta = 0):\n",
    "    \"\"\"\n",
    "    Calculates ReLU function for the given level of theta.\n",
    "\n",
    "    Inputs:\n",
    "    - x (np.ndarray): input data.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "\n",
    "    Outputs:\n",
    "    - thres_x (np.ndarray): filtered values.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    ## Fill out the following then remove\n",
    "    raise NotImplementedError(\"Student exercise: complete `thres_x` array calculation as defined.\")\n",
    "    ###################################################################\n",
    "\n",
    "    thres_x = ...\n",
    "\n",
    "    return thres_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_e8a7baa1.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot your results\n",
    "\n",
    "plot_relu_signal(sig, theta = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us also take a look at the aggregated plot, which takes threshold parameter values from 0 to 240 with step 20 and plots the ReLU version of the signal for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Threshold value impact on ReLU version of the signal\n",
    "\n",
    "theta_values = np.arange(0, 240, 20)\n",
    "plot_relu_signals(sig ,theta_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Finally, let's calculate the kurtosis value to estimate the signal's sparsity compared to its version passed through the ReLU function.\n",
    "\n",
    "Try to gradually increase the threshold parameter ($\\theta$) from 0 to 240 in intervals of 20 and plot the result for each value. How does the threshold affect the sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Kurtosis value comparison\n",
    "\n",
    "slider = IntSlider(min=0, max=240, step=20, value=0, description='Threshold')\n",
    "interact(plot_kurtosis, theta_value = slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "    <summary>Kurtosis value behaviour</summary>\n",
    "    You might notice that, at first, the kurtosis value decreases (around till $\\theta = 140$), and then it drastically increases (reflecting the desired sparsity property). If we take a closer look at the kurtosis formula, it measures the expected value (average) of standardized data values raised to the 4th power. That being said, if the data point lies in the range of standard deviation, it doesn’t contribute to the kurtosis value almost at all (something less than 1 to the fourth degree is small), and most of the contribution is produced by extreme outliers (lying far away from the range of standard deviation). So, the main characteristic it measures is the tailedness of the data - it will be high when the power of criticality of outliers will overweight the “simple” points (as kurtosis is an average metric for all points). What happens is that with $\\theta \\le 120$, outliers don't perform that much to the kurtosis.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_sparsity_as_the_result_of_thresholding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 2.2: Sparsity from temporal differentiation\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 20 minutes.*\n",
    "\n",
    "In this section, you will increase temporal sparsity in a natural 1D time series by *temporal differencing*. Changes in the world are sparse and thus tend to be especially informative, so computations highlighting those changes can be beneficial.\n",
    "\n",
    "This could be implemented by feedforward inhibition in a neural circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Temporal differentiation\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'VaDioX2ZOqI'), ('Bilibili', 'BV1yE421P73H')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_temporal_differentiation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Temporal differencing signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Denote the pixel value at time $t$ by $pixel_t$. Mathematically, we define the (absolute) temporal differences as\n",
    "$$\\Delta_t = |pixel_t - pixel_{t-1}|$$\n",
    "\n",
    "In code, define these absolute temporal differences to compute `temporal_diff` by applying `np.diff` on the signal `sig` and then applying `np.abs` to get absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete temporal differentiation.\")\n",
    "###################################################################\n",
    "temporal_diff = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_0b47c17f.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the result\n",
    "plot_signal(temporal_diff, title = \"\", ylabel = \"$| pixel_t - pixel_{t-1} | $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's take a look at the histogram of the temporal difference values as well as kurtosis values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Histograms for the signal and its temporal differences\n",
    "plot_temporal_difference_histogram(sig, temporal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Kurtosis values for the signal and its temporal differences\n",
    "plot_labeled_kurtosis(sig, temporal_diff, labels = ['Signal', 'Temporal \\n Diff.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_temporal_differencing_signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.3: Changes over longer delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "What happens if we look at differences at longer delays $\\tau>1$?\n",
    "$$\\Delta_t(\\tau) = |pixel_t - pixel_{t-\\tau}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this exercise, we will explore the effects of increasing $\\tau$ values on the sparsity of the temporal differentiation signal.\n",
    "\n",
    "1) Create an array of 10 different $\\tau$ values: $taus = [1, 11, 21... , 91]$.\n",
    "\n",
    "2) Create a list called **taus_list** composed of 10 arrays where each array is the temporal differentiation with a different interval $\\tau$.\n",
    "\n",
    "3) Compare the histograms of temporal differences for each different tau. Plot these histograms together, with all histograms using the same bins.\n",
    "\n",
    "Pay attention: here, it is NOT recommended to use the built-in `np.diff` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `taus` and `taus_list`.\")\n",
    "###################################################################\n",
    "num_taus = 10\n",
    "\n",
    "# create taus\n",
    "taus = np.linspace(1, 91, ...).astype(int)\n",
    "\n",
    "# create taus_list\n",
    "taus_list = [np.abs(sig[...:] - sig[:-tau]) for tau in taus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_5a7b462a.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot your results\n",
    "plot_temp_diff_histogram(sig, taus, taus_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let us look at the histograms for each $\\tau$ separately, as well as for the kurtosis values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Histogram plots for different values of $\\tau$\n",
    "plot_temp_diff_separate_histograms(sig, taus, taus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot sparsity (kurtosis) for different values of $\\tau$\n",
    "plot_temp_diff_kurtosis(sig, taus, taus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_increasing_the temporal_differcing_intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exploring temporal differencing with a box filter\n",
    "\n",
    "Instead of differences separated by delay $\\tau$, we'll compute differences between one value and the average over a *range* of delays. This is closer to what brains actually do. Here we'll use a box filter for the average.\n",
    "\n",
    "We define a `diff_box` function, which accepts a signal and the window size as inputs. Internally, it computes the difference between the signal and the average signal over a delayed time window. Observe the results for `window = 10`. We will explore changes at different times scales by choosing different window sizes and then comparing the raw signal with its `diff_box` temporal derivatives for each size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Filter visualization\n",
    "\n",
    "def diff_box(data, window, pad_size = 4):\n",
    "    filter = np.concatenate([np.repeat(0,pad_size), np.repeat(0,window), np.array([1]), np.repeat(-1,window), np.repeat(0,pad_size)]).astype(float)\n",
    "    filter /= np.sum(filter**2)**0.5\n",
    "\n",
    "    filter_plus_sum =  filter[filter > 0].sum()\n",
    "    filter_min_sum = np.abs(filter[filter < 0]).sum()\n",
    "    filter[filter > 0] *= filter_min_sum/filter_plus_sum\n",
    "    diff_box = np.convolve(data, filter, mode='full')[:len(data)]\n",
    "    diff_box[:window] = diff_box[window]\n",
    "    return diff_box, filter\n",
    "\n",
    "window = 10\n",
    "diff_box_signal, filter = diff_box(sig, window)\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_e1 = np.arange(len(filter))\n",
    "    plot_e2 = np.arange(len(filter)) + 1\n",
    "    plot_edge_mean = 0.5*(plot_e1 + plot_e2)\n",
    "    plot_edge = lists2list( [[e1 , e2] for e1 , e2 in zip(plot_e1, plot_e2)])\n",
    "    ax.plot(plot_edge, np.repeat(filter, 2), alpha = 0.3, color = 'purple')\n",
    "    ax.scatter(plot_edge_mean, filter, color = 'purple')\n",
    "    add_labels(ax,ylabel = 'filter value', title = 'box filter', xlabel = 'Frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "1. Why do you think the filter is asymmetric?\n",
    "2. How might a filter influence the sparsity patterns observed in data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_e286253f.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot signal and its temporal derivative on a longer timescale\n",
    "plot_diff_box(sig, filter, diff_box_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we will define the window for 10 different values: `windows = [1,11,21,...91]` and calculate corresponding `diff_box` signal versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Define different window values\n",
    "windows = np.linspace(1, 91, 10)\n",
    "diff_box_values = [diff_box(sig, int(window))[0] for window in windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualize temporal differences for different window sizes\n",
    "\n",
    "plot_diff_with_diff_box(sig, windows, diff_box_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Histogram for each of the window size\n",
    "\n",
    "plot_temp_diff_separate_histograms(sig, windows, diff_box_values, tau = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Compare sparsity (measured by kurtosis) for different window sizes\n",
    "\n",
    "plot_temp_diff_kurtosis(sig, windows, diff_box_values, tau = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "1. What do you observe about the kurtosis after applying the temporal differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_45a72023.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_exploring_temporal_differencing_with_a_box_filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "At the very end of the notebook, you can find a bonus section on spatial sparsity. We recommend you check it out if you are going to have time after going through the main materials of this day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Sparse coding\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 35 minutes.*\n",
    "\n",
    "*Sparse coding* is a coding strategy where the inputs are represented by a linear combination of features, most with zero coefficients but a few that are nonzero or active. Often, this is applied with an overcomplete basis set: we use more features that are necessary to cover the input space. This concept is often applied to sensory inputs, such as images or sounds, where the goal is to find a concise and efficient representation that captures the essential features of the input.\n",
    "\n",
    "In Section 3.1, we assume that the basis set is fixed and known, and we just want to find sparse coefficients (or activities) that best explain the input data.\n",
    "\n",
    "In Section 3.2, we then describe how to find a good basis set for use in sparse coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 3.1: Finding coefficients for sparse codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Sparse coding\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'TkfPF24CZfk'), ('Bilibili', 'BV1sJ4m1u7Ed')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_sparse_coding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Neuroscience measures of sparse responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In a pivotal experiment [1] at Johns Hopkins University, Hubel and Wiesel implanted an electrode into the visual cortex of a living cat and measured its electrical activity in response to displayed images.\n",
    "\n",
    "Despite prolonged exposure to various images, no significant activity was recorded.\n",
    "\n",
    "However, unexpectedly, when the slide was inserted and removed from the projector, the neurons responded robustly. This led to the discovery of neurons highly sensitive to **edge orientation and location**, providing the first insights into the type of information coded by neurons.\n",
    "\n",
    "\n",
    "[1] *Hubel DH, Wiesel TN (1962). [\"Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/) J. Physiol.160, 106-154.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion\n",
    "\n",
    "1. What implications do these specialized properties of neural representation hold for our understanding of visual perception?\n",
    "2. How might these findings inform computational models of visual processing in artificial systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Computational Neuroscience model of sparse coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In 1996, Olshausen and Field [2] demonstrated that sparse coding could be a good model of the early visual system, particularly in V1. They found that neuron selectivity in the visual cortex could be explained through **sparse coding**, where only *a small subset of neurons responded to specific features or patterns*.\n",
    "Receptive fields learned through this objective looked like orientation-specific edge detectors, like those in biological visual systems, as we will see below.\n",
    "\n",
    "[2] *Olshausen BA, Field DJ (1996). [\"Emergence of simple-cell receptive field properties by learning a sparse code for natural images.\"](https://www.nature.com/articles/381607a0) Nature 381: 607-609.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### $\\ell_0$ pseudo-norm regularization to promote sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The $\\ell_0$ pseudo-norm is defined as the number of non-zero features in the signal. Particularly, let $h \\in \\mathbb{R}^{J}$ be a vector with $J$ \"latent activity\" features. Then:\n",
    "$$\\|h\\|_0 = \\sum_{j = 1}^J \\mathbb{1}_{h_{j} \\neq 0}$$\n",
    "Hence, the  $\\|\\ell\\|_0$ pseudo-norm can be used to promote sparsity by adding it to a cost function to \"punish\" the number of non-zero features.\n",
    "\n",
    "Let's assume that we have a simple linear model where we want to capture the observations $y$ using the linear model $D$ (which we will later call dictionary). $D$'s features (columns) can have sparse weights denoted by $h$. This is known as a generative model, as it generates the sensory input. \n",
    "\n",
    "For instance, in the brain, $D$ can represent a basis of neuronal networks while $h$ can capture their sparse time-changing contributions to the overall brain activity (e.g. see the dLDS model in [3]). \n",
    "\n",
    "Hence, we are looking for the weights $h$ under the assumption that:\n",
    "$$ y = Dh + \\epsilon$$\n",
    "where $\\epsilon$ is an *i.i.d* Gaussian noise with zero mean and std of $\\sigma_\\epsilon$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$.\n",
    "\n",
    "To enforce that $h$ is sparse, we penalize the number of non-zero features with penalty $\\lambda$. We thus want to solve the following minimization problem:\n",
    "$$\n",
    "\\hat{h} = \\arg \\min_x \\|y - Dh \\|_2^2 + \\lambda \\|h\\|_0\n",
    "$$\n",
    "\n",
    "[3] Mudrik, N., Chen, Y., Yezerets, E., Rozell, C. J., & Charles, A. S. (2024). Decomposed linear dynamical systems (dlds) for learning the latent components of neural dynamics. Journal of Machine Learning Research, 25(59), 1-44."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Features to data.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/static/dictionary.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the above figure and throughout this tutorial, we will use the following definitions:\n",
    "\n",
    "- **$D$:** Dictionary\n",
    "- **Features:** Refers to the columns of $D$ (i.e., basis elements)\n",
    "- **Basis:** Refers to the collection of features\n",
    "- **$h$:** A sparse vector assigning weights to the elements of $D$\n",
    "\n",
    "These definitions will help clarify the terminology used in discussing the concepts of dictionary learning and sparse coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### How can we find the sparse vector $h$ given a dictionary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "One method to find a sparse solution for a linear decomposition with $\\ell_0$ regularization is OMP (Orthogonal Matching Pursuit) [4]. As explained in the video, OMP is an approximate method to find the best matching features to represent a target signal.\n",
    "\n",
    "OMP iteratively selects the features that best correlate with the remaining part of the signal, updates the remaining part by subtracting the contribution of the chosen features, and repeats until the remaining part is minimized or the desired number of features is selected.\n",
    "\n",
    "In this context, a \"dictionary\" is a collection of features that we use to represent the target signal. These features are like building blocks, and the goal of the OMP algorithm is to find the right combination of these blocks from the dictionary to match the target signal best.\n",
    "\n",
    "[4] Pati, Y. C., Rezaiifar, R., & Krishnaprasad, P. S. (1993, November). [\"Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition.\"](https://www.khoury.northeastern.edu/home/eelhami/courses/EE290A/OMP_Krishnaprasad.pdf) In Proceedings of 27th Asilomar conference on signals, systems, and computers (pp. 40-44). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3: OMP algorithm\n",
    "\n",
    "Now, we will explore the Orthogonal Matching Pursuit (OMP) algorithm with increasing sparsity levels and examine how pixel values are captured by different frequencies.\n",
    "\n",
    "**We will follow the following steps:**\n",
    "\n",
    "1. Generate sinusoidal features with varying frequencies ranging from 0.001 to 1, applying each frequency to a time array. These features will serve in this exercise as the dictionary.\n",
    "   \n",
    "2. Implement the OMP algorithm with increasing sparsity levels. Feel free to change the number of non-zero coefficients (in `n_nonzero_coefs` to explore its effect.\n",
    "\n",
    "3. Fit the generated sinusoidal signals to the dictionary of frequencies you defined.\n",
    "\n",
    "4. Evaluate how well each sparsity level captures the variations in features.\n",
    "\n",
    "5. Explore the results to understand the trade-off between sparsity and the accuracy of signal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete OMP algorithm preparation.\")\n",
    "###################################################################\n",
    "T_ar = np.arange(len(sig))\n",
    "\n",
    "#100 different frequency values from 0.001 to 1, then apply each frequency on `T_ar`\n",
    "freqs = np.linspace(0.001, 1, ...)\n",
    "set_sigs = [np.sin(...*...) for f in freqs]\n",
    "\n",
    "# define 'reg' --- an sklearn object of OrthogonalMatchingPursuit, and fit it to the data, where the frequency bases are the features and the signal is the label\n",
    "reg = OrthogonalMatchingPursuit(fit_intercept = True, n_nonzero_coefs = 10).fit(np.vstack(set_sigs).T, sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_f770be90.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Observe the plot of 3 example signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot of 3 basis signals\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(3,1,sharex = True, figsize = (10,5), sharey = True)\n",
    "    axs[0].plot(set_sigs[0], lw = 3)\n",
    "    axs[1].plot(set_sigs[1], lw = 3)\n",
    "    axs[2].plot(set_sigs[-1], lw = 3)\n",
    "    [remove_edges(ax) for ax in axs]\n",
    "    [ax.set_xlim(left = 0) for ax in axs]\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, run the following code to plot the basis features and the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualize basis features and signal reconstruction\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(2,2, figsize = (15,10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    sns.heatmap(np.vstack(set_sigs), ax = axs[0])\n",
    "    axs[0].set_yticks(np.arange(0,len(freqs)-.5, 4)+ 0.5)\n",
    "    axs[0].set_yticklabels(['$f = %.4f$'%freqs[int(j)] for j in np.arange(0,len(freqs)-0.5, 4)], rotation = 0)\n",
    "    add_labels(axs[0], xlabel = 'Time', ylabel= 'Basis Features', title = 'Frequency Basis Features')\n",
    "\n",
    "    axs[1].plot(sig, label = 'Original', lw = 4)\n",
    "    axs[1].plot(reg.predict(np.vstack(set_sigs).T), lw = 4, label = 'Reconstruction')\n",
    "    remove_edges(axs[1])\n",
    "    axs[1].legend()\n",
    "    add_labels(axs[1], xlabel = 'Time', ylabel= 'Signal', title = 'Reconstruction')\n",
    "    axs[1].set_xlim(left = 0)\n",
    "\n",
    "    axs[2].stem(freqs, reg.coef_)\n",
    "    remove_edges(axs[2])\n",
    "    add_labels(axs[2], xlabel = 'Frequencies', ylabel= 'Frequency weight', title = 'Frequency Components Contributions')\n",
    "    axs[2].set_xlim(left = 0)\n",
    "\n",
    "    num_colors = np.sum(reg.coef_ != 0)\n",
    "    cmap_name = 'winter'\n",
    "    samples = np.linspace(0, 1, num_colors)\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "    colors_expand = np.zeros((len(reg.coef_), 4))\n",
    "    colors_expand[reg.coef_!= 0] = np.vstack(colors)\n",
    "\n",
    "    axs[-1].plot(sig, label = 'Original', color = 'black')\n",
    "    [axs[-1].plot(reg.coef_[j]*set_sigs[j] + reg.intercept_, label = '$f = %.4f$'%f, lw = 4, color = colors_expand[j]) for j, f in enumerate(freqs) if  reg.coef_[j] != 0]\n",
    "    remove_edges(axs[-1])\n",
    "    axs[-1].legend(ncol = 4)\n",
    "    axs[-1].set_xlim(left = 0)\n",
    "    add_labels(axs[-1], xlabel = 'Time', ylabel= 'Signal', title = 'Contribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we will explore the effect of increasing the number of non-zero coefficients.\n",
    "Below, please run OMP with increasing numbers of non-zero coefficients, from 1 to 101 in intervals of 5.\n",
    "We will then compare the accuracy and reconstruction performance of each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title OMP for different cardinality\n",
    "\n",
    "# define a list or numpy array of optional cardinalities from 1 to 51 in intervals of 5.\n",
    "cardinalities = np.arange(1,101,5)\n",
    "\n",
    "# For each of the optional cardinalities, run OMP using the pixel's signal from before. Create a list called \"regs\" that includes all OMP's fitted objects\n",
    "regs = [OrthogonalMatchingPursuit(fit_intercept = True, n_nonzero_coefs = card).fit(np.vstack(set_sigs).T, sig) for card in cardinalities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's observe the effect of the cardinality on the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Cardinality impact on reconstruction quality\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (15,5))\n",
    "    ax = axs[0]\n",
    "    ax.plot(cardinalities, [reg.score(np.vstack(set_sigs).T, sig) for reg in regs], marker = '.')\n",
    "\n",
    "    ax.set_xlim(left = 0)\n",
    "    ax.set_ylim(bottom = 0)\n",
    "    add_labels(ax, ylabel = 'coefficient of determination', xlabel  = 'Cardinality')\n",
    "    remove_edges(ax)\n",
    "    mean_er = np.vstack([np.mean((reg.predict(np.vstack(set_sigs).T) - sig)**2)**0.5 for reg in regs])\n",
    "\n",
    "    axs[1].plot(cardinalities, mean_er)\n",
    "    axs[1].set_xlim(left = 0)\n",
    "    axs[1].set_ylim(bottom = 0)\n",
    "    remove_edges(axs[1])\n",
    "    add_labels(axs[1], ylabel = 'rMSE', xlabel  = 'Cardinality')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**How would you choose the ideal cardinality?**\n",
    "\n",
    "- Hint: Look for an 'ankle' in complexity vs. sparsity plots, which indicates a balance between expressivity and sparsity. Discuss these observations with your group to determine the most effective cardinality.\n",
    "\n",
    "**How does the above affect generalization?**\n",
    "\n",
    "- Hint: Consider how learning fewer but most relevant features from noisy data might help the model generalize better to new, unseen data.\n",
    "\n",
    "**What is your conclusion?**\n",
    "\n",
    "- Hint: Discuss how sparsity, which emphasizes few significant features, improves the model's robustness and utility in real-world scenarios and the considerations needed when setting the level of sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Identifying Sparse Contributions Across Frequencies\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(5, int(len(cardinalities)/5), figsize = (15,20),sharey = False, sharex = True)\n",
    "    axs = axs.flatten()\n",
    "    for j, (ax,reg) in enumerate(zip(axs,regs)):\n",
    "      ax.stem(freqs, reg.coef_)\n",
    "      remove_edges(ax)\n",
    "      add_labels(ax, xlabel = 'Frequencies', ylabel= 'Frequency weight', title = 'Cardinality %d'%cardinalities[j])\n",
    "      ax.set_xlim(left = 0)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Challenges with OMP**\n",
    "\n",
    "While OMP is a common and simple practice for finding the sparse representation of a dictionary of features, it presents multiple challenges. The $\\ell_0$ norm is not convex, making it hard to identify the sparse vector. Another challenge is computational complexity. Approaches addressing these issues exist. Some of them replace the $\\ell_0$ norm with its $\\ell_1$ approximation, promoting sparsity while retaining convexity, making it easier to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_omp_algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 3.2: Hidden features as dictionary learning\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 55 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Dictionary learning\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'k6W4acjVOkg'), ('Bilibili', 'BV17M4m1U7fQ')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_dictionary_learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### What if we do not know the dictionary of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In OMP, we've assumed that we already know the features and are solely focused on identifying which sparse combination can best describe the data. However, in real-world scenarios, we frequently lack knowledge about the fundamental components underlying the data. In other words, we don't know the features and must learn them.\n",
    "\n",
    "**Dictionary Learning:**\n",
    "To address this problem, Dictionary Learning aims to simultaneously learn both the dictionary of features and the sparse representation of the data. It iteratively updates the dictionary and the sparse codes until convergence, typically minimizing a reconstruction error term along with a sparsity-inducing regularizer.\n",
    "\n",
    "**How is it related to the brain?**\n",
    "Dictionary Learning draws parallels to the visual system's ability to extract features from raw sensory input. In the brain, the visual system processes raw visual stimuli through hierarchical layers, extracting increasingly complex features at each stage. Similarly, Dictionary Learning aims to decompose complex data into simpler components, akin to how the visual system breaks down images into basic visual features like edges and textures.\n",
    "\n",
    "Now, let's apply Dictionary Learning to various types of data to unveil the latent components underlying it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 4: Dictionary Learning for MNIST\n",
    "\n",
    "In this exercise, we first load a new short video created from MNIST images, as shown below. We then use sklearn's `DictionaryLearning` to find the dictionary's ($D$) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In particular, we will follow the following steps:\n",
    "\n",
    "1) **Load the Video Data**: Begin by loading the 3D video data from the `reweight_digits.npy` file, which contains pixel data over time.\n",
    "\n",
    "2) **Preprocess the Video Data**: Create a copy of the video data to work on without altering the original data. Extract the total number of frames, as well as the dimensions of each frame, adjusting for any specific modifications (e.g., reducing the width by 10 columns).\n",
    "\n",
    "3) **Prepare Frames for Analysis**: Flatten each frame of the video to convert the 2D image data into 1D vectors. Store these vectors in a list, which is then converted into a NumPy array for efficient processing.\n",
    "\n",
    "4) **Initialize Dictionary Learning**: Set up a dictionary learning model with specific parameters (e.g., number of components, the algorithm for transformation, regularization strength, and random state for reproducibility).\n",
    "\n",
    "5) **Fit and Transform the Data (YOUR EXERCISE)**: Fit the dictionary learning model to the prepared video data and transform it to obtain sparse representations. This involves completing the implementation of `dict_learner.fit(...).transform(...)` by providing the necessary input arguments based on your tutorial's context.\n",
    "Pay attention, as usual, you will need to remove the placeholder `raise NotImplementedError` and replace it with the appropriate function call to `fit` and `transform` the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove.\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `D_transformed`.\")\n",
    "###################################################################\n",
    "\n",
    "# Video_file is a 3D array representing pixels X pixels X time\n",
    "video_file = np.load('reweight_digits.npy')\n",
    "\n",
    "# Create a copy of the video_fire array\n",
    "im_focus = video_file.copy()\n",
    "\n",
    "# Get the number of frames in the video\n",
    "T = im_focus.shape[2]\n",
    "\n",
    "# Get the number of rows in the video\n",
    "N0 = im_focus.shape[0]\n",
    "\n",
    "# Get the number of columns in the video, leaving out 10 columns\n",
    "N1 = im_focus.shape[1] - 10\n",
    "\n",
    "# Create a copy of the extracted frames\n",
    "low_res = im_focus.copy()\n",
    "\n",
    "# Get the shape of a single frame\n",
    "shape_frame = low_res[:, :, 0].shape\n",
    "\n",
    "# Flatten each frame and store them in a list\n",
    "video_file_ar = [low_res[:, :, frame].flatten() for frame in range(low_res.shape[2])]\n",
    "\n",
    "# Create dict_learner object\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.9,\n",
    "    random_state=402,\n",
    ")\n",
    "\n",
    "# List to np.array\n",
    "video_v = np.vstack(video_file_ar)\n",
    "\n",
    "# Fit and transform `video_v`\n",
    "D_transformed = dict_learner.fit(...).transform(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_eb440839.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's visualize the identified features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualization of features\n",
    "\n",
    "with plt.xkcd():\n",
    "    num_thetas = 20\n",
    "    cmap_name = 'plasma'\n",
    "    samples = np.linspace(0, 1, len(dict_learner.components_)+1)\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "\n",
    "    num_comps = len(dict_learner.components_)\n",
    "\n",
    "    fig, axs = plt.subplots(2, int((1+num_comps)/2) , figsize = (16,4), sharey = True, sharex = True)\n",
    "    axs = axs.flatten()\n",
    "    [sns.heatmap(dict_learner.components_[j].reshape(shape_frame), square = True, cmap ='gray', robust = True, ax = axs[j]) for j in range(num_comps)]\n",
    "    titles = ['Atom %d'%j for j in range(1, num_comps +1)]\n",
    "    [ax.set_title(title, color = colors[j], fontsize = 10)\n",
    "    for j, (title, ax) in enumerate(zip(titles, axs))]\n",
    "    axs[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualization of impact of features through the time\n",
    "\n",
    "D = np.linalg.pinv(dict_learner.components_.T) @ video_v.T\n",
    "D_transformed_norm = D.T.copy()\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(figsize = (16,3))\n",
    "    [ax.plot(D_transformed_norm[:,j], color = colors[j], lw = 5, label = 'Atom %d'%(j+1))\n",
    "     for j in range(num_comps)]\n",
    "    remove_edges(ax)\n",
    "    ax.set_xlim(left = 0)\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "\n",
    "    add_labels(ax, xlabel = 'Time', ylabel = 'Coefficients', title = 'Features Ceofficients')\n",
    "    create_legend({'Atom %d'%(j+1): colors[j] for j in range(num_comps)},\n",
    "                  params_leg = {'ncol': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's compare the components to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title PCA components visualization\n",
    "\n",
    "data_mat = np.vstack(video_file_ar)\n",
    "# Assuming your data is stored in a variable named 'data_mat' (N by p numpy array)\n",
    "# where N is the number of samples and p is the number of features\n",
    "\n",
    "# Create a PCA object with 20 components\n",
    "pca = PCA(n_components=15)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(data_mat)\n",
    "\n",
    "# Access the 20 PCA components\n",
    "pca_components = pca.components_\n",
    "\n",
    "pca_componens_images = [pca_components[comp,:].reshape(shape_frame) for comp in range(num_comps)]\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(2, int((1+num_comps)/2) , figsize = (16,4), sharey = True, sharex = True)\n",
    "    axs = axs.flatten()\n",
    "    [sns.heatmap(im, ax = axs[j], square = True, cmap ='gray', robust = True) for j,im in enumerate(pca_componens_images)]\n",
    "    titles = ['Atom %d'%j for j in range(1, num_comps +1)]\n",
    "    [ax.set_title(title, color = colors[j], fontsize = 10) for j, (title, ax) in enumerate(zip(titles, axs))]\n",
    "    axs[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_dictionary_learning_for_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Identifying sparse visual fields in natural images using sparse coding\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 1 hour 5 minutes.*\n",
    "\n",
    "In this section we will understand the basis of visual field perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Sparse visual fields\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'zqmrxK_N6Jw'), ('Bilibili', 'BV1Pi421v7Wk')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_sparse_visual_fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Dictionary Learning for CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will use the CIFAR10 dataset. Let's, at first, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Load CIFAR-10 dataset\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's create a short video of these images switching. Now, we will detect the underlying visual fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Create & explore video\n",
    "\n",
    "num_image_show = 50\n",
    "img_index = np.random.randint(0, len(x_train), num_image_show)\n",
    "x_train_ar = np.copy(x_train)[img_index]\n",
    "video_gray = 0.2989 * x_train_ar[:, :, :,0] + 0.5870 * x_train_ar[:, :, :,1] + 0.1140 * x_train_ar[:, :, :,2]\n",
    "\n",
    "def show_slice_CIFAR(slice_index):\n",
    "    with plt.xkcd():\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(data_CIFAR[slice_index] , cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "data_CIFAR = np.repeat(video_gray, 1, axis = 0)\n",
    "\n",
    "# Create a slider to select the slice\n",
    "slider = IntSlider(min=0, max=data_CIFAR.shape[0] - 1, step=1, value=0, description='Frame')\n",
    "\n",
    "# Connect the slider and the function\n",
    "interact(show_slice_CIFAR, slice_index=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's try to extract the visual field from the images. Look at the components identified below and compare the ones identified by PCA to those identified by Olshausen & Field via sparse dictionary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Components comparison.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/static/components.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Following Olshausen & Field's paper, we will demonstrate how to receive visual field components that emerge from real-world images.\n",
    "\n",
    "We expect to find features that are:\n",
    "1.   Spatially localized.\n",
    "2.   Oriented.\n",
    "3.   Selective to structure at different spatial scales.\n",
    "\n",
    "We would like to understand the receptive field properties of neurons in the visual cortex by applying sparsity to identify the basic fundamental components underlying natural images.\n",
    "\n",
    "We will reshape the CIFAR data we loaded before so that the time (frame) axis is the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "video_file = data_CIFAR.transpose((1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For the exercise, we will focus on a small subset of the data so that you can understand the idea. We will later load better results trained for a longer time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will first set some parameters to take a small subset of the video. In the cell below, you can explore parameters that are set by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set default parameters\n",
    "\n",
    "size_check =  4 # 32 for the full size\n",
    "\n",
    "# how many images we want to consider? (choose larger numbers for better results)\n",
    "num_frames_include =  80\n",
    "\n",
    "# set number of dictionary features\n",
    "dict_comp = 8\n",
    "\n",
    "# set number of model iterations\n",
    "max_iter = 100\n",
    "\n",
    "num_frames_include = np.min([num_frames_include , x_train.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's visualize the small subset we are taking for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Images visualization\n",
    "\n",
    "# let's choose some random images to focus on\n",
    "img_index = np.random.randint(0, len(x_train), num_frames_include)\n",
    "x_train_ar = np.copy(x_train)[img_index]\n",
    "\n",
    "# change to grayscale\n",
    "video_gray = 0.2989 * x_train_ar[:, :, :,0] + 0.5870 * x_train_ar[:, :, :,1] + 0.1140 * x_train_ar[:, :, :,2]\n",
    "\n",
    "# update data and create short movie\n",
    "data_CIFAR = np.repeat(video_gray, 1, axis = 0)\n",
    "\n",
    "# Create a slider to select the slice\n",
    "slider = IntSlider(min=0, max=data_CIFAR.shape[0] - 1, step=1, value=0, description='Frame')\n",
    "\n",
    "# Connect the slider and the function\n",
    "interact(show_slice_CIFAR, slice_index=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 5: Find the latent components by training the dictionary learner\n",
    "Training time will take around 1 minute.\n",
    "\n",
    "*Please follow the following steps:*\n",
    "\n",
    "**Choose Alpha ($\\alpha$)**:\n",
    "   - Each group member should select a unique value for $\\alpha$ between approximately ~0.0001 and ~1.\n",
    "   - $\\alpha$ controls the sparsity of the learned dictionary components in the DictionaryLearning algorithm.\n",
    "   - **Discussion**: Before running the model, discuss your expectations on how different $\\alpha$ values might affect the sparsity and quality of learned features.\n",
    "\n",
    "**Apply Dictionary Learning**:\n",
    "   - Use the chosen $\\alpha$ to fit the DictionaryLearning model (`dict_learner`).\n",
    "   - Compare and document how each $\\alpha$ (selected by different group members)  influences the sparsity and quality of learned dictionary components. \n",
    "   *Note: Each member should choose only one $\\alpha$ due to computational constraints.*\n",
    "\n",
    "**Visualize Results**:\n",
    "   - Plot heatmap visualizations (`sns.heatmap`) to examine individual dictionary components for each $\\alpha$ value.\n",
    "   - Plot coefficients over time (`D_transformed`) to analyze and compare how they vary across different $\\alpha$ values.\n",
    "\n",
    "**Group Discussion**:\n",
    "   - Compare your results with peers who selected different $\\alpha$ values.\n",
    "   - Discuss and interpret the impact of $\\alpha$ on sparsity and feature representation.\n",
    "   - Summarize findings on which $\\alpha$ values appear most effective for achieving desired sparsity and representation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Get the number of frames in the video\n",
    "T = video_file.shape[2]\n",
    "\n",
    "# Get the number of rows in the video\n",
    "N0 = video_file.shape[0]\n",
    "\n",
    "# Get the number of columns in the video, leaving out 10 columns\n",
    "N1 = video_file.shape[1] - 10\n",
    "\n",
    "# Get the shape of a single frame\n",
    "shape_frame = video_file[:, :, 0].shape\n",
    "\n",
    "# Flatten each frame and store them in a list\n",
    "num_frames_include = np.min([num_frames_include, video_file.shape[2]])\n",
    "\n",
    "size_check = np.min([size_check, video_file.shape[0]])\n",
    "\n",
    "video_file_ar = [video_file[:size_check, :size_check, frame].flatten() for frame in range(num_frames_include)]\n",
    "\n",
    "###################################################################\n",
    "## Fill out the following then remove.\n",
    "raise NotImplementedError(\"Student exercise: please choose an $\\alpha$ value here. Recommended $0.0001 <= \\alpha <= 1$\")\n",
    "###################################################################\n",
    "\n",
    "alpha = ...\n",
    "\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=dict_comp, transform_algorithm = 'lasso_lars', transform_alpha=alpha,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "D_transformed = dict_learner.fit_transform(np.vstack(video_file_ar))\n",
    "\n",
    "with plt.xkcd():\n",
    "    num_rows = 3\n",
    "    num_cols =  int(( num_frames_include + 2)/num_rows)\n",
    "    shape_frame = (size_check,size_check)\n",
    "\n",
    "    cmap_name = 'plasma'\n",
    "    samples = np.linspace(0, 1, len(dict_learner.components_)+1)\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "    num_comps = len(dict_learner.components_)\n",
    "\n",
    "    vmin =  np.min(np.vstack(dict_learner.components_))\n",
    "    vmax = np.max(np.vstack(dict_learner.components_))\n",
    "\n",
    "\n",
    "    [sns.heatmap(dict_learner.components_[j].reshape(shape_frame), ax = axs[j], square = True, vmin = vmin, vmax = vmax,\n",
    "                  cmap = 'PiYG', center = 0, cbar = False)  for j in range(num_comps)]\n",
    "\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "\n",
    "    titles = ['Atom %d'%j for j in range(1, num_comps +1)];\n",
    "    [ax.set_title(title,  fontsize = 40) for j, (title, ax) in enumerate(zip(titles, axs))]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,5))\n",
    "    [ax.plot(D_transformed[:,j],  lw = 5, label = 'Atom %d'%(j+1))\n",
    "      for j in range(num_comps)]\n",
    "    remove_edges(ax)\n",
    "    ax.set_xlim(left = 0)\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "\n",
    "    add_labels(ax, xlabel = 'Time', ylabel = 'Coefficients', title = \"Features' Coefficients\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Get the number of frames in the video\n",
    "T = video_file.shape[2]\n",
    "\n",
    "# Get the number of rows in the video\n",
    "N0 = video_file.shape[0]\n",
    "\n",
    "# Get the number of columns in the video, leaving out 10 columns\n",
    "N1 = video_file.shape[1] - 10\n",
    "\n",
    "# Get the shape of a single frame\n",
    "shape_frame = video_file[:, :, 0].shape\n",
    "\n",
    "# Flatten each frame and store them in a list\n",
    "num_frames_include = np.min([num_frames_include, video_file.shape[2]])\n",
    "\n",
    "size_check = np.min([size_check, video_file.shape[0]])\n",
    "\n",
    "video_file_ar = [video_file[:size_check, :size_check, frame].flatten() for frame in range(num_frames_include)]\n",
    "\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "dict_learner = DictionaryLearning(\n",
    "    n_components=dict_comp, transform_algorithm = 'lasso_lars', transform_alpha=alpha,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "D_transformed = dict_learner.fit_transform(np.vstack(video_file_ar))\n",
    "\n",
    "with plt.xkcd():\n",
    "    num_rows = 3\n",
    "    num_cols =  int(( num_frames_include + 2)/num_rows)\n",
    "    shape_frame = (size_check,size_check)\n",
    "\n",
    "    cmap_name = 'plasma'\n",
    "    samples = np.linspace(0, 1, len(dict_learner.components_)+1)\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "    num_comps = len(dict_learner.components_)\n",
    "\n",
    "    vmin =  np.min(np.vstack(dict_learner.components_))\n",
    "    vmax = np.max(np.vstack(dict_learner.components_))\n",
    "\n",
    "\n",
    "    [sns.heatmap(dict_learner.components_[j].reshape(shape_frame), ax = axs[j], square = True, vmin = vmin, vmax = vmax,\n",
    "                  cmap = 'PiYG', center = 0, cbar = False)  for j in range(num_comps)]\n",
    "\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "\n",
    "    titles = ['Atom %d'%j for j in range(1, num_comps +1)];\n",
    "    [ax.set_title(title,  fontsize = 40) for j, (title, ax) in enumerate(zip(titles, axs))]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,5))\n",
    "    [ax.plot(D_transformed[:,j],  lw = 5, label = 'Atom %d'%(j+1))\n",
    "      for j in range(num_comps)]\n",
    "    remove_edges(ax)\n",
    "    ax.set_xlim(left = 0)\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "\n",
    "    add_labels(ax, xlabel = 'Time', ylabel = 'Coefficients', title = \"Features' Coefficients\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_find_the_latent_components_by_training_the_dictionary_learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Due to our efforts to limit computation time, we focused on a small area of the images, and we only considered a limited number of them with a restricted number of iterations. As a result, the quality of the outcomes was poor. For your exploration, we are providing better results obtained from training on a larger amount of images.\n",
    "This is important for discovering the components that better resemble the biological visual field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Load full model & visualize features\n",
    "res = np.load('model.npy', allow_pickle = True).item()\n",
    "comps = res['dict_learner_components_']\n",
    "\n",
    "with plt.xkcd():\n",
    "    shape_frame = (32,32)\n",
    "\n",
    "    cmap_name = 'plasma'\n",
    "    samples = np.linspace(0, 1, len(comps)+1)\n",
    "    colors = plt.colormaps[cmap_name](samples)\n",
    "    num_comps = len(comps)\n",
    "\n",
    "    vmin =  np.min(np.vstack(comps))\n",
    "    vmax = np.max(np.vstack(comps))\n",
    "\n",
    "    fig, axs = plt.subplots(2, int(np.ceil((num_comps+1)/2)) , figsize = (20,15), sharey = True, sharex = True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "\n",
    "    [axs[j].imshow(comps[j].reshape(shape_frame), vmin = 0)  for j in range(num_comps)]\n",
    "\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "\n",
    "    titles = ['Atom %d'%j for j in range(1, num_comps +1)];\n",
    "    [ax.set_title(title,  fontsize = 40, color = colors[j]) for j, (title, ax) in enumerate(zip(titles, axs))]\n",
    "    axs[-1].set_visible(False)\n",
    "    axs[-2].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Sparse Activity of the components ($h$)\n",
    "\n",
    "D_transformed = res['X_transformed']\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(figsize = (20,5))\n",
    "    [ax.plot(D_transformed[:,j], color = colors[j], lw = 5, label = 'Atom %d'%(j+1))\n",
    "      for j in range(num_comps)]\n",
    "    remove_edges(ax)\n",
    "    ax.set_xlim(left = 0)\n",
    "    [ax.set_yticks([]) for ax in axs]\n",
    "    [ax.set_xticks([]) for ax in axs]\n",
    "\n",
    "    add_labels(ax, xlabel = 'Time', ylabel = 'Coefficients', title = \"Features' Coefficients\")\n",
    "    create_legend({'Atom %d'%(j+1): colors[j] for j in range(num_comps)}, params_leg = {'ncol': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_dictionary_learning_for_cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, you've identified sparse components underlying natural images, similar to the approach taken by Olshausen and Field (1996) [2], which led to the widespread use of Dictionary Learning in research. \n",
    "\n",
    "Interestingly, applying sparsity to the number of active components is crucial and closely related to how the visual cortex, including V1, processes visual information.\n",
    "\n",
    "As discovered in seminal experiments by Hubel and Wiesel [mentioned before in 1], neurons in V1 are tuned to respond optimally to specific, sparse patterns of visual stimuli—this represents an efficient data representation that minimizes redundancy while maximizing information content. \n",
    "\n",
    "This mechanism demonstrates that neurons in V1 intensely fire for certain edge orientations and light patterns, closely paralleling the sparse coding models used today in AI.\n",
    "\n",
    "- How do you think these insights about V1 might enhance the algorithms you will develop in the field of neuroAI?\n",
    "- What other neural mechanisms do you think utilize sparsity and can inspire AI research?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "*Estimated timing of tutorial: 1 hour 20 minutes.*\n",
    "\n",
    "In this tutorial, we first discovered the notion of \"sparsity\" by looking at the signal and its temporal difference. We also introduced kurtosis as one of the main metrics used to measure sparsity and visually observed this effect by looking at the histograms of pixel values. Then, we introduced the notion of \"sparse coding\" by deriving fundamental units (basis) that constitute complex signals. For that, we used the OMP algorithm and compared the identified features to those found by applying PCA. In the end, we used the algorithm for 2 datasets: MNIST & CIFAR10.\n",
    "\n",
    "In the next tutorial, we will look at another essential operation to be realized in brains & machines - normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus Section 1: Sparsity in space\n",
    "\n",
    "In this section, we will focus on spatial sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus Video 1: Sparsity in space\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'bGH4NiD1Ye4'), ('Bilibili', 'BV1sT421a7MX')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_sparsity_in_space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Coding Excercise 1: Hard thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Below, you can find the 1st frame, which we will focus on for the spatial differentiation (i.e., differentiation in space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualization of the frame\n",
    "\n",
    "frame = np.load('frame1.npy')\n",
    "plot_images([frame])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So far, we have focused on applying ReLU, which can also be considered as a form of **'soft thresholding'**. Now, we will apply a different kind of thresholding called '**hard thresholding**'.\n",
    "\n",
    "Here, you will have to write a function called **hard_thres** that receives 2 inputs:\n",
    "\n",
    "1) 'frame' - 2d array as input of $p \\times p$ floats.\n",
    "\n",
    "2) 'theta' - threshold scalar values.\n",
    "\n",
    "The function has to return a 2d array called **frame_HT** with the same dimensions as of **frame**, however, all values smaller than $\\theta$ need to be set to 0, i.e.:\n",
    "\n",
    " $$\n",
    "\\text{frame-HT}_{[i,j]} =\n",
    "\\begin{cases}\n",
    "frame_{[i,j]} & \\text{if } frame_{[i,j]} \\geq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for every pixel [i,j] in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def hard_thres(frame, theta):\n",
    "    \"\"\"\n",
    "    Return a hard thresholded array of values based on the parameter value theta.\n",
    "\n",
    "    Inputs:\n",
    "    - frame (np.array): 2D signal.\n",
    "    - theta (float, default = 0): threshold parameter.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    ## Fill out the following then remove.\n",
    "    raise NotImplementedError(\"Student exercise: complete hard thresholding function.\")\n",
    "    ###################################################################\n",
    "    frame_HT = frame.copy()\n",
    "    frame_HT[...] = 0\n",
    "    return frame_HT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_f1932a08.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Test your function\n",
    "\n",
    "frame_HT = hard_thres(frame, 150)\n",
    "plot_images([frame, frame_HT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Notice how many pixels became of the same **violet** color. Above, the violet color corresponds to 0. Hence, these pixels are the **thresholded** ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we will define the variable **frame_HT** as the first frame after the hard threshold operator, using the above **hard_thres** function with a threshold of 80% of the frame values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe 80% of thresholded signal\n",
    "\n",
    "low_perc = np.percentile(frame, 80)\n",
    "frame_HT = hard_thres(frame, low_perc)\n",
    "plot_images([frame, frame_HT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Notice that even more pixels become violet as we threshold the vast majority of the signal (80%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will now explore the differences before and after applying hard thresholding using the histogram. Generate a histogram for the `frame_HT`. It might take a while as the image is of high quality and it contains a lot of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Histogram comparison\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(1,2,figsize = (15,5), sharey = True)\n",
    "    axs[0].hist(frame.flatten(), bins = 100);\n",
    "    axs[1].hist(frame_HT.flatten(), bins = 100);\n",
    "\n",
    "    #utils\n",
    "    [ax.set_yscale('log') for ax in axs]\n",
    "    [remove_edges(ax) for ax in axs]\n",
    "    [add_labels(ax, ylabel = 'Count', xlabel = 'Value') for ax in axs]\n",
    "    [ax.set_title(title) for title, ax in zip(['Before Thresholding', 'After Thresholding'], axs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us also take a look at the kurtosis to assess sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Kurtosis values comparison\n",
    "\n",
    "plot_labeled_kurtosis(frame, frame_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_hard_thresholding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus Section 2: Spatial differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, we will apply differentiation operations to spatial data. Spatial differentiation is common in identifying image patterns and extracting meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus Video 2: Spatial differentiation\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'IpMDGo7WYpM'), ('Bilibili', 'BV1nS411N7fK')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_spatial_differentiation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Coding Exercise 2: Spatial filtering\n",
    "\n",
    "Let's apply spatial differentiation on the 1st frame to look at the result.\n",
    "\n",
    "Below, apply 1-frame spatial differentiation on **frame**.\n",
    "\n",
    "Define a variable **diff_x** that stores the x-axis differentiation and **diff_y** that stores the y-axis differentiation. Compare the results by presenting the differentiated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `diff_x` and `diff_y`.\")\n",
    "###################################################################\n",
    "diff_x = np.diff(frame, axis = ...)\n",
    "diff_y = np.diff(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_ba516464.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the result\n",
    "plot_spatial_diff(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, as usual, let's look at the histogram and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the histogram\n",
    "plot_spatial_histogram(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the kurtosis values\n",
    "plot_spatial_kurtosis(frame, diff_x, diff_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Multi-Step Analysis in Spatial Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Rather than focusing on differences between adjacent pixels, we'll employ more comprehensive filters for spatial differencing. Specifically, we will apply the differentiation using the `diff_box` method, which is similar to our approach in the temporal case. Define the `diff_box_values_y` variable for the specified windows for the first frame in both `x` and `y' directions.\n",
    "\n",
    "In contrast to the temporal differencing, we aim for the `diff_box` to be symmetric in the spatial case. This is important because, in spatial analysis, we need to consider the context from both sides of a given point, as opposed to time-series data, where there is a clear directional flow from past to present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "![Comparison of filters.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/static/filters.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Define a **symmetric** box filter in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def diff_box_spatial(data, window, pad_size = 4):\n",
    "    \"\"\"\n",
    "    Implement & apply spatial filter on the signal.\n",
    "\n",
    "    Inputs:\n",
    "    - data (np.ndarray): input signal.\n",
    "    - window (int): size of the window.\n",
    "    - pad_size (int, default = 4): size of pad around data.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    ## Fill out the following then remove\n",
    "    raise NotImplementedError(\"Student exercise: add pad around to complete filter operation.\")\n",
    "    ###################################################################\n",
    "    filter = np.concatenate([np.repeat(0, ...), np.repeat(-1, window), np.array([1]), np.repeat(-1, window), np.repeat(0, ...)]).astype(float)\n",
    "\n",
    "    #normalize\n",
    "    filter /= np.sum(filter**2)**0.5\n",
    "\n",
    "    #make sure the filter sums to 0\n",
    "    filter_plus_sum =  filter[filter > 0].sum()\n",
    "    filter_min_sum = np.abs(filter[filter < 0]).sum()\n",
    "    filter[filter > 0] *= filter_min_sum/filter_plus_sum\n",
    "\n",
    "    #convolution of the signal with the filter\n",
    "    diff_box = np.convolve(data, filter, mode='full')[:len(data)]\n",
    "    diff_box[:window] = diff_box[window]\n",
    "    return diff_box, filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_b78a81d4.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's observe the filter. How it differs from the temporal one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualization of the filter\n",
    "window = 10\n",
    "diff_box_signal, filter = diff_box_spatial(sig, window)\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_e1 = np.arange(len(filter))\n",
    "    plot_e2 = np.arange(len(filter)) + 1\n",
    "    plot_edge_mean = 0.5*(plot_e1 + plot_e2)\n",
    "    plot_edge = lists2list( [[e1 , e2] for e1 , e2 in zip(plot_e1, plot_e2)])\n",
    "    ax.scatter(plot_edge_mean, filter, color = 'purple')\n",
    "    ax.plot(plot_edge, np.repeat(filter, 2), alpha = 0.3, color = 'purple')\n",
    "    add_labels(ax,ylabel = 'Filter value', title = 'Box Filter', xlabel = 'Space (pixel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's apply the filter to the data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## Fill out the following then remove.\n",
    "raise NotImplementedError(\"Student exercise: complete calcualtion of `diff_box_values_y`.\")\n",
    "###################################################################\n",
    "\n",
    "num_winds = 10\n",
    "windows = np.linspace(2,92,num_winds)\n",
    "rows = frame.shape[0]\n",
    "cols = frame.shape[1]\n",
    "\n",
    "diff_box_values_x = [np.array([diff_box_spatial(frame[row], int(window))[0]  for row in range(rows)]) for window in windows]\n",
    "\n",
    "diff_box_values_y = [np.array([diff_box_spatial(frame[:, ...], int(window))[0] for col in range(cols)]) for window in windows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D5_Microcircuits/solutions/W1D5_Tutorial1_Solution_a4b46e71.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the results\n",
    "\n",
    "visualize_images_diff_box(frame, diff_box_values_x, diff_box_values_y, num_winds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Kurtosis values comparison\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(figsize = (17,7))\n",
    "    tauskur = [kurtosis(np.abs(frame - diff_box_values_i).flatten()) for diff_box_values_i in diff_box_values_x]\n",
    "    tauskur_y = [kurtosis(np.abs(frame.T - diff_box_values_i).flatten()) for diff_box_values_i in diff_box_values_y]\n",
    "\n",
    "    df1 = pd.DataFrame([kurtosis(sig)] + tauskur, index = ['Signal']+ ['$window = {%d}$'%tau for tau in windows], columns = ['kurtosis x'])\n",
    "    df2 = pd.DataFrame([kurtosis(sig)] + tauskur_y, index = ['Signal']+ ['$window = {%d}$'%tau for tau in windows], columns = ['kurtosis y'])\n",
    "    dfs = pd.concat([df1,df2], axis = 1)\n",
    "    dfs.plot.barh(ax = ax, alpha = 0.5, color =['purple', 'pink'])\n",
    "    remove_edges(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_spatial_filtering\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
