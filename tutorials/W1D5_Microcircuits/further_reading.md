# Suggested further readings 

## Tutorial 1: Sparsity and Sparse Coding

- [Effects of monocular deprivation in kittens](http://hubel.med.harvard.edu/papers/HubelWiesel1964NaunynSchmiedebergsArchExpPatholPharmakol.pdf)
- [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://www.nature.com/articles/381607a0)
- [Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition](https://www.khoury.northeastern.edu/home/eelhami/courses/EE290A/OMP_Krishnaprasad.pdf)

## Tutorial 2: Normalization

- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Probabilistic Models of the Brain: Perception and Neural Function. Natural Image Statistics and Divisive Normalization](https://direct.mit.edu/books/edited-volume/2733/chapter-abstract/73923/Natural-Image-Statistics-and-Divisive?redirectedFrom=fulltext)
- [Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes](https://arxiv.org/abs/1611.04520)
- [Flexible gating of contextual influences in natural vision](https://pubmed.ncbi.nlm.nih.gov/26436902/)
- [Normalization as a canonical neural computation](https://www.nature.com/articles/nrn3136)
- [Attention-related changes in correlated neuronal activity arise from normalization mechanisms](https://www.nature.com/articles/nn.4572)
- [Spatially tuned normalization explains attention modulation variance within neurons](https://journals.physiology.org/doi/full/10.1152/jn.00218.2017)
- [Attention-related changes in correlated neuronal activity arise from normalization mechanisms](https://www.nature.com/articles/nn.4572)

## Tutorial 3: Attention

- [Attention and the detection of signals](https://pubmed.ncbi.nlm.nih.gov/7381367/)
- [Parallel Distributed Processing](https://mitpress.mit.edu/9780262680530/parallel-distributed-processing/)
- [Attention is all you need](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- [Inductive biases and variable creation in self-attention mechanisms](https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf)
- [Transformer as a Graph Neural Network](https://docs.dgl.ai/en/0.8.x/tutorials/models/4_old_wines/7_transformer.html)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)