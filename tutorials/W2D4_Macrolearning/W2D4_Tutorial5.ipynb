{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/course-content-template/blob/main/tutorials/W1D2_Template/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/course-content-template/main/tutorials/W1D2_Template/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 5: Replay\n",
    "\n",
    "**Week 2, Day 4: Macro-Learning**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Hlib Solodzhuk, Ximeng Mao, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "In this tutorial, you will discover what replay is and how it helps with continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "# !pip3 install vibecheck datatops --quiet\n",
    "\n",
    "# from vibecheck import DatatopsContentReviewContainer\n",
    "# def content_review(notebook_section: str):\n",
    "#     return DatatopsContentReviewContainer(\n",
    "#         \"\",  # No text prompt - leave this as is\n",
    "#         notebook_section,\n",
    "#         {\n",
    "#             \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "#             \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "#             \"user_key\": \"y1x3mpx5\",\n",
    "#         },\n",
    "#     ).render()\n",
    "\n",
    "# feedback_prefix = \"W2D4_T5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#working with data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "\n",
    "#interactive display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#modeling\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def plot_rewards(rewards, max_rewards):\n",
    "    \"\"\"\n",
    "    Plot the rewards over time.\n",
    "\n",
    "    Inputs:\n",
    "    - rewards (list): list containing the rewards at each time step.\n",
    "    - max_rewards(list): list containing the maximum rewards at each time step.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        obtained, = plt.plot(range(len(rewards)), rewards, marker='o')\n",
    "        obtained_shadow, = plt.plot(range(len(rewards)), rewards, marker='o', color='gray', linewidth=10, alpha=0.5, zorder=0)\n",
    "        max_feedback, = plt.plot(range(len(max_rewards)), max_rewards, marker='*')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Reward Value')\n",
    "        plt.title('Reward Over Time')\n",
    "        plt.yticks(np.arange(0, 5, 1))\n",
    "        plt.xticks(np.arange(0, len(rewards), 1))\n",
    "        plt.legend([(obtained, obtained_shadow), max_feedback], [\"Obtained Reward\", \"Maximum Reward\"],\n",
    "               handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "        plt.show()\n",
    "\n",
    "def plot_confusion_matrix(rewards, max_rewards, mode = 1):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix for the chosen rewards and the maximum ones.\n",
    "\n",
    "    Inputs:\n",
    "    - rewards (list): list containing the rewards at each time step.\n",
    "    - max_rewards (list): list containing the maximum rewards at each time step.\n",
    "    - mode (int, default = 1): mode of the environment.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "      all_colors = [color for color in mode_colors[mode]]\n",
    "\n",
    "      cm = confusion_matrix(max_rewards, rewards)\n",
    "\n",
    "      missing_classes = np.setdiff1d(np.array([color_names_rewards[color_name] for color_name in all_colors]), np.unique(max_rewards + rewards))\n",
    "      for cls in missing_classes:\n",
    "          cm = np.insert(cm, cls - 1, 0, axis=0)\n",
    "          cm = np.insert(cm, cls - 1, 0, axis=1)\n",
    "\n",
    "      cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = all_colors)\n",
    "      cm.plot()\n",
    "      plt.xlabel(\"Chosen color\")\n",
    "      plt.ylabel(\"Maximum-reward color\")\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def run_dummy_agent(env):\n",
    "    \"\"\"\n",
    "    Implement dummy agent strategy: chooses random action.\n",
    "\n",
    "    Inputs:\n",
    "    - env (ChangingEnv): An environment.\n",
    "    \"\"\"\n",
    "    action = 0\n",
    "    rewards = [0]\n",
    "    max_rewards = [0]\n",
    "\n",
    "    for _ in (range(num_trials)):\n",
    "        _, reward, max_reward = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        max_rewards.append(max_reward)\n",
    "\n",
    "        #dummy agent\n",
    "        if np.random.random() < 0.5:\n",
    "            action = 1 - action #change action\n",
    "    return rewards, max_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"FirstModeAgent.pt\", \"SecondModeAgent.pt\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/zuxc4/download\", \"https://osf.io/j9kht/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"eca5aa69751dad8ca06742c819f2dc76\", \"cdd0338d0b40ade20d6433cd615aaa82\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU).\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1: Changing Environment\n",
    "\n",
    "In this section we will introduce environment which can be extended to new states and adapted to forget some of the established ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Video 1 Name\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# video_ids = [('Youtube', '<video_id_1>'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "# tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "# tabs = widgets.Tab()\n",
    "# tabs.children = tab_contents\n",
    "# for i in range(len(tab_contents)):\n",
    "#   tabs.set_title(i, video_ids[i][0])\n",
    "# display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_changing_environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: Colorful State\n",
    "\n",
    "For this tutorial, the environment is tricky for our agent, it won't show the specific states until the agent is trained enough and after that, it will remove some of the known ones to check whether agent will remember them later. Each state will be represented by the color and its RGB values (thus, it is vector of 3 values), each color is associated with the stable reward which is unchanged throughout explorations.\n",
    "In other words, our environment will have 2 different modes: 3 states or 3 states where 2 of them are preserved from the previous mode and 1 is new one. Each time, the agent will be exposed to two different colors and should choose one of them, meaning that in the very end, the agent should learn relative positioning of the rewards for the colors (the rewards will correspond to the position of the color in the rainbow) and choose the one which is more rewarded.\n",
    "\n",
    "For this exercise, you should complete missing parts of the enviornment. You would like to inspect the auxiliary helpful lists and dictionaries in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "color_names_rewards = {\n",
    "    \"red\": 1,\n",
    "    \"yellow\": 2,\n",
    "    \"green\": 3,\n",
    "    \"blue\": 4\n",
    "}\n",
    "\n",
    "color_names_values = {\n",
    "    \"red\": [255, 0, 0],\n",
    "    \"yellow\": [255, 255, 0],\n",
    "    \"green\": [0, 128, 0],\n",
    "    \"blue\": [0, 0, 255]\n",
    "}\n",
    "\n",
    "first_mode = [\"red\", \"yellow\", \"green\"]\n",
    "second_mode = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "mode_colors = {\n",
    "    1: first_mode,\n",
    "    2: second_mode\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ChangingEnv():\n",
    "    def __init__(self, mode = 1):\n",
    "        \"\"\"Initialize changing environment.\n",
    "\n",
    "        Inputs:\n",
    "        - mode (int, default = 1): defines mode of the enviornment. Should be only 1 or 2.\n",
    "        \"\"\"\n",
    "        if mode not in [1, 2]:\n",
    "            raise ValueError(\"Mode is out of allowed range. Please consider entering 1 or 2 as digit.\")\n",
    "\n",
    "        self.mode = mode\n",
    "        self.colors = mode_colors[self.mode]\n",
    "        self.update_state()\n",
    "\n",
    "    def update_state(self):\n",
    "        \"\"\"Update state which depends on the mode of the environment.\"\"\"\n",
    "        ###################################################################\n",
    "        ## Fill out the following then remove\n",
    "        raise NotImplementedError(\"Student exercise: complete state update and choose appropriate feedback.\")\n",
    "        ###################################################################\n",
    "\n",
    "        self.first_color, self.second_color = np.random.choice(..., 2, replace = False)\n",
    "        self.color_state = np.array([self.first_color, self.second_color])\n",
    "        self.state = np.array([color_names_values[...], color_names_values[...]])\n",
    "\n",
    "    def reset(self, mode = 1):\n",
    "        \"\"\"Reset environment by updating its mode (colors to sample from). Set the first state in the given mode.\"\"\"\n",
    "        self.mode = mode\n",
    "        self.colors = mode_colors[self.mode]\n",
    "        self.update_state()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Evaluate agent's perfromance, return reward, max reward (for tracking agent's performance) and next observation.\"\"\"\n",
    "        feedback = color_names_rewards[self.color_state[...]]\n",
    "        max_feedback = np.max([..., ...])\n",
    "        self.update_state()\n",
    "        return self.state, feedback, max_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "class ChangingEnv():\n",
    "    def __init__(self, mode = 1):\n",
    "        \"\"\"Initialize changing environment.\n",
    "\n",
    "        Inputs:\n",
    "        - mode (int, default = 1): defines mode of the enviornment. Should be only 1 or 2.\n",
    "        \"\"\"\n",
    "        if mode not in [1, 2]:\n",
    "            raise ValueError(\"Mode is out of allowed range. Please consider entering 1 or 2 as digit.\")\n",
    "\n",
    "        self.mode = mode\n",
    "        self.colors = mode_colors[self.mode]\n",
    "        self.update_state()\n",
    "\n",
    "    def update_state(self):\n",
    "        \"\"\"Update state which depends on the mode of the environment.\"\"\"\n",
    "        self.first_color, self.second_color = np.random.choice(self.colors, 2, replace = False)\n",
    "        self.color_state = np.array([self.first_color, self.second_color])\n",
    "        self.state = np.array([color_names_values[self.first_color], color_names_values[self.second_color]])\n",
    "\n",
    "    def reset(self, mode = 1):\n",
    "        \"\"\"Reset environment by updating its mode (colors to sample from). Set the first state in the given mode.\"\"\"\n",
    "        self.mode = mode\n",
    "        self.colors = mode_colors[self.mode]\n",
    "        self.update_state()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Evaluate agent's perfromance, return reward, max reward (for tracking agent's performance) and next observation.\"\"\"\n",
    "        feedback = color_names_rewards[self.color_state[action]]\n",
    "        max_feedback = np.max([color_names_rewards[self.color_state[action]], color_names_rewards[self.color_state[1 - action]]])\n",
    "        self.update_state()\n",
    "        return self.state, feedback, max_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As in previous tutorial, let us test the environment with dummy agent. For this particular enviornment (in mode 1), we will use random strategy - just select one of two colors with tossing a fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "\n",
    "set_seed(42)\n",
    "num_trials = 20\n",
    "env = ChangingEnv()\n",
    "env.reset()\n",
    "rewards, max_rewards = run_dummy_agent(env)\n",
    "\n",
    "plot_rewards(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Observe that maximum reward plot is always higher than obtained reward one or coincides with it (when agent chooses more rewarded color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_colorful_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 2: A2C Agent in Changing Environment\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 10 minutes*\n",
    "\n",
    "Welcome the friend from the previous tutorial, A2C agent;) Still, it has evolved in its new form! The \"inside\"-architecture of the agent is slightly modified, LSTM cells being replaced with 1 linear layer with ReLUs on top of it. Variable `num_inputs` is changed too as now input is represented by 3-dimensional vector and not single digit. Moreover, we will make training and evaluation functions separate - as we don't have \"task\" and \"meta-space of tasks\" notions here, we don't need to keep track of this consistency and this division will benefit us with extra flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs = 9, num_actions = 2):\n",
    "        \"\"\"Initialize Actor-Critic agent.\"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        #num_actions is 2 because left/right hand\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        #num_inputs is 9 because one-hot encoding of action (2) + reward (1) + previous state (2*3 = 6)\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #hyperparameters involved in training (important to keep assigned to the agent)\n",
    "        self.learning_rate = 0.00075 #learning rate for optimizer\n",
    "        self.discount_factor = 0.91 #gamma\n",
    "        self.state_value_estimate_cost = 0.4 #beta_v\n",
    "        self.entropy_cost = 0.001 #beta_e\n",
    "\n",
    "        self.emb = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.critic_linear = nn.Linear(hidden_size, 1)\n",
    "        self.actor_linear = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Implement forward pass through agent.\"\"\"\n",
    "        #at first, input goes through embedding\n",
    "        state = F.linear(state.unsqueeze(0), self.emb.weight.clone(), self.emb.bias)\n",
    "        state = self.relu1(F.linear(state, self.linear1.weight.clone(), self.linear1.bias))\n",
    "\n",
    "        #critic -> value\n",
    "        value = F.linear(state, self.critic_linear.weight.clone(), self.critic_linear.bias)\n",
    "\n",
    "        #actor -> policy\n",
    "        policy_logits = F.linear(state, self.actor_linear.weight.clone(), self.actor_linear.bias)\n",
    "\n",
    "        return value, policy_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the cell below we define the training procedure for the A2C agent as well as its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def train_agent(env, agent, optimizer_func, mode = 1, num_gradient_steps = 1000, num_trials = 100):\n",
    "    \"\"\"Training for agent in changing colorful environment.\n",
    "    Observe that training happens for one particular mode.\n",
    "\n",
    "    Inputs:\n",
    "    - env (ChangingEnv): environment.\n",
    "    - agent (ActorCritic): particular instance of Actor Critic agent to train.\n",
    "    - optimizer_func (torch.Optim): optimizer to use for training.\n",
    "    - mode (int, default = 1): mode of the environment.\n",
    "    - num_gradient_steps (int, default = 1000): number of gradient steps to perform.\n",
    "    - num_trials (int, default = 200): number of times the agent is exposed to the environment per gradient step to be trained.\n",
    "    \"\"\"\n",
    "\n",
    "    #reset environment\n",
    "    state = env.reset(mode = mode)\n",
    "\n",
    "    #define optimizer\n",
    "    optimizer = optimizer_func(agent.parameters(), agent.learning_rate, eps = 1e-5)\n",
    "\n",
    "    for _ in range(num_gradient_steps):\n",
    "\n",
    "      #for storing variables for training\n",
    "      log_probs = []\n",
    "      values = []\n",
    "      rewards = []\n",
    "      entropy_term = torch.tensor(0.)\n",
    "\n",
    "      #start conditions\n",
    "      preceding_reward = torch.Tensor([0])\n",
    "      preceding_action = torch.Tensor([0, 0])\n",
    "\n",
    "      for trial in range(num_trials):\n",
    "          #state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!\n",
    "          full_state = torch.cat((torch.from_numpy(state.flatten() / 255).float(), preceding_reward, preceding_action), dim = 0)\n",
    "          value, policy_logits = agent(full_state)\n",
    "          value = value.squeeze(0)\n",
    "\n",
    "          #sample action from policy\n",
    "          dist = torch.distributions.Categorical(logits=policy_logits.squeeze(0))\n",
    "          action = dist.sample()\n",
    "\n",
    "          #perform action to get reward and new state\n",
    "          new_state, reward, _ = env.step(action)\n",
    "\n",
    "          #we normalize reward too\n",
    "          reward /= 4\n",
    "\n",
    "          #update preceding variables\n",
    "          preceding_reward = torch.Tensor([reward])\n",
    "          preceding_action = F.one_hot(action, num_classes=2).float()\n",
    "          state = new_state\n",
    "\n",
    "          #for training\n",
    "          log_prob = dist.log_prob(action)\n",
    "          entropy = dist.entropy()\n",
    "          rewards.append(reward)\n",
    "          values.append(value)\n",
    "          log_probs.append(log_prob)\n",
    "          entropy_term += entropy\n",
    "\n",
    "      #calculataing loss\n",
    "      Qval = 0\n",
    "      Qvals = torch.zeros(len(rewards))\n",
    "      for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + agent.discount_factor * Qval\n",
    "        Qvals[t] = Qval\n",
    "      values = torch.stack(values)\n",
    "      log_probs = torch.stack(log_probs)\n",
    "      advantage = Qvals - values\n",
    "      actor_loss = (-log_probs * advantage.detach()).mean()\n",
    "      critic_loss = advantage.pow(2).mean()\n",
    "      entropy_term = entropy_term / num_trials\n",
    "\n",
    "      #loss incorporates actor/critic terms + entropy\n",
    "      loss = actor_loss + agent.state_value_estimate_cost * critic_loss - agent.entropy_cost * entropy_term\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "def evaluate_agent(env, agent, mode = 1, num_evaluation_trials = 20):\n",
    "    \"\"\"Evaluation for agent in changing colorful environment.\n",
    "    Observe that evaluation happens for one particular mode which can differ from training one.\n",
    "\n",
    "    Inputs:\n",
    "    - env (ChangingEnv): environment.\n",
    "    - agent (ActorCritic): particular instance of Actor Critic agent to train.\n",
    "    - mode (int, default = 1): mode of the environment.\n",
    "    - num_evaluation_trials (int, default = 20): number of times the agent is exposed to the environment to evaluate it (no training happend during this phase).\n",
    "\n",
    "    Outputs:\n",
    "    - scores (list): rewards over all trials of evaluation.\n",
    "    - max_scores (list): maximum rewards over all trials of evaluation.\n",
    "    \"\"\"\n",
    "    #reset environment\n",
    "    state = env.reset(mode = mode)\n",
    "    scores = []\n",
    "    max_scores = []\n",
    "\n",
    "    #start conditions\n",
    "    preceding_reward = torch.Tensor([0])\n",
    "    preceding_action = torch.Tensor([0, 0])\n",
    "\n",
    "    for _ in range(num_evaluation_trials):\n",
    "\n",
    "      #state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!\n",
    "      full_state = torch.cat((torch.from_numpy(state.flatten() / 255).float(), preceding_reward, preceding_action), dim = 0)\n",
    "      value, policy_logits = agent(full_state)\n",
    "      value = value.squeeze(0)\n",
    "\n",
    "      #sample action from policy\n",
    "      dist = torch.distributions.Categorical(logits=policy_logits.squeeze(0))\n",
    "      action = dist.sample()\n",
    "\n",
    "      #perform action to get reward and new state\n",
    "      new_state, reward, max_reward = env.step(action)\n",
    "\n",
    "      #update preceding variables; we normalize reward too\n",
    "      preceding_reward = torch.Tensor([reward / 4])\n",
    "      preceding_action = F.one_hot(action, num_classes=2).float()\n",
    "      state = new_state\n",
    "\n",
    "      #add reward to the scores of agent\n",
    "      scores.append(reward)\n",
    "      max_scores.append(max_reward)\n",
    "\n",
    "    return scores, max_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the following code cell, let's observe the agent's performance on the first mode after being trained on it. As training of the agent takes around 3 minutes, we have provided you with already trained version (still feel free to uncomment training code to receive the same results). You will also have the opportunity to train the agent from scratch in the next section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "#define environment\n",
    "env = ChangingEnv()\n",
    "\n",
    "##UNCOMMENT TO TRAIN\n",
    "# agent = ActorCritic(hidden_size = 100)\n",
    "# optimizer_func = optim.RMSprop\n",
    "# train_agent(env, agent, optimizer_func)\n",
    "##UNCOMMENT TO TRAIN\n",
    "\n",
    "#load agent\n",
    "agent = torch.load(\"FirstModeAgent.pt\")\n",
    "\n",
    "#evaluate agent\n",
    "rewards, max_rewards = evaluate_agent(env, agent)\n",
    "plot_rewards(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Pretty nice! Let us also observe the confusion matrix. Indeed, it might reveal to us the weaknesses being incorporated in the particular colors. We will increase the number of evaluation trials to get the more statistically true results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "No specific patterns here, the only thing (which is also expected) that whenever colors are close in their rewards, agent makes more mistakes for those.\n",
    "\n",
    "Notice that the blue color is missing as it is indeed excluded from the first mode. Let us evaluate agent on the second mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 2)\n",
    "plot_rewards(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can see that green color is chosen often times when the blue one proporses higher reward (which agent doesn't really know). Let's check with confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 2, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards, mode = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As expected, agent doesn't know perfectly what to do with new color.\n",
    "\n",
    "Let's continue training the same agent already in the second mode and see whether we can improve this situation. Same goes here, you have pretrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "##UNCOMMENT TO TRAIN\n",
    "# env = ChangingEnv()\n",
    "# optimizer_func = optim.RMSprop\n",
    "# train_agent(env, agent, optimizer_func, mode = 2)\n",
    "##UNCOMMENT TO TRAIN\n",
    "\n",
    "#load agent\n",
    "agent = torch.load(\"SecondModeAgent.pt\")\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 2)\n",
    "plot_rewards(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 2, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards, mode = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Awesome, agent has learnt the relationship given in the second mode. Still, what about the first one? Did the agent forget the previously seen colors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 1)\n",
    "plot_rewards(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 1, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Oops! Introducing the blue color in the second mode messed up learnt relations between red and yellow (we didn't include yellow in the second mode). What to do? You will discover the bio-inspired mechanism which allows for correcting this behvaiour in the next section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_a2c_agent_in_changing_enviornment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 3: Replay Buffer\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 25 minutes*\n",
    "\n",
    "This section discusses the underlying biological reasoning behind replau buffer as well as proposes its code implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Video 2 Name\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# video_ids = [('Youtube', '<video_id_1>'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "# tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "# tabs = widgets.Tab()\n",
    "# tabs.children = tab_contents\n",
    "# for i in range(len(tab_contents)):\n",
    "#   tabs.set_title(i, video_ids[i][0])\n",
    "# display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_replay_buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 2: Experience Again\n",
    "\n",
    "Replay buffer is mechanism which allow to remember certain experience with environment which happened previously so that we can replay it in mind during the training in the new mode and thus trying to preserve the structure of both modes in the agent. Each of the gradient steps in the first mode is going to be \"experience\" we are going to save and which we will play artificially (train) during training in second mode. In the proposed auxiliary storage it is enough to store only final loss value as it preserves information about computational graph (architecture of the agent) will allow the agent to implement the replay.\n",
    "\n",
    "The procedure for the retrieval of the past experience is the following: for each gradient step in the new mode there is going to be one gradient step on remembered experience from the previous mode.\n",
    "\n",
    "In this exercise you need to complete `ReplayBuffer` class which will remember information about training experience. Observe that `train_agent` is redefined and slightly modified so it accepts `ReplayBuffer` instance as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_experience = 250, num_trials = 100):\n",
    "        \"\"\"Initialize replay buffer.\n",
    "        Notice that when replay buffer is full of experience and new one should be remembered, it replaces existing ones, starting\n",
    "        from the oldest.\n",
    "\n",
    "        Inputs:\n",
    "        - max_experience (int, default = 250): the maximum number of experience (gradient steps) which can be stored.\n",
    "        - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.\n",
    "        \"\"\"\n",
    "        self.max_experience = max_experience\n",
    "\n",
    "        #variable which fully describe experience\n",
    "        self.losses = [0 for _ in range(self.max_experience)]\n",
    "\n",
    "        #number of memory cell to point to (write or overwrite experience)\n",
    "        self.writing_pointer = 0\n",
    "        self.reading_pointer = 0\n",
    "\n",
    "        #to keep track how many experience there were\n",
    "        self.num_experience = 0\n",
    "\n",
    "    def write_experience(self, loss):\n",
    "        \"\"\"Write new experience.\"\"\"\n",
    "        ###################################################################\n",
    "        ## Fill out the following then remove\n",
    "        raise NotImplementedError(\"Student exercise: complete retrieval and storing procedure for replay buffer.\")\n",
    "        ###################################################################\n",
    "        self.losses[...] = ...\n",
    "\n",
    "        #so that pointer is in range of max_experience and will point to the older experience while full\n",
    "        self.writing_pointer = (self.writing_pointer + 1) % self.max_experience\n",
    "        self.num_experience += 1\n",
    "\n",
    "    def read_experience(self):\n",
    "        \"\"\"Read existing experience.\"\"\"\n",
    "        loss = self.losses[...]\n",
    "\n",
    "        #so that pointer is in range of self.max_experience and will point to the older experience while full\n",
    "        self.reading_pointer = (self.reading_pointer + 1) % min(self.max_experience, self.num_experience)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_experience = 250, num_trials = 100):\n",
    "        \"\"\"Initialize replay buffer.\n",
    "        Notice that when replay buffer is full of experience and new one should be remembered, it replaces existing ones, starting\n",
    "        from the oldest.\n",
    "\n",
    "        Inputs:\n",
    "        - max_experience (int, default = 250): the maximum number of experience (gradient steps) which can be stored.\n",
    "        - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.\n",
    "        \"\"\"\n",
    "        self.max_experience = max_experience\n",
    "\n",
    "        #variable which fully describe experience\n",
    "        self.losses = [0 for _ in range(self.max_experience)]\n",
    "\n",
    "        #number of memory cell to point to (write or overwrite experience)\n",
    "        self.writing_pointer = 0\n",
    "        self.reading_pointer = 0\n",
    "\n",
    "        #to keep track how many experience there were\n",
    "        self.num_experience = 0\n",
    "\n",
    "    def write_experience(self, loss):\n",
    "        \"\"\"Write new experience.\"\"\"\n",
    "        self.losses[self.writing_pointer] = loss\n",
    "\n",
    "        #so that pointer is in range of max_experience and will point to the older experience while full\n",
    "        self.writing_pointer = (self.writing_pointer + 1) % self.max_experience\n",
    "        self.num_experience += 1\n",
    "\n",
    "    def read_experience(self):\n",
    "        \"\"\"Read existing experience.\"\"\"\n",
    "        loss = self.losses[self.reading_pointer]\n",
    "\n",
    "        #so that pointer is in range of self.max_experience and will point to the older experience while full\n",
    "        self.reading_pointer = (self.reading_pointer + 1) % min(self.max_experience, self.num_experience)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def train_agent_with_replay(env, agent, optimizer_func, replay, mode = 1, training_mode = \"write\", num_gradient_steps = 1000, num_trials = 100):\n",
    "    \"\"\"Training for agent in changing colorful environment.\n",
    "    Observe that training happens for one particular mode.\n",
    "\n",
    "    Inputs:\n",
    "    - env (ChangingEnv): environment.\n",
    "    - agent (ActorCritic): particular instance of Actor Critic agent to train.\n",
    "    - optimizer_func (torch.Optim): optimizer to use for training.\n",
    "    - replay (ReplayBuffer): replay buffer which is used during training.\n",
    "    - mode (int, default = 1): mode of the environment.\n",
    "    - training_mode (str, default = \"write\"): training mode with replay buffer (\"write\", \"read\").\n",
    "    - num_gradient_steps (int, default = 1000): number of gradient steps to perform.\n",
    "    - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.\n",
    "    \"\"\"\n",
    "    #reset environment\n",
    "    state = env.reset(mode = mode)\n",
    "\n",
    "    #define optimizer\n",
    "    optimizer = optimizer_func(agent.parameters(), agent.learning_rate, eps = 1e-5)\n",
    "\n",
    "    for index in range(num_gradient_steps):\n",
    "\n",
    "      #for storing variables for training\n",
    "      log_probs = []\n",
    "      values = []\n",
    "      rewards = []\n",
    "      entropy_term = torch.tensor(0.)\n",
    "\n",
    "      #start conditions\n",
    "      preceding_reward = torch.Tensor([0])\n",
    "      preceding_action = torch.Tensor([0, 0])\n",
    "\n",
    "      for trial in range(num_trials):\n",
    "          #state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!\n",
    "          full_state = torch.cat((torch.from_numpy(state.flatten() / 255).float(), preceding_reward, preceding_action), dim = 0)\n",
    "          value, policy_logits = agent(full_state)\n",
    "          value = value.squeeze(0)\n",
    "\n",
    "          #sample action from policy\n",
    "          dist = torch.distributions.Categorical(logits=policy_logits.squeeze(0))\n",
    "          action = dist.sample()\n",
    "\n",
    "          #perform action to get reward and new state\n",
    "          new_state, reward, _ = env.step(action)\n",
    "\n",
    "          #we normalize reward too\n",
    "          reward /= 4\n",
    "\n",
    "          #update preceding variables\n",
    "          preceding_reward = torch.Tensor([reward])\n",
    "          preceding_action = F.one_hot(action, num_classes=2).float()\n",
    "          state = new_state\n",
    "\n",
    "          #for training\n",
    "          log_prob = dist.log_prob(action)\n",
    "          entropy = dist.entropy()\n",
    "          rewards.append(reward)\n",
    "          values.append(value)\n",
    "          log_probs.append(log_prob)\n",
    "          entropy_term += entropy\n",
    "\n",
    "      #calculataing loss\n",
    "      Qval = 0\n",
    "      Qvals = torch.zeros(len(rewards))\n",
    "      for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + agent.discount_factor * Qval\n",
    "        Qvals[t] = Qval\n",
    "      values = torch.stack(values)\n",
    "      log_probs = torch.stack(log_probs)\n",
    "      advantage = Qvals - values\n",
    "      actor_loss = (-log_probs * advantage.detach()).mean()\n",
    "      critic_loss = advantage.pow(2).mean()\n",
    "      entropy_term = entropy_term / num_trials\n",
    "\n",
    "      #loss incorporates actor/critic terms + entropy\n",
    "      loss = actor_loss + agent.state_value_estimate_cost * critic_loss - agent.entropy_cost * entropy_term\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward(retain_graph = True)\n",
    "      optimizer.step()\n",
    "\n",
    "      # write this training example into memory\n",
    "      if training_mode == \"write\":\n",
    "          replay.write_experience(loss)\n",
    "\n",
    "      #retrieve previous experience\n",
    "      if training_mode == \"read\":\n",
    "          replay_loss = replay.read_experience()\n",
    "          optimizer.zero_grad()\n",
    "          replay_loss.backward(retain_graph = True)\n",
    "          optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "At first, we are going to train the newbie agent in the first mode using writing mode of replay buffer. Then, during the training in the second mode, we will incorporate reading from this replay buffer and observe whether it impacts agent's performance.\n",
    "\n",
    "The training time will take around 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#define environment\n",
    "env = ChangingEnv()\n",
    "replay = ReplayBuffer()\n",
    "\n",
    "#define agent and optimizer\n",
    "agent = ActorCritic(hidden_size = 100)\n",
    "optimizer_func = optim.RMSprop\n",
    "\n",
    "#train agent\n",
    "train_agent_with_replay(env, agent, optimizer_func, replay)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great! We've trained agent in the first mode and saved the experience in replay buffer. Now, let us change the mode to \"read\" and train the agent in the second mode with replaying saved experience each gradient step of the new one. The observed plot is the confusion matrix for the second mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "train_agent_with_replay(env, agent, optimizer_func, replay, mode = 2, training_mode = \"read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, mode = 2, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards, mode = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to observe the plot!\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "rewards, max_rewards = evaluate_agent(env, agent, num_evaluation_trials = 5000)\n",
    "plot_confusion_matrix(rewards, max_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Perfect match! Our agent has learnt both of the environment's modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "# content_review(f\"{feedback_prefix}_experience_again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "Have a summary of what they learned with specific points.\n",
    "\n",
    "1. Specific point A\n",
    "\n",
    "2. Specific point B"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D4_Tutorial5",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
