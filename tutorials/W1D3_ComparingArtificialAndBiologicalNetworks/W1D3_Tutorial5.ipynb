{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3d2b26-a059-4683-8bfd-2499a50eb346",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial5.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5410f-0745-4a99-a40f-cd591c3b4d45",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 5: Dynamical Similarity Analysis (DSA)\n",
    "\n",
    "**Week 1, Day 3: Comparing Artificial And Biological Networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Mitchell Ostrow, Alex Murphy\n",
    "\n",
    "__Content reviewers:__ Xaq Pitkow, Hlib Solodzhuk, Alex Murphy\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk, Patrick Mineault, Alex Murphy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f673e-3657-47aa-aeb0-08c55904bd6d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D5_DSA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03579bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper & Plotting Functions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_2d_random_process(A, B, T=1000):\n",
    "    \"\"\"\n",
    "    Generates a 2D random process with the equation x(t+1) = A.x(t) + B.noise.\n",
    "\n",
    "    Args:\n",
    "        A: 2x2 transition matrix.\n",
    "        B: 2x2 noise scaling matrix.\n",
    "        T: Number of time steps.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (T+1, 2) representing the trajectory.\n",
    "    \"\"\"\n",
    "    # Assuming equilibrium distribution is zero mean and identity covariance for simplicity.\n",
    "    # You may adjust this according to your actual equilibrium distribution\n",
    "    x = np.zeros(2)\n",
    "\n",
    "    trajectory = [x.copy()] # Initialize with x(0)\n",
    "    for t in range(T):\n",
    "      noise = np.random.normal(size=2)  # Standard normal noise\n",
    "      x = np.dot(A, x) + np.dot(B, noise)\n",
    "      trajectory.append(x.copy())\n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af6b0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Intro\n",
    "\n",
    "Welcome to Tutorial 5 of Day 3 (W1D3) of the NeuroAI course. In this tutorial we are going to look at an exciting method that measures similarity from a slightly different perspective, a temporal one. The prior methods we have looked at were centeed around geometry and spatial representations, where we looked at metrics such as the Euclidean and Mahalanobis distance metrics. However, one thing we often want to study in neuroscience and in AI separately - is the temporal domain. Even more so in our own field of NeuroAI, we often deal with time series of neuronal / biological recordings. One thing you should already have a broad level of awareness of is that end structures can end up looking the same even though the paths taken to arrive at those end structures were very different.\n",
    "\n",
    "In NeuroAI, we're often confronted with systems that seem to have some sort of overlap and we want to study whether this implies there is a shared computation pairs up with the shared task (we looked at this in detail yesterday in our *Comparing Tasks* day). Today, we will begin by watching a short intro video by Mitchell Ostrow, who will describe his method to compare representations over temporal sequences (the method is called Dynamic Similarity Analysis). Then we are going to introduce three simple dynamical systems and we will explore them from the perspective of Dynamic Similarity Analysis and also include a control analysis using Representational Similarity Analysis to highlight the point that similarity and comparisons can be derived from different perspectives. You will have a short coding exercise on this topic which Alex Murphy will introduce in a short video.\n",
    "\n",
    "At the end of the tutorial, we will finally look at a further aspect of temporal sequences using RNNs. This is an adaptation of the ideas introduced in Tutorial 2 but now based around recurrent representations from RNNs. We hope you enjoy this tutorial today and that it gets you thinking not just what similarity values mean, but which ones are appropriate (here, from a spatial or temporal perspective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a92e7-e76c-48de-b574-15a1272717cf",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Load Slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"8fx23\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6178f-ddf5-41ae-b676-15e452dc8b78",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Dynamical Similarity Analysis\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'FHikIsQFQvM'), ('Bilibili', 'BV1qm421g7hV')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239760a",
   "metadata": {},
   "source": [
    "## Section 1: Visualization of Three Temporal Sequences\n",
    "\n",
    "We are going to be working with the analysis of three temporal sequences today:\n",
    "\n",
    "* The circular time series (`Circle`)\n",
    "* The oval time series (`Oval`)\n",
    "* The random walk (`R-Walk`)\n",
    "\n",
    "The random walk is going to be broadly *oval shaped*. Now, what do you think, from a geometric perspective, might result from a spatial analysis of these three different *representations*? You will probably assume because the random walk has an oval shape and there is also an oval time series (that's not a random walk) that these would result in a higher spatial similarity. You'd be right to assume this. However, what we're going to do with the `Circle` and `Oval` time series is to include an oscillator at a specific frequency, shared amongst these two time series. In effect, this means that although when plotted in totality the shapes are different, during the dynamic (temporal) evolution of these time series, a very similar shared pattern is emerging. We want methods that are sensitive to these changes to give higher scores for time series sharing similar temporal patterns (e.g. both containing oscillating patterns at similar frequences) rather than just a random walk that resembles (geometrically) one of the other shapes (`R-Walk`). Before we continue, we'll just define this random walk in a little more detail. A random walk at a specific location / timepoint takes a random step of fixed length in a specific direction, but this can be broadly controlled to resemble geometric shapes. We've taken a random walk and then reframed it to be similar in shape to `Oval`. \n",
    "\n",
    "Let's now visualize these three temporal sequences, to make the previous paragraph a little clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b93041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle\n",
    "r = .1; # rotation\n",
    "A = np.array([[np.cos(r), np.sin(r)], [-np.sin(r), np.cos(r)]])\n",
    "B = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "trajectory = generate_2d_random_process(A, B)\n",
    "trajectory_circle = trajectory\n",
    "\n",
    "# Oval\n",
    "r = .1; # rotation\n",
    "s = 4;  # scaling\n",
    "S = np.array([[1, 0], [0, s]])\n",
    "Si = np.array([[1, 0], [0, 1/s]])\n",
    "V = np.array([[1, 1], [-1, 1]])/np.sqrt(2)\n",
    "Vi = np.array([[1, -1], [1, 1]])/np.sqrt(2)\n",
    "R = np.array([[np.cos(r), np.sin(r)], [-np.sin(r), np.cos(r)]])\n",
    "A = np.linalg.multi_dot([V,Si,R,S,Vi])\n",
    "B = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "trajectory = generate_2d_random_process(A, B)\n",
    "trajectory_oval = trajectory\n",
    "\n",
    "# R-Walk (random walk)\n",
    "r = .1; # rotation\n",
    "A = np.array([[.9, 0], [0, .9]])\n",
    "c = -.95; # correlation coefficient\n",
    "B = np.array([[1, c], [0, np.sqrt(1-c*c)]])\n",
    "\n",
    "trajectory = generate_2d_random_process(A, B)\n",
    "trajectory_walk = trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400136e0",
   "metadata": {},
   "source": [
    "Can you see how the spatial / geometric similarity of `R-Walk` and `Oval` are more similar, but the oscillations during the temporal sequence are shared between `Circle` and `Oval`? Let's run Dynamic Similarity Analysis on these temporal sequences and see what scores are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484740c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71164d2d",
   "metadata": {},
   "source": [
    "---\n",
    "# (Bonus) Representational Geometry of Recurrent Models\n",
    "\n",
    "Transformations of representations can occur across space and time, e.g., layers of a neural network and steps of recurrent computation. We've looked at the temporal dimension today and earlier today in the other tutorials we looked mainly at spatial representations.\n",
    "\n",
    "Just as the layers in a feedforward DNN can change the representational geometry to perform a task, steps in a recurrent network can reuse the same layer to reach the same computational depth.\n",
    "\n",
    "In this section, we look at a very simple recurrent network with only 2650 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53063596",
   "metadata": {},
   "source": [
    "Here is a diagram of this network:\n",
    "\n",
    "![Recurrent convolutional neural network](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/static/rcnn_tutorial.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "# TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Grab a recurrent model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"recurrent_model.pth\"\n",
    "model_recurrent = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a108f2",
   "metadata": {},
   "source": [
    "<br>We can first look at the computational steps in this network. As we see below, the `conv2` operation is repeated for 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, _ = get_graph_node_names(model_recurrent)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7b1b3",
   "metadata": {},
   "source": [
    "Plotting the RDMs after each application of the `conv2` operation shows the same progressive emergence of the blockwise structure around the diagonal, mediating the correct classification in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65047a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a55054",
   "metadata": {},
   "source": [
    "We can also look at how the different dimensionality reduction techniques capture the dynamics of changing geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf787f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8af84",
   "metadata": {},
   "source": [
    "## Representational geometry paths for recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5cb76",
   "metadata": {},
   "source": [
    "We can look at the model's recurrent computational steps as a path in the representational geometry space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eab11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "#rdms, rdms_dict = calc_rdms(model_features)\n",
    "features = {'recurrent model': model_features_recurrent}\n",
    "model_colors = {'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c7151",
   "metadata": {},
   "source": [
    "We can also look at the paths taken by the feedforward and the recurrent models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4569de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'recurrent model': model_features_recurrent}\n",
    "model_colors = {'feedforward model': 'b', 'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4968b",
   "metadata": {},
   "source": [
    "What we can see here is that different models can take very different paths in the space of representational geometries to map images to labels. This is because there exist many different functional mappings to do a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_recurrent_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7067a",
   "metadata": {},
   "source": [
    "We are now going to introduce an exciting new method to compare dynamical systems, Dynamic Similarity Analysis! Please watch the video below and follow through the example code in the following subsection to reproduce some of the figures you will see in the explanatory video. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce83bc-7e86-44d3-a40a-4ad46fd5a6df",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_DSA_video\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D3_Tutorial5",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
