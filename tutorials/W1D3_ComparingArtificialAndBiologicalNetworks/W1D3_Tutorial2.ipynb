{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Computation as transformation of representational geometries\n",
    "\n",
    "**Week 1, Day 3: Comparing Artificial And Biological Networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Hossein Adeli Jelodar\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "In this tutorial we aim to understand that in networks computation can be understood as a sequence of transformations of the representation of the input, where each transformation can remove information, preserve other information, and change the format in which the information is represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"qmv5r\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "!pip install -q torch torchvision matplotlib numpy scikit-learn rsatoolbox scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import rsatoolbox\n",
    "from rsatoolbox.data import Dataset\n",
    "from rsatoolbox.rdm.calc import calc_rdm\n",
    "from scipy import stats\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"\n",
    "    Samples a specified number of images from a data loader.\n",
    "\n",
    "    Args:\n",
    "    - data_loader (torch.utils.data.DataLoader): Data loader containing images and labels.\n",
    "    - n (int): Number of images to sample per class.\n",
    "    - plot (bool): Whether to plot the sampled images using matplotlib.\n",
    "\n",
    "    Returns:\n",
    "    - imgs (torch.Tensor): Sampled images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the sampled images.\n",
    "    \"\"\"\n",
    "\n",
    "    imgs, targets = next(iter(data_loader))\n",
    "\n",
    "    imgs_o = []\n",
    "    labels = []\n",
    "    for value in range(10):\n",
    "        cat_imgs = imgs[np.where(targets == value)][0:n]\n",
    "        imgs_o.append(cat_imgs)\n",
    "        labels.append([value]*len(cat_imgs))\n",
    "\n",
    "    imgs = torch.cat(imgs_o, dim=0)\n",
    "    labels = torch.tensor(labels).flatten()\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(torch.moveaxis(make_grid(imgs, nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "        plt.axis('off')\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "def plot_rdms(model_rdms):\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig = plt.figure(figsize=(15, 4))\n",
    "        gs = fig.add_gridspec(1, len(model_rdms))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    \n",
    "        for l in range(len(model_rdms)):\n",
    "    \n",
    "            layer = list(model_rdms.keys())[l]\n",
    "            rdm = np.squeeze(model_rdms[layer])\n",
    "    \n",
    "            if len(rdm.shape) < 2:\n",
    "                rdm = rdm.reshape( (int(np.sqrt(rdm.shape[0])), int(np.sqrt(rdm.shape[0]))) )\n",
    "    \n",
    "            rdm = rdm / np.max(rdm)\n",
    "    \n",
    "            ax = plt.subplot(gs[0,l])\n",
    "            ax_ = ax.imshow(rdm, cmap='Greys')\n",
    "            ax.set_title(f'{layer}')\n",
    "    \n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "        cbar_ax.text(-1.6, 0.05, 'Normalized euclidean distance', size=12, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax)\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "def rep_path(model_features, model_colors, labels=None, rdm_calc_method='euclidean', rdm_comp_method='cosine'):\n",
    "    \"\"\"\n",
    "    Represents paths of model features in a reduced-dimensional space.\n",
    "\n",
    "    Args:\n",
    "    - model_features (dict): Dictionary containing model features for each model.\n",
    "    - model_colors (dict): Dictionary mapping model names to colors for visualization.\n",
    "    - labels (array-like, optional): Array of labels corresponding to the model features.\n",
    "    - rdm_calc_method (str, optional): Method for calculating RDMS ('euclidean' or 'correlation').\n",
    "    - rdm_comp_method (str, optional): Method for comparing RDMS ('cosine' or 'corr').\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot).\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        path_len = []\n",
    "        path_colors = []\n",
    "        rdms_list = []\n",
    "        ax_ticks = []\n",
    "        tick_colors = []\n",
    "        model_names = list(model_features.keys())\n",
    "        for m in range(len(model_names)):\n",
    "            model_name = model_names[m]\n",
    "            features = model_features[model_name]\n",
    "            path_colors.append(model_colors[model_name])\n",
    "            path_len.append(len(features))\n",
    "            ax_ticks.append(list(features.keys()))\n",
    "            tick_colors.append([model_colors[model_name]]*len(features))\n",
    "            rdms, _ = calc_rdms(features, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "    \n",
    "        path_len = np.insert(np.cumsum(path_len),0,0)\n",
    "    \n",
    "        if labels is not None:\n",
    "            rdms, _ = calc_rdms({'labels' : F.one_hot(labels).float().to(device)}, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "            ax_ticks.append(['labels'])\n",
    "            tick_colors.append(['m'])\n",
    "            idx_labels = -1\n",
    "    \n",
    "        rdms = rsatoolbox.rdm.concat(rdms_list)\n",
    "    \n",
    "        #Flatten the list\n",
    "        ax_ticks = [l for model_layers in ax_ticks for l in model_layers]\n",
    "        tick_colors = [l for model_layers in tick_colors for l in model_layers]\n",
    "        tick_colors = ['k' if tick == 'input' else color for tick, color in zip(ax_ticks, tick_colors)]\n",
    "    \n",
    "        rdms_comp = rsatoolbox.rdm.compare(rdms, rdms, method=rdm_comp_method)\n",
    "        if rdm_comp_method == 'cosine':\n",
    "            rdms_comp = np.arccos(rdms_comp)\n",
    "        rdms_comp = np.nan_to_num(rdms_comp, nan=0.0)\n",
    "    \n",
    "        # reduce dim to 2\n",
    "        transformer = manifold.MDS(n_components = 2, max_iter=1000, n_init=10, normalized_stress='auto')\n",
    "        dims= transformer.fit_transform(rdms_comp)\n",
    "    \n",
    "        # remove duplicates of the input layer from multiple models\n",
    "        remove_duplicates = np.where(np.array(ax_ticks) == 'input')[0][1:]\n",
    "        for index in remove_duplicates:\n",
    "            del ax_ticks[index]\n",
    "            del tick_colors[index]\n",
    "            rdms_comp = np.delete(np.delete(rdms_comp, index, axis=0), index, axis=1)\n",
    "    \n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    \n",
    "        ax = plt.subplot(gs[0,0])\n",
    "        ax_ = ax.imshow(rdms_comp, cmap='viridis_r')\n",
    "        fig.subplots_adjust(left=0.2)\n",
    "        cbar_ax = fig.add_axes([-0.01, 0.2, 0.01, 0.5])\n",
    "        #cbar_ax.text(-7, 0.05, 'angles between rdms', size=10, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax,location='left')\n",
    "        ax.set_title('Angles between layer rdms')\n",
    "        ax.set_xticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7, rotation=83)\n",
    "        ax.set_yticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7)\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.xaxis.get_ticklabels())]\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.yaxis.get_ticklabels())]\n",
    "    \n",
    "        ax = plt.subplot(gs[0,1])\n",
    "        amin, amax = dims.min(), dims.max()\n",
    "        amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "    \n",
    "        for i in range(len(rdms_list)-1):\n",
    "    \n",
    "            path_indices = np.arange(path_len[i], path_len[i+1])\n",
    "            ax.plot(dims[path_indices, 0], dims[path_indices, 1], color=path_colors[i], marker='.')\n",
    "            ax.set_title('Representational geometry path')\n",
    "            ax.set_xlim([amin, amax])\n",
    "            ax.set_ylim([amin, amax])\n",
    "            ax.set_xlabel(f\"dim 1\")\n",
    "            ax.set_ylabel(f\"dim 2\")\n",
    "    \n",
    "        # if idx_input is not None:\n",
    "        idx_input = 0\n",
    "        ax.plot(dims[idx_input, 0], dims[idx_input, 1], color='k', marker='s')\n",
    "    \n",
    "        if labels is not None:\n",
    "            ax.plot(dims[idx_labels, 0], dims[idx_labels, 1], color='m', marker='*')\n",
    "    \n",
    "        ax.legend(model_names, fontsize=8)\n",
    "        fig.tight_layout()\n",
    "\n",
    "def plot_dim_reduction(model_features, labels, transformer_funcs):\n",
    "    \"\"\"\n",
    "    Plots the dimensionality reduction results for model features using various transformers.\n",
    "\n",
    "    Args:\n",
    "    - model_features (dict): Dictionary containing model features for each layer.\n",
    "    - labels (array-like): Array of labels corresponding to the model features.\n",
    "    - transformer_funcs (list): List of dimensionality reduction techniques to apply ('PCA', 'MDS', 't-SNE').\n",
    "\n",
    "    Returns:\n",
    "    None (displays the plot).\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        transformers = []\n",
    "        for t in transformer_funcs:\n",
    "            if t == 'PCA': transformers.append(PCA(n_components=2))\n",
    "            if t == 'MDS': transformers.append(manifold.MDS(n_components = 2, normalized_stress='auto'))\n",
    "            if t == 't-SNE': transformers.append(manifold.TSNE(n_components = 2, perplexity=40, verbose=0))\n",
    "    \n",
    "        fig = plt.figure(figsize=(15, 4*len(transformers)))\n",
    "        # and we add one plot per reference point\n",
    "        gs = fig.add_gridspec(len(transformers), len(model_features))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    \n",
    "        return_layers = list(model_features.keys())\n",
    "    \n",
    "        for f in range(len(transformer_funcs)):\n",
    "    \n",
    "            for l in range(len(return_layers)):\n",
    "                layer =  return_layers[l]\n",
    "                feats = model_features[layer].detach().cpu().flatten(1)\n",
    "                feats_transformed= transformers[f].fit_transform(feats)\n",
    "    \n",
    "                amin, amax = feats_transformed.min(), feats_transformed.max()\n",
    "                amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "                #TODO : copy over\n",
    "                ax = plt.subplot(gs[f,l])\n",
    "                ax.set_xlim([amin, amax])\n",
    "                ax.set_ylim([amin, amax])\n",
    "                ax.axis(\"off\")\n",
    "                #ax.set_title(f'{layer}')\n",
    "                if f == 0: ax.text(0.5, 1.12, f'{layer}', size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                if l == 0: ax.text(-0.3, 0.5, transformer_funcs[f], size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                # Create a discrete color map based on unique labels\n",
    "                num_colors = len(np.unique(labels))\n",
    "                cmap = plt.get_cmap('viridis_r', num_colors) # 10 discrete colors\n",
    "                norm = mpl.colors.BoundaryNorm(np.arange(-0.5,num_colors), cmap.N)\n",
    "                ax_ = ax.scatter(feats_transformed[:, 0], feats_transformed[:, 1], c=labels, cmap=cmap, norm=norm)\n",
    "    \n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.6])\n",
    "        fig.colorbar(ax_, cax=cbar_ax, ticks=np.linspace(0,9,10))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "# modified from https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = torch.log(output)  # to make it a log_softmax\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, return_features=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) # to make it a log_softmax\n",
    "            output = torch.log(output)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def build_args():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1200, metavar='N',\n",
    "                        help='input batch size for testing (default: 1200)')\n",
    "    parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    return args\n",
    "\n",
    "def fetch_dataloaders(args):\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == 'cuda':\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model(args, model, optimizer):\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "def extract_features(model, imgs, return_layers):\n",
    "    \"\"\"\n",
    "    Extracts features from a given model and input images.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model from which to extract features.\n",
    "    - imgs (torch.Tensor): Input images for feature extraction.\n",
    "    - return_layers (str or list): Indicates which layers to return features from.\n",
    "        - 'all': Returns features from all layers in the model.\n",
    "        - 'layers': Returns features from input, convolutional, and fully connected layers.\n",
    "        - list of layers: Returns features from the listed layers\n",
    "\n",
    "    Returns:\n",
    "    - model_features (dict): Dictionary containing model features for each layer.\n",
    "    \"\"\"\n",
    "    if return_layers == 'all':\n",
    "        return_layers, _ = get_graph_node_names(model)\n",
    "    elif return_layers == 'layers':\n",
    "        layers, _ = get_graph_node_names(model)\n",
    "        return_layers = [l for l in layers if 'input' in l or 'conv' in l or 'fc' in l]\n",
    "\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_layers)\n",
    "    model_features = feature_extractor(imgs)\n",
    "\n",
    "    return model_features\n",
    "\n",
    "def calc_rdms(model_features, method='euclidean'):\n",
    "    \"\"\"\n",
    "    Calculates Representational Dissimilarity Matrices (RDMs) from model features.\n",
    "\n",
    "    Args:\n",
    "    - model_features (dict): Dictionary containing model features for each layer.\n",
    "    - method (str): Method used to calculate the RDMs ('euclidean' or 'correlation').\n",
    "\n",
    "    Returns:\n",
    "    - rdms (numpy.ndarray): Representational Dissimilarity Matrices Objects.\n",
    "    - rdms_dict (dict): Dictionary containing RDMs for each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        if device == 'cuda':\n",
    "            feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape)>2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "\n",
    "        ds = Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = calc_rdm(ds_list, method=method)\n",
    "\n",
    "    rdms_dict = {list(model_features.keys())[i]: rdms.get_matrices()[i] for i in range(len(model_features))}\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "# modified from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method (FGSM) attack.\n",
    "\n",
    "    Args:\n",
    "    image (torch.Tensor): The input image.\n",
    "    epsilon (float): Perturbation magnitude for FGSM.\n",
    "    data_grad (torch.Tensor): Gradient of the loss w.r.t. the input image.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Perturbed image.\n",
    "    \"\"\"\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "# restores the tensors to their original scale\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(batch.device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(batch.device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "def generate_adversarial(model, imgs, labels, epsilon):\n",
    "    \"\"\"\n",
    "    Generate adversarial images using FGSM attack.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The neural network model.\n",
    "    imgs (torch.Tensor): The input images.\n",
    "    labels (torch.Tensor): The true labels of the input images.\n",
    "    epsilon (float): Perturbation magnitude for FGSM.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Adversarial images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_imgs = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for img, target in zip(imgs, labels):\n",
    "\n",
    "        img = img.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        img.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(img)\n",
    "        output = torch.log(output)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect ``datagrad``\n",
    "        data_grad = img.grad.data\n",
    "\n",
    "        # Restore the data to its original scale\n",
    "        data_denorm = denorm(img)\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "        # Reapply normalization\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        adv_imgs.append(perturbed_data_normalized.detach())\n",
    "\n",
    "    return torch.cat(adv_imgs)\n",
    "\n",
    "# modified from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "def test_adversarial(model, imgs, labels, plot=False):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    output = model(imgs)\n",
    "    output = torch.log(output)\n",
    "\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    final_acc = correct/float(len(imgs))\n",
    "    print(f\"adversarial test accuracy = {correct} / {len(imgs)} = {final_acc}\")\n",
    "\n",
    "    if plot:\n",
    "        cnt = 0\n",
    "        plt.figure(figsize=(8,10))\n",
    "        for i in range(10):\n",
    "            for j in range(len(labels)//10):\n",
    "\n",
    "                plt.subplot(10,5,cnt+1)\n",
    "                plt.xticks([], [])\n",
    "                plt.yticks([], [])\n",
    "                orig,adv,ex = labels[cnt].cpu().numpy(), pred[cnt].cpu().numpy(), imgs[cnt].moveaxis(0,-1).cpu()\n",
    "                if orig == adv:\n",
    "                    plt.title(f\"{orig} -> {adv}\")\n",
    "                else:\n",
    "                    plt.title(f\"{orig} -> {adv}\", fontweight=\"bold\")\n",
    "\n",
    "                plt.imshow(ex, cmap=\"gray\")\n",
    "                cnt += 1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def train_one_epoch_adversarial(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # train the model on adversarial images\n",
    "        epsilon = 0.2\n",
    "        data = generate_adversarial(model, data, target, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = torch.log(output)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def train_model_adversarial(args, model, optimizer):\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch_adversarial(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"robust_mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"standard_model.pth\", \"adversarial_model.pth\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/s5rt6/download\", \"https://osf.io/qv5eb/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"2e63c2cd77bc9f1fa67673d956ec910d\", \"25fb34497377921b54368317f68a7aa7\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU)\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Computation & representational geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We are going to use the same models, standard and adversarially trained, as in the previous tutorial, thus they are already prepare to get the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Grab a standard model\n",
    "\n",
    "args = build_args()\n",
    "path = \"standard_model.pth\"\n",
    "model = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In order to better visualize the sturcture in the model features, we grab 5 test images from each of the 10 digit categories. Below is a simple function to perform that. For the purpose of the visualization, we order the test images by category identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Just sample 5 per category to show the order\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=5, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First let's look at the computational steps in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Computational steps\n",
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can then focus on the main steps for visualization. We extract the features from these specific layers of the model for the sampled test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Features for different layers\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "print('features gathered for:', list(model_features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Calculating and plotting the RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now for each of the test images, we have the model features from each layer of the trained network. We can look at how the representational (dis)similarity between these samples changes across the layers of the network. For each layer, we flatten the feautres so that each test image is represented by a vector (i.e. a point in space) and then we use the rsatooblox to compute the euclidean distances between the points in that reprsentational space.\n",
    "\n",
    "The result is a matrix of distances between each two samples for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now plot the rdms for the input pixels and the two conv and two fully connected layers below.\n",
    "\n",
    "The dimension of these RDMs are `#stimuli` by `#stimuli`.\n",
    "The brighter cells show smaller dissimlarity between the two stimuli and the diagonal would have the dissimiarity of 0 (comparing the representation of one image to itself).\n",
    "\n",
    "Since the instances of each category are next to each other, the brighter blocks emerging around the diagonal show that the representatiaons for category instances become similar to one another across the levels of the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Putting it all together\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20) #grab 20 samples from each category\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Computation in the network can be understood as the transformation of representational geometries because the information-content of the representation at each stage are captured by its representational geometry.\n",
    "\n",
    "As we see in above rdms the geomety captures how the representation in later layers become more similar for instances of the same category since this model was trained for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Another way to examine the geometry of the representaion changing across layers is by reducing the dimensionality and displaying the samples in 2D space.\n",
    "\n",
    "The representaionl spaces of each layer of a deep network project each stimulus to a point in very high dimentional spaces. Methods such as PCA (principal component analysis), MDS (Multidimensional_scaling), and t-SNE (t-distributed stochastic neighbor embedding) attemp to capture the same geometry in lower dimensions.\n",
    "\n",
    "Below we look at the representational geometry of the same network layers using these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For this visulization we take 500 samples from the test set and we color code them based on their category. The figure below shows the scatter plots for these samples across the layers of the network. Each panel is the 2D projection of the feature representations using a specific method.\n",
    "\n",
    "The methods all show that image representations progressively cluster based on category across the layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Sequential image representation clustering\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 50 samples from the each category\n",
    "\n",
    "# Alternatively get a whole test batch\n",
    "#imgs, labels = next(iter(test_loader))\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational path\n",
    "\n",
    "We can now go a step further and think of the computational steps in a network as steps in a path in the space of representational geometries.\n",
    "\n",
    "This means that each stage in the network is changing the geometry to make it more and more like the desired geometry which would be the labels for a network trained for classification.\n",
    "\n",
    "To examine this process, we first have to quantify what the geometry after each step looks like. For this we can calculate the rdms based on the euclidean distances between the representations of images for each layer.\n",
    "\n",
    "The next step is to quantify how different the geometries from each step of the network are from one another. To perform this operation we first flatten the rdms from different model steps to make them a vecotr and then we can look at the angles between those vectors. Note that this measure of distance is between the representational geometries of different layers and not the representations themselves.\n",
    "\n",
    "In this tutorial we compute the similarity between geometries with a metric measure like the arccos of cos distance between the layers of the network. The resulting matrix has the dimentions of #model_computational_steps by #model_computational_steps.\n",
    "\n",
    "The last step to visualize the path is to then embed the distances between the geometries in a lower dimensional space. We use MDS to reduce the dimensions to two in order to show each computational step as a point in a 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The function below plots the computation that takes place across the layers of the network as a path in the space of representational gemoetries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Representational Path\n",
    "\n",
    "imgs, labels = next(iter(test_loader)) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The left panel below shows the angles between the rdms that catpure the representational geometry for each layer.\n",
    "\n",
    "The brigher colors indicate higher similarity. The diagonal is the angle between a layer geometry with itself which is zero.  \n",
    "\n",
    "The figure on the right shows the embedding of these angles for each layer in a 2D space using MDS. Connecting the steps starting from the input (shown as the black square) forms a path in the space of representional geometry. The last step in this path (the softmax layer) reaches close to the embedding of the lables in the same 2D space.\n",
    "\n",
    "Thus the computation performed by this model is a path from the geometry of the input pixels to the geometry of the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. How do you think the representational path will be different if we take a smaller sample of images to create the RDMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To test what will actually happen by taking a sample of 5 images per category and visualizing the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Test your idea by executing this cell!\n",
    "\n",
    "#imgs, labels = next(iter(test_loader)) #grab 500 samples from the test set\n",
    "imgs, labels = sample_images(test_loader, n=5)\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Adversarial images\n",
    "\n",
    "Adversarial images pose a generalization challange to DNNs. Here we examine how this challenge and lower perofrmance are reflected in the the representational geometry path. First we use FGSM (Fast Gradient Sign method) to generate adversarial images for the trained feedforward model. This is a \"white-boxed\" attack that uses the gradients from the model loss with respect to to an image to adjust the image in order to maximize the loss. The pretrained model is already available for you to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also observe the signature of these mistakes reflected in the layer wise geometry of the representation. When the model is applied to the adversarial images, the rdms no longer show the progressive emergence of block-wise structure strongly around the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Extract model features from the adversarial images and plot the new RDMs\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "eps = 0.2\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Grab an adversarially robust model\n",
    "\n",
    "path = \"adversarial_model.pth\"\n",
    "model_robust = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Testing the model on a newly generated set of adversarial images, we can see that it performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Test an adversarially robust model\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "adv_imgs = generate_adversarial(model_robust, imgs.to(device), labels.to(device), eps)\n",
    "test_adversarial(model_robust, adv_imgs.to(device), labels.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "And the structure in the rdms are also restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Extract robust model features from the adversarial images and plot the new RDMs\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model_robust, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding exercise\n",
    "\n",
    "Use MDS to visualize the changes in the representational geometry acorss the layers of this network in response to original mnist test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Fill out and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = ...\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['MDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model_robust, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['MDS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Comparing representational geometry paths between models\n",
    "\n",
    "We can generalize our method to look at the representioanl geometry paths of multiple models. We will first compute the rdms for each layer of each network and then concatenate them all. Then we look at the angles between all the layers (from different models).\n",
    "\n",
    "In this case, we will be looking at the feedforward and the robust feedforward models. The left panel in the figure below shows the angles between the representational geometries of all the layers in these two models. Networks are color coded for clarity, with the original feedforward model shown in blue and the robust feedforward model shown in green. The input pixels are shown again in black and the ground truth labels in magenta.\n",
    "\n",
    "Embedding this matrix in a 2D space with MDS again gives us the path that each nework took from the input images to the labels.\n",
    "\n",
    "In this case, they are both tested on the original MNIST test images and both can perform the task. This is reflected in both models reaching the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "But what if we test the models on adversarial images. We can see below that in this case only the robust model can actually reach the labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, adv_imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can see from this example that these path depend on the chosen stimuli!\n",
    "\n",
    "When we use adversarial imges, the two networks are taking different and diverging paths in the space of representational geometries. With only the robust model arriving near the embedding of the lables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding exercise\n",
    "\n",
    "Train another instance of the model on the original MNIST data for only one epoch with a different seed. Then compare the representationl paths of the two instances that are both trained on the original MNIST with one another. Display the paths in blue and cyan colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: fill in the missing variables ##\n",
    "# Fill out function and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################\n",
    "\n",
    "args = build_args()\n",
    "torch.manual_seed(args.seed+1)\n",
    "args.epochs = 1\n",
    "#build_model\n",
    "model_new_seed = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(...)\n",
    "train_model(args, model_new_seed, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "args = build_args()\n",
    "torch.manual_seed(args.seed+1)\n",
    "args.epochs = 1\n",
    "#build_model\n",
    "model_new_seed = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model_new_seed.parameters(), lr=args.lr)\n",
    "train_model(args, model_new_seed, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_new_seed = extract_features(model_new_seed, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'model': model_features, 'model new seed': model_features_new_seed}\n",
    "model_colors = {'model': 'b', 'model new seed': 'c'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. Think about the original space of representations: can you understand why the paths all look curved when we project them into 2D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Materal\n",
    "\n",
    "### Representational geometry of recurrent models\n",
    "\n",
    "Transformations of representations can occur across space and time, e.g. layers of a neural network and steps of recurrent computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Just as the layers in a feedforward DNN can change the representational geometry in order to perfom the task, steps in a reucrrent network can reuse the same layer to reach the same computational depth.\n",
    "\n",
    "In this section, we look at a very simple recurrent network with only 2650 trainable paramters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here is a simple diagram of this network\n",
    "\n",
    "![Recurrent convolutional neural network](https://raw.githubusercontent.com/Hosseinadeli/representational_geometry/main/figures/rcnn_tutorial.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Definition & training of RNN\n",
    "\n",
    "class recurrent_Net(nn.Module):\n",
    "    def __init__(self, time_steps=5):\n",
    "        super(recurrent_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, 1, padding=1)\n",
    "        self.readout = nn.Sequential(OrderedDict([\n",
    "            ('dropout', nn.Dropout(0.25)),\n",
    "            ('avgpool', nn.AdaptiveAvgPool2d(1)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('linear', nn.Linear(16, 10))\n",
    "        ]))\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.conv1(input)\n",
    "        x = input\n",
    "        for t in range(0, self.time_steps):\n",
    "            x = input + self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.readout(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# build and train the model\n",
    "args = build_args()\n",
    "torch.manual_seed(args.seed)\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "\n",
    "#build_model\n",
    "model_recurrent = recurrent_Net(5).to(device)\n",
    "\n",
    "print(model_recurrent)\n",
    "print('number of parameters: ', sum(p.numel() for p in model_recurrent.parameters() if p.requires_grad))\n",
    "optimizer = optim.Adadelta(model_recurrent.parameters(), lr=args.lr)\n",
    "\n",
    "args.epochs = 5\n",
    "train_model(args, model_recurrent, optimizer) #train the model for 5 epochs ~ %99 accuracy ~ 30 sec colab gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<br>We can first look at the computational steps in this network. As we see below, the conv2 operation is repeated for 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, _ = get_graph_node_names(model_recurrent)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Plotting the rdms after each application of the conv2 operation shows the same progressive emergence of the blockwise structure around the diagonal mediating the correct classificaiton in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4'] #\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers) #, plot = 'rolled'\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also look at how the different dimensionality reduction techniques capture the dynamics of changing geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational geometry paths for recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can look at recurrent computational steps of the model as a path in the representaionl geometry space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "#rdms, rdms_dict = calc_rdms(model_features)\n",
    "features = {'recurrent model': model_features_recurrent}\n",
    "model_colors = {'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also look at the paths taken by the feedforwward and the reucrrent models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'recurrent model': model_features_recurrent}\n",
    "model_colors = {'feedforward model': 'b', 'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
