{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Computation as transformation of representational geometries\n",
    "\n",
    "**Week 1, Day 3: Comparing Artificial And Biological Networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Hossein Adeli\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "In this tutorial we aim to understand that in networks computation can be understood as a sequence of transformations of the representation of the input, where each transformation can remove information, preserve other information, and change the format in which the information is represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"qmv5r\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "\n",
    "!pip install -q torch torchvision matplotlib numpy scikit-learn rsatoolbox scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import rsatoolbox\n",
    "from rsatoolbox.data import Dataset\n",
    "from rsatoolbox.rdm.calc import calc_rdm\n",
    "from scipy import stats\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"\n",
    "    Samples a specified number of images from a data loader.\n",
    "\n",
    "    Inputs:\n",
    "    - data_loader (torch.utils.data.DataLoader): Data loader containing images and labels.\n",
    "    - n (int): Number of images to sample per class.\n",
    "    - plot (bool): Whether to plot the sampled images using matplotlib.\n",
    "\n",
    "    Outputs:\n",
    "    - imgs (torch.Tensor): Sampled images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the sampled images.\n",
    "    \"\"\"\n",
    "\n",
    "    imgs, targets = next(iter(data_loader))\n",
    "\n",
    "    imgs_o = []\n",
    "    labels = []\n",
    "    for value in range(10):\n",
    "        cat_imgs = imgs[np.where(targets == value)][0:n]\n",
    "        imgs_o.append(cat_imgs)\n",
    "        labels.append([value]*len(cat_imgs))\n",
    "\n",
    "    imgs = torch.cat(imgs_o, dim=0)\n",
    "    labels = torch.tensor(labels).flatten()\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(torch.moveaxis(make_grid(imgs, nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "        plt.axis('off')\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "def plot_rdms(model_rdms):\n",
    "    \"\"\"\n",
    "    Plots the Representational Dissimilarity Matrices (RDMs) for each layer of a model.\n",
    "\n",
    "    Inputs:\n",
    "    - model_rdms (dict): A dictionary where keys are layer names and values are the corresponding RDMs.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig = plt.figure(figsize=(14, 4))\n",
    "        gs = fig.add_gridspec(1, len(model_rdms))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        for l in range(len(model_rdms)):\n",
    "\n",
    "            layer = list(model_rdms.keys())[l]\n",
    "            rdm = np.squeeze(model_rdms[layer])\n",
    "\n",
    "            if len(rdm.shape) < 2:\n",
    "                rdm = rdm.reshape( (int(np.sqrt(rdm.shape[0])), int(np.sqrt(rdm.shape[0]))) )\n",
    "\n",
    "            rdm = rdm / np.max(rdm)\n",
    "\n",
    "            ax = plt.subplot(gs[0,l])\n",
    "            ax_ = ax.imshow(rdm, cmap='Greys')\n",
    "            ax.set_title(f'{layer}')\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        cbar_ax.text(-1.6, 0.05, 'Normalized euclidean distance', size=12, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def rep_path(model_features, model_colors, labels=None, rdm_calc_method='euclidean', rdm_comp_method='cosine'):\n",
    "    \"\"\"\n",
    "    Represents paths of model features in a reduced-dimensional space.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): Dictionary containing model features for each model.\n",
    "    - model_colors (dict): Dictionary mapping model names to colors for visualization.\n",
    "    - labels (array-like, optional): Array of labels corresponding to the model features.\n",
    "    - rdm_calc_method (str, optional): Method for calculating RDMS ('euclidean' or 'correlation').\n",
    "    - rdm_comp_method (str, optional): Method for comparing RDMS ('cosine' or 'corr').\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        path_len = []\n",
    "        path_colors = []\n",
    "        rdms_list = []\n",
    "        ax_ticks = []\n",
    "        tick_colors = []\n",
    "        model_names = list(model_features.keys())\n",
    "        for m in range(len(model_names)):\n",
    "            model_name = model_names[m]\n",
    "            features = model_features[model_name]\n",
    "            path_colors.append(model_colors[model_name])\n",
    "            path_len.append(len(features))\n",
    "            ax_ticks.append(list(features.keys()))\n",
    "            tick_colors.append([model_colors[model_name]]*len(features))\n",
    "            rdms, _ = calc_rdms(features, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "\n",
    "        path_len = np.insert(np.cumsum(path_len),0,0)\n",
    "\n",
    "        if labels is not None:\n",
    "            rdms, _ = calc_rdms({'labels' : F.one_hot(labels).float().to(device)}, method=rdm_calc_method)\n",
    "            rdms_list.append(rdms)\n",
    "            ax_ticks.append(['labels'])\n",
    "            tick_colors.append(['m'])\n",
    "            idx_labels = -1\n",
    "\n",
    "        rdms = rsatoolbox.rdm.concat(rdms_list)\n",
    "\n",
    "        #Flatten the list\n",
    "        ax_ticks = [l for model_layers in ax_ticks for l in model_layers]\n",
    "        tick_colors = [l for model_layers in tick_colors for l in model_layers]\n",
    "        tick_colors = ['k' if tick == 'input' else color for tick, color in zip(ax_ticks, tick_colors)]\n",
    "\n",
    "        rdms_comp = rsatoolbox.rdm.compare(rdms, rdms, method=rdm_comp_method)\n",
    "        if rdm_comp_method == 'cosine':\n",
    "            rdms_comp = np.arccos(rdms_comp)\n",
    "        rdms_comp = np.nan_to_num(rdms_comp, nan=0.0)\n",
    "\n",
    "        # reduce dim to 2\n",
    "        transformer = manifold.MDS(n_components = 2, max_iter=1000, n_init=10, normalized_stress='auto')\n",
    "        dims= transformer.fit_transform(rdms_comp)\n",
    "\n",
    "        # remove duplicates of the input layer from multiple models\n",
    "        remove_duplicates = np.where(np.array(ax_ticks) == 'input')[0][1:]\n",
    "        for index in remove_duplicates:\n",
    "            del ax_ticks[index]\n",
    "            del tick_colors[index]\n",
    "            rdms_comp = np.delete(np.delete(rdms_comp, index, axis=0), index, axis=1)\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        ax = plt.subplot(gs[0,0])\n",
    "        ax_ = ax.imshow(rdms_comp, cmap='viridis_r')\n",
    "        fig.subplots_adjust(left=0.2)\n",
    "        cbar_ax = fig.add_axes([-0.01, 0.2, 0.01, 0.5])\n",
    "        #cbar_ax.text(-7, 0.05, 'angles between rdms', size=10, rotation=90)\n",
    "        fig.colorbar(ax_, cax=cbar_ax,location='left')\n",
    "        ax.set_title('Angles between layer rdms')\n",
    "        ax.set_xticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7, rotation=83)\n",
    "        ax.set_yticks(np.arange(len(ax_ticks)), labels=ax_ticks, fontsize=7)\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.xaxis.get_ticklabels())]\n",
    "        [t.set_color(i) for (i,t) in zip(tick_colors, ax.yaxis.get_ticklabels())]\n",
    "\n",
    "        ax = plt.subplot(gs[0,1])\n",
    "        amin, amax = dims.min(), dims.max()\n",
    "        amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "\n",
    "        for i in range(len(rdms_list)-1):\n",
    "\n",
    "            path_indices = np.arange(path_len[i], path_len[i+1])\n",
    "            ax.plot(dims[path_indices, 0], dims[path_indices, 1], color=path_colors[i], marker='.')\n",
    "            ax.set_title('Representational geometry path')\n",
    "            ax.set_xlim([amin, amax])\n",
    "            ax.set_ylim([amin, amax])\n",
    "            ax.set_xlabel(f\"dim 1\")\n",
    "            ax.set_ylabel(f\"dim 2\")\n",
    "\n",
    "        # if idx_input is not None:\n",
    "        idx_input = 0\n",
    "        ax.plot(dims[idx_input, 0], dims[idx_input, 1], color='k', marker='s')\n",
    "\n",
    "        if labels is not None:\n",
    "            ax.plot(dims[idx_labels, 0], dims[idx_labels, 1], color='m', marker='*')\n",
    "\n",
    "        ax.legend(model_names, fontsize=8)\n",
    "        fig.tight_layout()\n",
    "\n",
    "def plot_dim_reduction(model_features, labels, transformer_funcs):\n",
    "    \"\"\"\n",
    "    Plots the dimensionality reduction results for model features using various transformers.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): Dictionary containing model features for each layer.\n",
    "    - labels (array-like): Array of labels corresponding to the model features.\n",
    "    - transformer_funcs (list): List of dimensionality reduction techniques to apply ('PCA', 'MDS', 't-SNE').\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        transformers = []\n",
    "        for t in transformer_funcs:\n",
    "            if t == 'PCA': transformers.append(PCA(n_components=2))\n",
    "            if t == 'MDS': transformers.append(manifold.MDS(n_components = 2, normalized_stress='auto'))\n",
    "            if t == 't-SNE': transformers.append(manifold.TSNE(n_components = 2, perplexity=40, verbose=0))\n",
    "\n",
    "        fig = plt.figure(figsize=(14, 4*len(transformers)))\n",
    "        # and we add one plot per reference point\n",
    "        gs = fig.add_gridspec(len(transformers), len(model_features))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        return_layers = list(model_features.keys())\n",
    "\n",
    "        for f in range(len(transformer_funcs)):\n",
    "\n",
    "            for l in range(len(return_layers)):\n",
    "                layer =  return_layers[l]\n",
    "                feats = model_features[layer].detach().cpu().flatten(1)\n",
    "                feats_transformed= transformers[f].fit_transform(feats)\n",
    "\n",
    "                amin, amax = feats_transformed.min(), feats_transformed.max()\n",
    "                amin, amax = (amin + amax) / 2 - (amax - amin) * 5/8, (amin + amax) / 2 + (amax - amin) * 5/8\n",
    "                ax = plt.subplot(gs[f,l])\n",
    "                ax.set_xlim([amin, amax])\n",
    "                ax.set_ylim([amin, amax])\n",
    "                ax.axis(\"off\")\n",
    "                #ax.set_title(f'{layer}')\n",
    "                if f == 0: ax.text(0.5, 1.12, f'{layer}', size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                if l == 0: ax.text(-0.3, 0.5, transformer_funcs[f], size=16, ha=\"center\", transform=ax.transAxes)\n",
    "                # Create a discrete color map based on unique labels\n",
    "                num_colors = len(np.unique(labels))\n",
    "                cmap = plt.get_cmap('viridis_r', num_colors) # 10 discrete colors\n",
    "                norm = mpl.colors.BoundaryNorm(np.arange(-0.5,num_colors), cmap.N)\n",
    "                ax_ = ax.scatter(feats_transformed[:, 0], feats_transformed[:, 1], c=labels, cmap=cmap, norm=norm)\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        fig.colorbar(ax_, cax=cbar_ax, ticks=np.linspace(0,9,10))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for image classification, consisting of two convolutional layers,\n",
    "    followed by two fully connected layers with dropout regularization.\n",
    "\n",
    "    Methods:\n",
    "    - forward(input): Defines the forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the network layers.\n",
    "\n",
    "        Layers:\n",
    "        - conv1: First convolutional layer with 1 input channel, 32 output channels, and a 3x3 kernel.\n",
    "        - conv2: Second convolutional layer with 32 input channels, 64 output channels, and a 3x3 kernel.\n",
    "        - dropout1: Dropout layer with a dropout probability of 0.25.\n",
    "        - dropout2: Dropout layer with a dropout probability of 0.5.\n",
    "        - fc1: First fully connected layer with 9216 input features and 128 output features.\n",
    "        - fc2: Second fully connected layer with 128 input features and 10 output features.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - input (torch.Tensor): Input tensor of shape (batch_size, 1, height, width).\n",
    "\n",
    "        Outputs:\n",
    "        - output (torch.Tensor): Output tensor of shape (batch_size, 10) representing the class probabilities for each input sample.\n",
    "        \"\"\"\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "class recurrent_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A recurrent neural network model for image classification, consisting of two convolutional layers\n",
    "    with recurrent connections and a readout layer.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(time_steps=5): Initializes the network layers and sets the number of time steps for recurrence.\n",
    "    - forward(input): Defines the forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_steps=5):\n",
    "        \"\"\"\n",
    "        Initializes the network layers and sets the number of time steps for recurrence.\n",
    "\n",
    "        Layers:\n",
    "        - conv1: First convolutional layer with 1 input channel, 16 output channels, and a 3x3 kernel with a stride of 3.\n",
    "        - conv2: Second convolutional layer with 16 input channels, 16 output channels, and a 3x3 kernel with padding of 1.\n",
    "        - readout: A sequential layer containing:\n",
    "            - dropout: Dropout layer with a dropout probability of 0.25.\n",
    "            - avgpool: Adaptive average pooling layer to reduce spatial dimensions to 1x1.\n",
    "            - flatten: Flatten layer to convert the 2D pooled output to 1D.\n",
    "            - linear: Fully connected layer with 16 input features and 10 output features.\n",
    "        - time_steps (int): Number of time steps for the recurrent connection.\n",
    "        \"\"\"\n",
    "        super(recurrent_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, 1, padding=1)\n",
    "        self.readout = nn.Sequential(OrderedDict([\n",
    "            ('dropout', nn.Dropout(0.25)),\n",
    "            ('avgpool', nn.AdaptiveAvgPool2d(1)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('linear', nn.Linear(16, 10))\n",
    "        ]))\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - input (torch.Tensor): Input tensor of shape (batch_size, 1, height, width).\n",
    "\n",
    "        Outputs:\n",
    "        - output (torch.Tensor): Output tensor of shape (batch_size, 10) representing the class probabilities for each input sample.\n",
    "        \"\"\"\n",
    "        input = self.conv1(input)\n",
    "        x = input\n",
    "        for t in range(0, self.time_steps):\n",
    "            x = input + self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.readout(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Arguments for training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - device (torch.device): The device to use for training (CPU/GPU).\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "    - epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader, return_features=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be evaluated.\n",
    "    - device (torch.device): The device to use for evaluation (CPU/GPU).\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    - return_features (bool): If True, returns the features from the model. Default is False.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def build_args():\n",
    "    \"\"\"\n",
    "    Builds and parses command-line arguments for training.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available() #not args.no_cuda and\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    args.use_cuda = use_cuda\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "def fetch_dataloaders(args):\n",
    "    \"\"\"\n",
    "    Fetches the data loaders for training and testing datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    \"\"\"\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if args.use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "        dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "        dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "def train_model(args, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model using the specified arguments and optimizer.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "\n",
    "    Outputs:\n",
    "    - None: The function trains the model and optionally saves it.\n",
    "    \"\"\"\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "def calc_rdms(model_features, method='correlation'):\n",
    "    \"\"\"\n",
    "    Calculates representational dissimilarity matrices (RDMs) for model features.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): A dictionary where keys are layer names and values are features of the layers.\n",
    "    - method (str): The method to calculate RDMs, e.g., 'correlation'. Default is 'correlation'.\n",
    "\n",
    "    Outputs:\n",
    "    - rdms (pyrsa.rdm.RDMs): RDMs object containing dissimilarity matrices.\n",
    "    - rdms_dict (dict): A dictionary with layer names as keys and their corresponding RDMs as values.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        if args.use_cuda:\n",
    "            feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape) > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        ds = Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = calc_rdm(ds_list, method=method)\n",
    "    rdms_dict = {list(model_features.keys())[i]: rdms.get_matrices()[i] for i in range(len(model_features))}\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Performs FGSM attack on an image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (torch.Tensor): Original image.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "    - data_grad (torch.Tensor): Gradient of the data.\n",
    "\n",
    "    Outputs:\n",
    "    - perturbed_image (torch.Tensor): Perturbed image after FGSM attack.\n",
    "    \"\"\"\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Converts a batch of normalized tensors to their original scale.\n",
    "\n",
    "    Inputs:\n",
    "    - batch (torch.Tensor): Batch of normalized tensors.\n",
    "    - mean (torch.Tensor or list): Mean used for normalization.\n",
    "    - std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - torch.Tensor: Batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(batch.device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(batch.device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "def generate_adversarial(model, imgs, targets, epsilon):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using FGSM attack.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to attack.\n",
    "    - imgs (torch.Tensor): Batch of images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "\n",
    "    Outputs:\n",
    "    - adv_imgs (torch.Tensor): Batch of adversarial images.\n",
    "    \"\"\"\n",
    "    adv_imgs = []\n",
    "\n",
    "    for img, target in zip(imgs, targets):\n",
    "        img = img.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        img.requires_grad = True\n",
    "\n",
    "        output = model(img)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad = img.grad.data\n",
    "        data_denorm = denorm(img)\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        adv_imgs.append(perturbed_data_normalized.detach())\n",
    "\n",
    "    return torch.cat(adv_imgs)\n",
    "\n",
    "def test_adversarial(model, imgs, targets):\n",
    "    \"\"\"\n",
    "    Tests the model on adversarial examples and prints the accuracy.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be tested.\n",
    "    - imgs (torch.Tensor): Batch of adversarial images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    output = model(imgs)\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    final_acc = correct / float(len(imgs))\n",
    "    print(f\"adversarial test accuracy = {correct} / {len(imgs)} = {final_acc}\")\n",
    "\n",
    "def extract_features(model, imgs, return_layers, plot='none'):\n",
    "    \"\"\"\n",
    "    Extracts features from specified layers of the model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model from which to extract features.\n",
    "    - imgs (torch.Tensor): Batch of input images.\n",
    "    - return_layers (list): List of layer names from which to extract features.\n",
    "    - plot (str): Option to plot the features. Default is 'none'.\n",
    "\n",
    "    Outputs:\n",
    "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
    "    \"\"\"\n",
    "    if return_layers == 'all':\n",
    "        return_layers, _ = get_graph_node_names(model)\n",
    "    elif return_layers == 'layers':\n",
    "        layers, _ = get_graph_node_names(model)\n",
    "        return_layers = [l for l in layers if 'input' in l or 'conv' in l or 'fc' in l]\n",
    "\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=return_layers)\n",
    "    model_features = feature_extractor(imgs)\n",
    "\n",
    "    return model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"standard_model.pth\", \"adversarial_model.pth\", \"recurrent_model.pth\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/s5rt6/download\", \"https://osf.io/qv5eb/download\", \"https://osf.io/6hnwk/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"2e63c2cd77bc9f1fa67673d956ec910d\", \"25fb34497377921b54368317f68a7aa7\", \"ee5cea3baa264cb78300102fa6ed66e8\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU)\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Computation & representational geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We are going to use the same models, standard and adversarially trained, as in the previous tutorial, thus they are already prepared to get the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Grab a standard model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"standard_model.pth\"\n",
    "model = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In order to better visualize the sturcture in the model features, we grab 5 test images from each of the 10 digit categories. Below is a simple function to perform that. For the purpose of the visualization, we order the test images by category identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Just sample 5 per category to show the order\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=5, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First let's look at the computational steps in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Computational steps\n",
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can then focus on the main steps for visualization. We extract the features from these specific layers of the model for the sampled test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Features for different layers\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "print('features gathered for:', list(model_features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Calculating and plotting the RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now for each of the test images, we have the model features from each layer of the trained network. We can look at how the representational (dis)similarity between these samples changes across the layers of the network. For each layer, we flatten the feautres so that each test image is represented by a vector (i.e. a point in space) and then we use the rsatooblox to compute the euclidean distances between the points in that reprsentational space.\n",
    "\n",
    "The result is a matrix of distances between each two samples for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now plot the rdms for the input pixels and the two conv and two fully connected layers below.\n",
    "\n",
    "The dimension of these RDMs are `#stimuli` by `#stimuli`.\n",
    "The brighter cells show smaller dissimlarity between the two stimuli and the diagonal would have the dissimiarity of 0 (comparing the representation of one image to itself).\n",
    "\n",
    "Since the instances of each category are next to each other, the brighter blocks emerging around the diagonal show that the representatiaons for category instances become similar to one another across the levels of the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Putting it all together\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20) #grab 20 samples from each category\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "model_features = extract_features(model, imgs.to(device), return_layers=return_layers)\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Computation in the network can be understood as the transformation of representational geometries because the information-content of the representation at each stage are captured by its representational geometry.\n",
    "\n",
    "As we see in above rdms the geomety captures how the representation in later layers become more similar for instances of the same category since this model was trained for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Another way to examine the geometry of the representaion changing across layers is by reducing the dimensionality and displaying the samples in 2D space.\n",
    "\n",
    "The representaionl spaces of each layer of a deep network project each stimulus to a point in very high dimentional spaces. Methods such as PCA (principal component analysis), MDS (Multidimensional_scaling), and t-SNE (t-distributed stochastic neighbor embedding) attemp to capture the same geometry in lower dimensions.\n",
    "\n",
    "Below we look at the representational geometry of the same network layers using these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For this visulization we take 500 samples from the test set and we color code them based on their category. The figure below shows the scatter plots for these samples across the layers of the network. Each panel is the 2D projection of the feature representations using a specific method.\n",
    "\n",
    "The methods all show that image representations progressively cluster based on category across the layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Sequential image representation clustering\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 50 samples from the each category\n",
    "\n",
    "# Alternatively get a whole test batch\n",
    "#imgs, labels = next(iter(test_loader))\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational path\n",
    "\n",
    "We can now go a step further and think of the computational steps in a network as steps in a path in the space of representational geometries.\n",
    "\n",
    "This means that each stage in the network is changing the geometry to make it more and more like the desired geometry which would be the labels for a network trained for classification.\n",
    "\n",
    "To examine this process, we first have to quantify what the geometry after each step looks like. For this we can calculate the rdms based on the euclidean distances between the representations of images for each layer.\n",
    "\n",
    "The next step is to quantify how different the geometries from each step of the network are from one another. To perform this operation we first flatten the rdms from different model steps to make them a vector and then we can look at the angles between those vectors. Note that this measure of distance is between the representational geometries of different layers and not the representations themselves.\n",
    "\n",
    "In this tutorial we compute the similarity between geometries with a metric measure like the arccos of cos distance between the RDMs from different layers of the network. The resulting matrix has the dimentions of #model_computational_steps by #model_computational_steps.\n",
    "\n",
    "The last step to visualize the path is to then embed the distances between the geometries in a lower dimensional space. We use MDS to reduce the dimensions to two in order to show each computational step as a point in a 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The function below plots the computation that takes place across the layers of the network as a path in the space of representational gemoetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Representational Path\n",
    "\n",
    "imgs, labels = next(iter(test_loader)) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The left panel shows the angles between the rdms that catpure the representational geometry for each layer.\n",
    "\n",
    "The brigher colors indicate higher similarity. The diagonal is the angle between a layer geometry with itself which is zero. The brighter colors around the diagonal show that the distances between adjacent layers are smaller and therefore each step is not chaning the representation too much.\n",
    "\n",
    "The figure on the right shows the embedding of these angles for each layer in a 2D space using MDS. This figure basically tries to capturet the same distances between the layers as shown in the matrix on the left in a 2D space. Connecting the steps starting from the input (shown as the black square) forms a path in the space of representional geometries. The last step in this path (the softmax layer) reaches close to the embedding of the lables in the same 2D space.\n",
    "\n",
    "There are a few things to note about the path. One is that the path is generally smooth. This shows that the steps in how the representatoins change are not big, which we could also observe from the distance matrix on the left. From the matrix, we can also see that the distance between the representaionl geomety of the input and the lables is large but we see these two points somewhat close to each other in the path. This is partly becuase MDS tries to optimize to capture all the distances in the 2D space, they cannot all be veridically captured and a stronger curvature is introduced to the path. Therefore details of the curvature in 2D should be interpreted with caution. However, the size of the steps are generally informative. For example, we see in the left matrix that there is a big change in the representiaonal geometry going form conv to fully connected layers, corresponding to a larger step in the path.\n",
    "\n",
    "Thus the computation performed by this model is a path from the geometry of the input pixels to the geometry of the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. How do you think the representational path will be different if we take a smaller sample of images to create the RDMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To test what will actually happen by taking a sample of 5 images per category and visualizing the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Test your idea by executing this cell!\n",
    "\n",
    "#imgs, labels = next(iter(test_loader)) #grab 500 samples from the test set\n",
    "imgs, labels = sample_images(test_loader, n=5)\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features}\n",
    "model_colors = {'feedforward model': 'b'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As you can see the two paths are very similar. As mentioned earlier, The differences that we see here (such as different rotations) are not meaningful differences as different runs of the MDS can vary along such factors. We should judge the general paths of the models to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Adversarial images\n",
    "\n",
    "Adversarial images pose a generalization challange to DNNs. Here we examine how this challenge and lower perofrmance are reflected in the the representational geometry path. First we use FGSM (Fast Gradient Sign method) to generate adversarial images for the trained feedforward model. This is a \"white-boxed\" attack that uses the gradients from the model loss with respect to to an image to adjust the image in order to maximize the loss. The pretrained model is already available for you to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also observe the signature of these mistakes reflected in the layer wise geometry of the representation. When the model is applied to the adversarial images, the rdms no longer show the progressive emergence of block-wise structure strongly around the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Extract model features from the adversarial images and plot the new RDMs\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "eps = 0.2\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Grab an adversarially robust model\n",
    "\n",
    "path = \"adversarial_model.pth\"\n",
    "model_robust = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Testing the model on a newly generated set of adversarial images, we can see that it performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Test an adversarially robust model\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "adv_imgs = generate_adversarial(model_robust, imgs.to(device), labels.to(device), eps)\n",
    "test_adversarial(model_robust, adv_imgs.to(device), labels.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "And the structure in the rdms are also restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Extract robust model features from the adversarial images and plot the new RDMs\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2'] #inputs/outputs will be added automatically\n",
    "model_features = extract_features(model_robust, adv_imgs.to(device), return_layers)\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1\n",
    "\n",
    "Use MDS to visualize the changes in the representational geometry acorss the layers of this network in response to original mnist test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Fill out and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################\n",
    "\n",
    "return_layers = ['input', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = ...\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['MDS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/solutions/W1D3_Tutorial2_Solution_4df573af.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1468.0 height=377.0 src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/static/W1D3_Tutorial2_Solution_4df573af_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Comparing representational geometry paths between models\n",
    "\n",
    "We can generalize our method to look at the representioanl geometry paths of multiple models. We will first compute the rdms for each layer of each network and then concatenate them all. Then we look at the angles between all the layers (from different models).\n",
    "\n",
    "In this case, we will be looking at the feedforward and the robust feedforward models. The left panel in the figure below shows the angles between the representational geometries of all the layers in these two models. Networks are color coded for clarity, with the original feedforward model shown in blue and the robust feedforward model shown in green. The input pixels are shown again in black and the ground truth labels in magenta.\n",
    "\n",
    "Embedding this matrix in a 2D space with MDS again gives us the path that each nework took from the input images to the labels.\n",
    "\n",
    "In this case, they are both tested on the original MNIST test images and both can perform the task. This is reflected in both models reaching the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "But what if we test the models on adversarial images. We can see below that in this case only the robust model can actually reach the labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "adv_imgs = generate_adversarial(model, imgs.to(device), labels.to(device), eps)\n",
    "\n",
    "model_features = extract_features(model, adv_imgs.to(device), return_layers='all')\n",
    "model_features_robust = extract_features(model_robust, adv_imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'robust feedforward model': model_features_robust}\n",
    "model_colors = {'feedforward model': 'b', 'robust feedforward model': 'g'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When we use adversarial imges, the two networks are taking different and diverging paths in the space of representational geometries. With only the robust model arriving near the embedding of the lables.\n",
    "\n",
    "We can see from this example that these path depend on the chosen stimuli!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. Can you identify different aspects of what you see in these paths from the distance matrix on the left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2\n",
    "\n",
    "Train another instance of the model on the original MNIST data for only one epoch with a different seed. Then compare the representationl paths of the two instances that are both trained on the original MNIST with one another. Display the paths in blue and cyan colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: fill in the missing variables ##\n",
    "# Fill out function and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################\n",
    "\n",
    "args = build_args()\n",
    "torch.manual_seed(args.seed+1)\n",
    "args.epochs = 1\n",
    "#build_model\n",
    "model_new_seed = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(...)\n",
    "train_model(args, model_new_seed, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/NeuroAI_Course/tree/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/solutions/W1D3_Tutorial2_Solution_c0332689.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_new_seed = extract_features(model_new_seed, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'model': model_features, 'model new seed': model_features_new_seed}\n",
    "model_colors = {'model': 'b', 'model new seed': 'c'}\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. What is your interpretation of how the two paths compare to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "By the end of this tutorial you are able to:\n",
    "\n",
    "- Understand how to characterize the computation that happens across different layers of a network as a path with each step gradually changing the geometry of the representation to go from input pixels to target labels.\n",
    "- Examine the paths for different model architectures and different inputs and learn how to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus Section: Representational geometry of recurrent models\n",
    "\n",
    "Transformations of representations can occur across space and time, e.g. layers of a neural network and steps of recurrent computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Just as the layers in a feedforward DNN can change the representational geometry in order to perfom the task, steps in a reucrrent network can reuse the same layer to reach the same computational depth.\n",
    "\n",
    "In this section, we look at a very simple recurrent network with only 2650 trainable paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here is a simple diagram of this network\n",
    "\n",
    "![Recurrent convolutional neural network](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/static/rcnn_tutorial.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Grab a recurrent model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"recurrent_model.pth\"\n",
    "model_recurrent = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<br>We can first look at the computational steps in this network. As we see below, the conv2 operation is repeated for 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "train_nodes, _ = get_graph_node_names(model_recurrent)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Plotting the rdms after each application of the conv2 operation shows the same progressive emergence of the blockwise structure around the diagonal mediating the correct classificaiton in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4'] #\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers) #, plot = 'rolled'\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also look at how the different dimensionality reduction techniques capture the dynamics of changing geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Representational geometry paths for recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can look at recurrent computational steps of the model as a path in the representaionl geometry space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "#rdms, rdms_dict = calc_rdms(model_features)\n",
    "features = {'recurrent model': model_features_recurrent}\n",
    "model_colors = {'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also look at the paths taken by the feedforwward and the reucrrent models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'recurrent model': model_features_recurrent}\n",
    "model_colors = {'feedforward model': 'b', 'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "What we can see here is that different models can take very different paths in the space of representational geomerties in order to map images to labels. This is because there exist many different functional mappings to do a classification task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
