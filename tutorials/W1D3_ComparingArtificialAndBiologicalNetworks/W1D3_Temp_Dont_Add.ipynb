{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus Section: Representational geometry of recurrent models\n",
    "\n",
    "Transformations of representations can occur across space and time, e.g., layers of a neural network and steps of recurrent computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the layers in a feedforward DNN can change the representational geometry to perform a task, steps in a recurrent network can reuse the same layer to reach the same computational depth.\n",
    "\n",
    "In this section, we look at a very simple recurrent network with only 2650 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram of this network:\n",
    "\n",
    "![Recurrent convolutional neural network](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/static/rcnn_tutorial.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Grab a recurrent model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"recurrent_model.pth\"\n",
    "model_recurrent = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We can first look at the computational steps in this network. As we see below, the `conv2` operation is repeated for 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, _ = get_graph_node_names(model_recurrent)\n",
    "print('The computational steps in the network are: \\n', train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the RDMs after each application of the `conv2` operation shows the same progressive emergence of the blockwise structure around the diagonal, mediating the correct classification in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=20)\n",
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "rdms, rdms_dict = calc_rdms(model_features)\n",
    "plot_rdms(rdms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how the different dimensionality reduction techniques capture the dynamics of changing geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = ['conv2', 'conv2_1', 'conv2_2', 'conv2_3', 'conv2_4']\n",
    "\n",
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model_recurrent, imgs.to(device), return_layers)\n",
    "\n",
    "plot_dim_reduction(model_features, labels, transformer_funcs =['PCA', 'MDS', 't-SNE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representational geometry paths for recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the model's recurrent computational steps as a path in the representational geometry space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "#rdms, rdms_dict = calc_rdms(model_features)\n",
    "features = {'recurrent model': model_features_recurrent}\n",
    "model_colors = {'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the paths taken by the feedforward and the recurrent models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_images(test_loader, n=50) #grab 500 samples from the test set\n",
    "model_features = extract_features(model, imgs.to(device), return_layers='all')\n",
    "model_features_recurrent = extract_features(model_recurrent, imgs.to(device), return_layers='all')\n",
    "\n",
    "features = {'feedforward model': model_features, 'recurrent model': model_features_recurrent}\n",
    "model_colors = {'feedforward model': 'b', 'recurrent model': 'y'}\n",
    "\n",
    "rep_path(features, model_colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is that different models can take very different paths in the space of representational geometries to map images to labels. This is because there exist many different functional mappings to do a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_recurrent_models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
