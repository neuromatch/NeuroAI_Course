{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Generalization and representational geometry\n",
    "\n",
    "**Week 1, Day 3: Comparing artificial and biological neural networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ JohnMark Taylor & Zhuofan Josh Ying\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Patrick Mineault, Hlib Solodzhuk\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk, Patrick Mineault\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "Welcome to Tutorial 1 on Generalization and Representational Geometry. Building on the Intro Lecture, this tutorial aims to help you get your feet wet and characterize representations in terms of their geometry, as captured by the distances among inputs at different stages of processing. The key point of this tutorial is that the way machine learning models generalize depends on how they represent their inputs and, in particular, on how similar a new input is to other inputs in the internal representations. We focus on linear models, where the close relationship between similarity and generalization can be most easily understood from an intuitive and mathematical perspective. \n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the concept of representational geometry and the representational dissimilarity matrix.\n",
    "- Understand the connection between generalization and representational similarity in the linear case.\n",
    "- Understand how the analytic solution for linear regression can be seen as a weighted sum of training values, weighted by the test inputs' similarity to the training inputs.\n",
    "\n",
    "Exercises include:\n",
    "\n",
    "1.   Question on comparing RDMs.\n",
    "2.   Question on the relationship between RDMs and model performances.\n",
    "3.   Interactive exercise on how similarity structure affects predictions.\n",
    "\n",
    "This tutorial includes an interactive widget that plays only **locally or in Colab**, but **not in Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"uwn2g\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This tutorial uses `Graphviz` for additional visualizations. For proper execution of the functions, you should install it locally. Please visit this [link](https://graphviz.org/download/) to get the instructions depending on your local OS configurations (if you are running this notebook on Google Colab, just skip it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "\n",
    "!pip install -q ipympl ipywidgets mpl_interactions[\"jupyter\"] rsatoolbox torchlens\n",
    "!pip install -q graphviz\n",
    "\n",
    "# To install jupyter-matplotlib (ipympl) via pip\n",
    "!pip install -q torchlens\n",
    "!pip install -q ipympl ipywidgets matplotlib numpy scikit-learn torch torchvision rsatoolbox scipy\n",
    "\n",
    "\n",
    "# To install jupyter-matplotlib via conda (comment out if you are not using conda)\n",
    "# !conda install -c conda-forge ipympl\n",
    "\n",
    "# To install the JupyterLab extension for ipywidgets\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# To install the JupyterLab extension for jupyter-matplotlib\n",
    "# !jupyter labextension install jupyter-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "feedback_prefix = \"W1D3_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "# Import future dependencies\n",
    "from __future__ import print_function\n",
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import warnings\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from scipy import stats\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms, datasets\n",
    "import torchlens as tl\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# rsatoolbox imports\n",
    "import rsatoolbox\n",
    "from rsatoolbox.data import Dataset\n",
    "from rsatoolbox.rdm.calc import calc_rdm\n",
    "\n",
    "# Jupyter-specific imports\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"\n",
    "    Samples a specified number of images from a data loader.\n",
    "\n",
    "    Inputs:\n",
    "    - data_loader (torch.utils.data.DataLoader): Data loader containing images and labels.\n",
    "    - n (int): Number of images to sample per class.\n",
    "    - plot (bool): Whether to plot the sampled images using matplotlib.\n",
    "\n",
    "    Outpus:\n",
    "    - imgs (torch.Tensor): Sampled images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the sampled images.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        imgs, labels = next(iter(data_loader))\n",
    "\n",
    "        imgs_o = []\n",
    "        targets = []\n",
    "        for value in range(10):\n",
    "            imgs_o.append(imgs[np.where(labels == value)][0:n])\n",
    "            targets.append([value]*5)\n",
    "\n",
    "        imgs = torch.cat(imgs_o, dim=0)\n",
    "        targets = torch.tensor(targets).flatten()\n",
    "\n",
    "        if plot:\n",
    "            plt.imshow(torch.moveaxis(make_grid(imgs, nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "            plt.axis('off')\n",
    "\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "def plot_maps(model_features, model_name):\n",
    "    \"\"\"\n",
    "    Plots representational dissimilarity matrices (RDMs) across different layers of a model.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): a dictionary where keys are layer names and values are numpy arrays representing RDMs for each layer.\n",
    "    - model_name (str): the name of the model being visualized.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        fig = plt.figure(figsize=(14, 4))\n",
    "        fig.suptitle(f\"RDMs across layers for {model_name}\")\n",
    "        # and we add one plot per reference point\n",
    "        gs = fig.add_gridspec(1, len(model_features))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        for l in range(len(model_features)):\n",
    "\n",
    "            layer = list(model_features.keys())[l]\n",
    "            map_ = np.squeeze(model_features[layer])\n",
    "\n",
    "            if len(map_.shape) < 2:\n",
    "                map_ = map_.reshape( (int(np.sqrt(map_.shape[0])), int(np.sqrt(map_.shape[0]))) )\n",
    "\n",
    "            map_ = map_ / np.max(map_)\n",
    "\n",
    "            ax = plt.subplot(gs[0,l])\n",
    "            ax_ = ax.imshow(map_, cmap='magma_r')\n",
    "            ax.set_title(f'{layer}')\n",
    "            ax.set_xlabel(\"input index\")\n",
    "            if l==0:\n",
    "              ax.set_ylabel(\"input index\")\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        cbar = fig.colorbar(ax_, cax=cbar_ax)\n",
    "        cbar.set_label('Dissimilarity', rotation=270, labelpad=15)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Arguments for training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - device (torch.device): The device to use for training (CPU/GPU).\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "    - epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader, return_features=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be evaluated.\n",
    "    - device (torch.device): The device to use for evaluation (CPU/GPU).\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    - return_features (bool): If True, returns the features from the model. Default is False.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def build_args():\n",
    "    \"\"\"\n",
    "    Builds and parses command-line arguments for training.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available() #not args.no_cuda and\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    args.use_cuda = use_cuda\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "def fetch_dataloaders(args):\n",
    "    \"\"\"\n",
    "    Fetches the data loaders for training and testing datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "    \"\"\"\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if args.use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "        dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "        dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "def train_model(args, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model using the specified arguments and optimizer.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "\n",
    "    Outputs:\n",
    "    - None: The function trains the model and optionally saves it.\n",
    "    \"\"\"\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "def train_one_epoch_adversarial(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using adversarial examples.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Arguments for training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - device (torch.device): The device to use for training (CPU/GPU).\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "    - epoch (int): The current epoch number.\n",
    "\n",
    "    Outputs:\n",
    "    - None: The function prints the training progress and loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # train the model on adversarial images\n",
    "        epsilon = 0.2\n",
    "        data = generate_adversarial(model, data, target, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def train_model_adversarial(args, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model using adversarial training.\n",
    "\n",
    "    Inputs:\n",
    "    - args (Namespace): Parsed arguments with training configuration.\n",
    "    - model (torch.nn.Module): The model to be trained.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
    "\n",
    "    Outputs:\n",
    "    - None: The function trains the model using adversarial examples and optionally saves it.\n",
    "    \"\"\"\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch_adversarial(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"robust_mnist_cnn.pt\")\n",
    "\n",
    "def calc_rdms(model_features, method='correlation'):\n",
    "    \"\"\"\n",
    "    Calculates representational dissimilarity matrices (RDMs) for model features.\n",
    "\n",
    "    Inputs:\n",
    "    - model_features (dict): A dictionary where keys are layer names and values are features of the layers.\n",
    "    - method (str): The method to calculate RDMs, e.g., 'correlation'. Default is 'correlation'.\n",
    "\n",
    "    Outputs:\n",
    "    - rdms (pyrsa.rdm.RDMs): RDMs object containing dissimilarity matrices.\n",
    "    - rdms_dict (dict): A dictionary with layer names as keys and their corresponding RDMs as values.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape) > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        ds = Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = calc_rdm(ds_list, method=method)\n",
    "    rdms_dict = {list(model_features.keys())[i]: rdms.get_matrices()[i] for i in range(len(model_features))}\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Performs FGSM attack on an image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (torch.Tensor): Original image.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "    - data_grad (torch.Tensor): Gradient of the data.\n",
    "\n",
    "    Outputs:\n",
    "    - perturbed_image (torch.Tensor): Perturbed image after FGSM attack.\n",
    "    \"\"\"\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Converts a batch of normalized tensors to their original scale.\n",
    "\n",
    "    Inputs:\n",
    "    - batch (torch.Tensor): Batch of normalized tensors.\n",
    "    - mean (torch.Tensor or list): Mean used for normalization.\n",
    "    - std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - torch.Tensor: Batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(batch.device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(batch.device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "def generate_adversarial(model, imgs, targets, epsilon):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using FGSM attack.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to attack.\n",
    "    - imgs (torch.Tensor): Batch of images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    - epsilon (float): Perturbation magnitude.\n",
    "\n",
    "    Outputs:\n",
    "    - adv_imgs (torch.Tensor): Batch of adversarial images.\n",
    "    \"\"\"\n",
    "    adv_imgs = []\n",
    "\n",
    "    for img, target in zip(imgs, targets):\n",
    "        img = img.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        img.requires_grad = True\n",
    "\n",
    "        output = model(img)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad = img.grad.data\n",
    "        data_denorm = denorm(img)\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        adv_imgs.append(perturbed_data_normalized.detach())\n",
    "\n",
    "    return torch.cat(adv_imgs)\n",
    "\n",
    "def test_adversarial(model, imgs, targets):\n",
    "    \"\"\"\n",
    "    Tests the model on adversarial examples and prints the accuracy.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model to be tested.\n",
    "    - imgs (torch.Tensor): Batch of adversarial images.\n",
    "    - targets (torch.Tensor): Batch of target labels.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    output = model(imgs)\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    final_acc = correct / float(len(imgs))\n",
    "    print(f\"adversarial test accuracy = {correct} / {len(imgs)} = {final_acc}\")\n",
    "\n",
    "def extract_features(model, imgs, return_layers, plot='none'):\n",
    "    \"\"\"\n",
    "    Extracts features from specified layers of the model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The model from which to extract features.\n",
    "    - imgs (torch.Tensor): Batch of input images.\n",
    "    - return_layers (list): List of layer names from which to extract features.\n",
    "    - plot (str): Option to plot the features. Default is 'none'.\n",
    "\n",
    "    Outputs:\n",
    "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
    "    \"\"\"\n",
    "    model_history = tl.log_forward_pass(model, imgs, layers_to_save='all', vis_opt=plot)\n",
    "    model_features = {}\n",
    "    for layer in return_layers:\n",
    "        model_features[layer] = model_history[layer].tensor_contents.flatten(1)\n",
    "\n",
    "    return model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"standard_model.pth\", \"adversarial_model.pth\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/s5rt6/download\", \"https://osf.io/qv5eb/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"2e63c2cd77bc9f1fa67673d956ec910d\", \"25fb34497377921b54368317f68a7aa7\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU)\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Tutorial Introduction\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'ppW9BmOr790'), ('Bilibili', 'BV1pf421Q7vA')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_tutorial_introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: MNIST DNN Comparisons\n",
    "\n",
    "In this section, we are going to take a look at the MNIST Dataset and its adversarial version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this activity, you will use linear regression on representations of two neural networks trained on MNIST, one with adversarial training and one without. The objective is to observe:\n",
    "\n",
    "- How predictions for clean images of numbers are similar between networks for a deep layer\n",
    "- The close alignment of the predictions with the category structure\n",
    "- How the dissimilarity matrix reflects a category structure\n",
    "- How dissimilarity matrices from the two networks compare\n",
    "- The significant differences in predictions and dissimilarity matrices when applying adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Understanding the MNIST DNNs\n",
    "\n",
    "We start by defining two models on the MNIST dataset: a standard model and an adversarially robust model. The training process for the adversarially robust model includes a step where adversarial examples are generated and used to train the robust model.\n",
    "\n",
    "You are already provided with the trained models to save time. Still, you are free to explore the training code being included in the tutorial.\n",
    "\n",
    "### Defining the MNIST Model\n",
    "\n",
    "First, we define a neural network model that will be trained on the MNIST dataset. The MNIST dataset consists of 28x28 pixel images of handwritten digits (0 through 9). Our model, based on the LeNet architecture, includes two convolutional layers followed by dropout layers to reduce overfitting and, finally, fully connected layers to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title MNIST Model\n",
    "\n",
    "# modified and reorganized from https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "#lenet model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Training the Standard Model\n",
    "\n",
    "Once the model is defined, we would normally proceed to train it on the MNIST dataset. Training involves feeding batches of images and their corresponding labels to the model and adjusting the model's weights based on the loss computed from its predictions and the actual labels through backpropagation and gradient descent. This process is iterated over multiple epochs to improve the model's accuracy.\n",
    "\n",
    "The training code is copied here for reference in a commented block. We ran this code offline and saved a checkpoint. In the interest of time, we'll skip ahead to loading this checkpoint into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Build and train the model\n",
    "\n",
    "# args = build_args()\n",
    "# torch.manual_seed(args.seed)\n",
    "# train_loader, test_loader = fetch_dataloaders(args)\n",
    "\n",
    "# # build_model\n",
    "# model = Net().to(args.device)\n",
    "\n",
    "# print(model)\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "# # train model\n",
    "# train_model(args, model, optimizer) #train the model for 2 epochs ~ %99 accuracy ~ 30 sec colab gpu\n",
    "\n",
    "# torch.save(model, \"standard_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Grab a pretrained model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"standard_model.pth\"\n",
    "model = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Adversarial Attack & Model Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Generating Adversarial Examples**\n",
    "\n",
    "Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake. They are often **indistinguishable** from real data by humans but result in incorrect predictions by the model. Generating these examples and using them in training can improve a model's robustness against such attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Grab 5 test images from each category and visualize them\n",
    "imgs, targets = sample_images(test_loader, n=5, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we generate adversarial images using the Fast Gradient Sign Method (FGSM). This is a popular and straightforward adversarial attack technique designed to test the robustness of neural networks. Developed by Ian Goodfellow and his colleagues in 2014, FGSM backprops through the neural network to create perturbed inputs that maximize the loss:\n",
    "\n",
    "$$\\text{perturbed image} = \\text{image} + \\epsilon \\cdot \\text{sign}(\\nabla_{\\text{image}} J(\\text{image}, \\text{true label}))$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\text{perturbed image}$ is the adversarial image\n",
    "* $\\text{image}$ is the original input image\n",
    "* $\\epsilon$ is the magnitude of the perturbation\n",
    "* $\\nabla_{\\text{image}} J(\\text{image}, \\text{true label})$ is the gradient of the loss with respect to the input image\n",
    "* $\\text{sign}(\\cdot)$ returns the sign of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Generate adversarial images for the standard model\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "eps = 0.2\n",
    "imgs =imgs.to(args.device)\n",
    "targets = targets.to(args.device)\n",
    "\n",
    "adv_imgs = generate_adversarial(model, imgs, targets, eps)\n",
    "\n",
    "plt.imshow(torch.moveaxis(make_grid(adv_imgs.cpu(), nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Training the Adversarially Robust Model\n",
    "\n",
    "With adversarial examples at hand, we now train a model designed to be robust against such examples. The idea is to include adversarial examples in the training process, enabling the model to learn from and defend against them.\n",
    "\n",
    "Again, we've included the training code in a commented section, and you are presented with an already adversarially trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Train adversirally robust model\n",
    "\n",
    "# model_robust = Net().to(args.device)\n",
    "# optimizer = optim.Adadelta(model_robust.parameters(), lr=args.lr)\n",
    "# train_model_adversarial(args, model_robust, optimizer) #train the model for 2 epochs ~ %98 accuracy ~ 2 minutes on cpu\n",
    "# torch.save(model_robust, \"adversarial_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Grab an adversarially pretrained model\n",
    "\n",
    "path = \"adversarial_model.pth\"\n",
    "model_robust = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Generate adversarial images for the adversarially trained model\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "eps = 0.2\n",
    "imgs =imgs.to(args.device)\n",
    "targets = targets.to(args.device)\n",
    "\n",
    "adv_imgs_advmodel = generate_adversarial(model_robust, imgs, targets, eps)\n",
    "\n",
    "plt.imshow(torch.moveaxis(make_grid(adv_imgs_advmodel.cpu(), nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Evaluating Model Robustness\n",
    "\n",
    "Now that we have a trained model, we evaluate its resistance to adversarial attacks. We do this by testing the models on both standard and adversarial images and comparing their accuracies.\n",
    "\n",
    "While these adversarially generated images are visually indistinguishable from the original ones, they cause dramatic decreases in the standard model, while the adversarially trained model is robust to this kind of attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Test model\n",
    "\n",
    "# test adversarial robustness\n",
    "print(\"For standard model trained without adversarial examples (eps=0.2):\")\n",
    "test_adversarial(model, adv_imgs, targets)\n",
    "print(\"For adversrially trained model (eps=0.2):\")\n",
    "test_adversarial(model_robust, adv_imgs_advmodel, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should observe that the adversarially trained model has much higher accuracy on the adversarial images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Extracting Model Features and Analyzing Representations\n",
    "\n",
    "With the models trained and adversarial images generated, we proceed to extract features from different layers of our networks using [torchlens](https://github.com/johnmarktaylor91/torchlens), a package for extracting neural network activations and visualizing their computational graph.  This will allow us to peer into the data representations at different layers of the models in normal and adversarial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Extract model features with torchlens\n",
    "\n",
    "return_layers = ['input_1', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "features_model_imgs = extract_features(model, imgs, return_layers, plot = 'rolled') #comment this line if Graphviz installation was unsuccessful for you\n",
    "\n",
    "features_model_advimgs = extract_features(model, adv_imgs, return_layers)\n",
    "features_advmodel_imgs = extract_features(model_robust, imgs, return_layers)\n",
    "features_advmodel_advimgs = extract_features(model_robust, adv_imgs_advmodel, return_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_mnist_dnn_comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Creating and Comparing Representation Dissimilarity Matrices\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 15 minutes*\n",
    "\n",
    "Using the Representational Similarity Analysis (RSA) toolbox, you will create representational dissimilarity matrices (RDMs) for each model and condition (standard and adversarial). These matrices provide a visual representation of how inputs are represented within the network, offering insights into the models' generalization capabilities and the effects of adversarial training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## What is an RDM?\n",
    "\n",
    "Before diving into the practical applications, let's clarify what an RDM is. An RDM is a tool used to measure and visualize the dissimilarities in the responses of a neural system to different inputs. It is represented as a matrix $M$ where each element $M_{ij}$ measures how dissimilar the response patterns in some layer of a neural network are to the $i$-th and $j$-th input. Let's consider a particular estimator of representational dissimilarity:\n",
    "\n",
    "$$M_{ij} = 1 - r(x_i, x_j)$$\n",
    "\n",
    "Here $r(x_i, x_j)$ is the Pearson correlation coefficient between the representations of the $i$-th and $j$-th inputs. $1 - r(x_i, x_j)$ is also known as the correlation distance, which quantifies the dissimilarity between two inputs.\n",
    "\n",
    "RDMs help us understand how well a model differentiates between various types of inputs, which is a key aspect of generalization. They are particularly useful in deep learning as they help illustrate the level at which different layers of a network or different networks recognize and differentiate between input categories. A well-performing model will show clear patterns of dissimilarity corresponding to different input categories, especially in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Visualizing RDMs\n",
    "\n",
    "We first visualize the category structure by computing the RDM of the labels. To see clear blocks in RDMs—where blocks indicate that inputs of the same class are represented similarly—we need to ensure that input images are organized and grouped by their class labels before calculating the RDM. Here, we have 10 classes (numbers 0-9), each class with 5 samples, which sums up to 50 input images. Recall that each element of the RDM $M_{ij}$ refers to the dissimilarity between the representations of the $i$-th and $j$-th inputs. Make sure you understand the figure of RDM of the labels below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title RDMs for the labels (see the category structures)\n",
    "\n",
    "one_hot_labels = {\"labels\": F.one_hot(targets, num_classes=10) }\n",
    "rdms, rdms_dict = calc_rdms(one_hot_labels, method='euclidean')\n",
    "\n",
    "# plot rdm\n",
    "with plt.xkcd():\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(rdms_dict['labels']/rdms_dict['labels'].max(), cmap='magma_r')\n",
    "    plt.title(\"RDM of the labels\")\n",
    "    plt.xlabel(\"input index\")\n",
    "    plt.ylabel(\"input index\")\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Dissimilarity', rotation=270, labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we compute and visualize the RDMs for **standard images** for both the standard model and the adversarially trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title For standard model + standard images\n",
    "rdms, rdms_dict = calc_rdms(features_model_imgs)\n",
    "plot_maps(rdms_dict, \"Standard Model with Standard Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title For adversarially trained model + standard images\n",
    "rdms, rdms_dict = calc_rdms(features_advmodel_imgs)\n",
    "plot_maps(rdms_dict, \"Adversarial Trained Model with Standard Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. For the standard clean images, how do the RDMs change across the model layers, how do they compare to the category structure, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: For the standard clean images, how do the RDMs change across the model layers, how do they compare to the category structure, and why?\n",
    "\n",
    "For clean images representing the same digit, their representations in the deeper layers of the network are remarkably similar and align well with the inherent category structure. This manifests as a block diagonal structure. In contrast, this block effect is less pronounced in the earlier layers of the network. The initial layers focus more on capturing general and granular visual features. This progression from generic to more refined feature extraction across layers underscores the hierarchical nature of learning in deep neural networks, where complex representations are built upon the simpler ones extracted at earlier stages.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "RDMs of the standard model and the adversarially trained model are very similar for clean images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we compute and visualize the RDMs for **adversarial images** for both the standard model and the adversarially trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title For standard model + adversarial images\n",
    "rdms, rdms_dict = calc_rdms(features_model_advimgs)\n",
    "plot_maps(rdms_dict, \"Standard Model with Adversarial Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title For adversarially trained model + adversarial images\n",
    "rdms, rdms_dict = calc_rdms(features_advmodel_advimgs)\n",
    "plot_maps(rdms_dict, \"Adversarial Trained Model with Adversarial Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. For adversarial images, how do the RDMs change when comparing representations from the standard model to the adversarially trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: For adversarial images, how do the RDMs change when comparing representations from the standard model to the adversarially trained model?\n",
    "\n",
    "With adversarial images, the within-category representation similarity (the block effect) is disrupted in the deeper layers of the standard model.\n",
    "The adversarially trained model preserves its block-like structure across categories when presented with adversarial images, however.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think!\n",
    "\n",
    "1. How do the RDMs relate to the performances of the standard and the adversarially trained models on clean and adversarial images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: How do the RDMs relate to the performances of the standard and the adversarially trained models on clean and adversarial images?\n",
    "\n",
    "RDMs provide a visual and quantitative way to analyze how a neural network processes and represents different stimuli. By comparing the similarity of responses within the network across various inputs, the RDM can reveal insights into the network's internal representations and, consequently, its ability to generalize.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_creating_comparing_rdms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Interactive Exploration with Widgets\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 30 minutes*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In Sections 1 & 2, we saw that networks generalize better when their test stimuli are \"similar\" to their training stimuli: the network trained only on normal MNIST images generalized poorly to adversarially generated images since they were not part of its training diet, whereas the network trained with adversarial images generalized more effectively to new adversarial images. It turns out that we can sometimes be mathematically precise about how the similarity of training and test examples affects generalization performance. In this interactive example, you will explore the connection between similarity and generalization in the case of linear regression.\n",
    "\n",
    "The manner in which a readout function generalizes to new inputs depends closely on the similarity of the new inputs to training stimuli--in other words, the more similar a test stimulus is to a training stimulus, the more similar the output of the readout will be. But what do we mean by \"similar\"? The relevant way to measure \"similarity\" in each case will depend on the readout function in question. In the case of linear regression, the relevant similarity metric is the kernel (dot product) similarity of the inputs. To see this, you can inspect the matrix form of the least squares solution for regression below:\n",
    "\n",
    "$$y_{pred} = X_{pred} \\left( X_{train}^T X_{train} \\right)^{-1} X_{train}^T y_{train}$$\n",
    "\n",
    "Note that the predicted outputs for the test stimuli are a weighted combination of the outputs for training stimuli, where the weights depend on the inner products of the training and test stimuli. **Thus, the more similar a training stimulus is to a given test stimulus, the more strongly the output for that training stimulus will contribute to the output for that test stimulus**. To give another example, in the case of a radial basis function readout, the relevant notion of similarity would be the Euclidean distance between the inputs.\n",
    "\n",
    "## Interactive exercise\n",
    "\n",
    "In this interactive exercise, you will explore how the similarity between training and test stimuli predicts how the network generalizes. The task is to use linear regression on activations from our MNIST network from above in order to predict how \"left-leaning\" or \"right-leaning\" a particular digit image is. You play the role of the human rater providing the outputs for the training data and will rate the leaning direction of each individual training image from 1-10 using the provided sliders (on the left), where a 1 is leaning strongly left, a 10 is leaning strongly right, and a 5 is perfectly upright, with no leaning. Based on these provided ratings, ridge regression is trained under the hood to predict those ratings using the activations from the selected layer of the neural network, and the resulting regression model is then applied to new images, yielding predicted leaning ratings for these images (red numbers at the bottom).\n",
    "\n",
    "The color-coded matrix shows the dot product similarity between activations for the training images (rows) and test images (columns)--yellow means that the two images are highly similar based on the dot product of the activations for those images, and dark blue means the two images are highly dissimilar. The goal of this exercise is to explore how the predicted leaning direction of the test stimuli is determined by your leaning ratings of the training stimuli, and the similarity of the training and test stimuli.\n",
    "\n",
    "1) Using the matrix, find training and test images that are highly similar and play around with the rating of the training image. How much does the predicted learning rating for the test image change? Try this for a few different pairs.\n",
    "\n",
    "2) Now find a highly dissimilar pair and play with the rating. How much does the predicted leaning of the test image change? Try this for a few different pairs.\n",
    "\n",
    "3) Using the dropdown menu, you can choose a different layer of the neural network to predict the leaning direction of the images, reflecting different stages of processing in the network. Try a few different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Enable the custom widget manager in Colab.\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Execute to see the widget!\n",
    "%matplotlib widget\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, download=True,\n",
    "                       transform=transform)\n",
    "\n",
    "num_train_samples = 10\n",
    "num_test_samples = 5\n",
    "\n",
    "train_data = torch.stack([train_dataset[i][0] for i in range(num_train_samples)])\n",
    "test_data = torch.stack([test_dataset[i][0] for i in range(num_test_samples)])\n",
    "\n",
    "train_patterns = tl.log_forward_pass(model, train_data)\n",
    "test_patterns = tl.log_forward_pass(model, test_data)\n",
    "\n",
    "out = widgets.Output()\n",
    "with plt.ioff():\n",
    "  with out:\n",
    "    fig, ax = plt.subplots(constrained_layout=True, figsize=(6, 9))\n",
    "fig.first = True\n",
    "\n",
    "def update_and_visualize(layer, rating1, rating2, rating3, rating4, rating5,\n",
    "                         rating6, rating7, rating8, rating9, rating10):\n",
    "    X_train = train_patterns[layer].tensor_contents.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    X_test = test_patterns[layer].tensor_contents.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    Y_train = [rating1, rating2, rating3, rating4, rating5,\n",
    "               rating6, rating7, rating8, rating9, rating10]\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    # Ridge Regression\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings('ignore')\n",
    "      model = Ridge(alpha=.99)\n",
    "      model.fit(X_train, Y_train)\n",
    "      Y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Dot product matrix visualization\n",
    "    dot_product_matrix = np.dot(X_train, X_test.T)\n",
    "    padded_matrix = np.zeros((dot_product_matrix.shape[0]+1,\n",
    "                              dot_product_matrix.shape[1]+1))\n",
    "    padded_matrix[0:-1, 1:] = dot_product_matrix\n",
    "    dot_product_matrix = padded_matrix\n",
    "\n",
    "    im = ax.imshow(dot_product_matrix, cmap='viridis')\n",
    "    if len(fig.axes) == 1:\n",
    "      plt.colorbar(im, label='Dot Product Similarity', shrink=.8)\n",
    "    im.set_clim([dot_product_matrix.min(), dot_product_matrix.max()])\n",
    "\n",
    "    dummy_image = np.zeros(train_data[0].shape)\n",
    "    dummy_image = dummy_image + float(train_data[0].max())\n",
    "    dummy_image[-1, 0, -1] = 0\n",
    "\n",
    "    # Set up figure if the first run\n",
    "\n",
    "    if fig.first:\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "      ax.set_ylabel(\"Training Images\", fontsize=15)\n",
    "      ax.xaxis.set_label_position('top')\n",
    "      ax.set_xlabel(\"Test Images\", fontsize=15)\n",
    "      imagebox = OffsetImage(dummy_image.squeeze(), zoom=1.92, cmap='gray')\n",
    "      ab = AnnotationBbox(imagebox, (0, num_train_samples+.05), frameon=False, box_alignment=(0.5, .5))\n",
    "      ax.add_artist(ab)\n",
    "      for i in range(num_train_samples):\n",
    "          imagebox = OffsetImage(train_data[i].squeeze(), zoom=1.9, cmap='gray')\n",
    "          ab = AnnotationBbox(imagebox, (0, i), frameon=False, box_alignment=(.5, 0.5))\n",
    "          ax.add_artist(ab)\n",
    "\n",
    "      for i in range(num_test_samples):\n",
    "          imagebox = OffsetImage(test_data[i].squeeze(), zoom=1.9, cmap='gray')\n",
    "          ab = AnnotationBbox(imagebox, (i+1, num_train_samples), frameon=False, box_alignment=(0.5, .5))\n",
    "          ax.add_artist(ab)\n",
    "      t = ax.text(0, X_train.shape[0], 'X:', va='top', ha='center',\n",
    "                fontsize=15, color='black', backgroundcolor='white', fontweight='bold')\n",
    "      t = ax.text(0, X_train.shape[0]+.65, 'Y:', va='top', ha='center',\n",
    "                fontsize=15, color='black', backgroundcolor='white', fontweight='bold')\n",
    "    fig.first = False\n",
    "\n",
    "\n",
    "    # Annotate with predicted Y_test values\n",
    "\n",
    "    for i in range(num_test_samples):\n",
    "        t = ax.text(i+1, X_train.shape[0]+.65, f'{Y_test_pred[i]:.1f}', va='top', ha='center',\n",
    "                fontsize=15, color='red', backgroundcolor='white', fontweight='bold')\n",
    "\n",
    "\n",
    "w = widgets.interactive(update_and_visualize,\n",
    "         layer = widgets.Dropdown(\n",
    "            options=['conv1', 'conv2', 'dropout1', 'fc1', 'dropout2', 'fc2'],\n",
    "            value='fc2',\n",
    "            description='Layer',\n",
    "            disabled=False,\n",
    "            layout = widgets.Layout(margin='40px 10px 0 0')),\n",
    "         rating1 = widgets.FloatSlider(description='Rating', value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='50px 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating2 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating3 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating4 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating5 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating6 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating7 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating8 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating9 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating10 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'})\n",
    "         )\n",
    "widgets.HBox([w, fig.canvas], layout=widgets.Layout(width='100%', display='flex', align_items='stretch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_interactive_exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "*Estimated timing of tutorial: 40 minutes*\n",
    "\n",
    "In this tutorial, we explored a foundational issue in artificial intelligence: how can we understand how a system generalizes to new inputs that it wasn’t trained on? We did this through the lens of representational similarity analysis (RSA), the characterization of a system based on the pattern of dissimilarities among a set of stimuli. This tutorial has two parts:\n",
    "\n",
    "- In part 1, you used RSA to understand how networks trained in different ways handle adversarial stimuli designed to fool the network.\n",
    "- In part 2, you used an interactive widget to understand how a network’s response to a new stimulus depends on its similarity to training stimuli."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
