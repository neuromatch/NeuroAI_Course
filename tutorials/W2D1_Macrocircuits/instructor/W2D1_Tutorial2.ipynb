{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af0a6c0-3476-4570-834c-4722911ce291",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W2D1_Macrocircuits/student/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W2D1_Macrocircuits/student/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd691bfb-ee4e-46e9-830b-c8c002ca0a01",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Double descent\n",
    "\n",
    "**Week 2, Day 1: Macrocircuits**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Andrew Saxe, Vidya Muthukumar\n",
    "\n",
    "__Content reviewers:__ Max Kanwal, Surya Ganguli, Xaq Pitkow, Hlib Solodzhuk, Patrick Mineault\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837a6fd-faff-4589-af15-8df70f0d141b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 1 hour*\n",
    "\n",
    "In this tutorial, we'll look at the sometimes surprising behavior of large neural networks, which is called double descent. This empirical phenomenon puts the classical understanding of the bias-variance tradeoff in question: in double descent, highly overparametrized models can display good performance. In particular, we will explore the following: \n",
    "\n",
    "- notions of low/high bias/variance;\n",
    "- improvement of test performance with the network's overparameterization, which leads to large model trends;\n",
    "- the conditions under which double descent is observed and what affects its significance;\n",
    "- the conditions under which double descent does not occur.\n",
    "  \n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28796c89-f407-4192-869e-bdc0a1ca5ff4",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "link_id = \"9n4fj\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4342f5-e4d4-4fbf-8d63-d460cc08d95d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06af3e6-6f22-4c1d-a8c6-8474cdf69344",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W2D1_T2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c12e25",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import torch\n",
    "from tqdm.notebook import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448daa5-bade-4610-b286-009b62c0a5f5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c69f4-21be-4aa7-8aea-762934c34807",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def plot_fit(x_train, y_train, x_test, y_test, y_pred = 0):\n",
    "    \"\"\"\n",
    "    Plot train and test data (as well as predicted values for test if given).\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.plot(x_test, y_test,label='Test data')\n",
    "        plt.plot(x_train, y_train,'o',label='Training data')\n",
    "        if y_pred:\n",
    "            plt.plot(x_test, y_pred, label='Prediction')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Input Feature')\n",
    "        plt.ylabel('Target Output')\n",
    "\n",
    "def predict(x_train, y_train, x_test, y_test, n_hidden = 10, reg = 0):\n",
    "    \"\"\"\n",
    "    Plot predicted values for test set.\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "    - reg (float, default = 0): regularization term.\n",
    "    \"\"\"\n",
    "    y_pred = fit_relu_implemented(x_train, y_train, x_test, y_test, n_hidden, reg)[2]\n",
    "\n",
    "    with plt.xkcd():\n",
    "        plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "        plt.plot(x_train, y_train,'o',label='Training data')\n",
    "        plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "        plt.xlabel('Input Feature')\n",
    "        plt.ylabel('Target Output')\n",
    "        plt.title('Number of Hidden Units = {}'.format(n_hidden))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15484d03",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def sin_dataset(sigma = 0):\n",
    "    \"\"\"\n",
    "    Create a sinusoidal dataset and sample 10 points from it for a training dataset.\n",
    "    Training set incorporates Gaussian noise added to the target values of the given standard deviation.\n",
    "\n",
    "    Inputs:\n",
    "    - sigma (float, default = 0): standard deviation of the train noise.\n",
    "\n",
    "    Outputs:\n",
    "    - x_train, y_train, x_test, y_test (tuple of np.ndarray): train and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a sinusoidal dataset and sample 10 points from it for a training dataset\n",
    "    x_test = np.linspace(-np.pi, np.pi, 100)\n",
    "    y_test = np.sin(x_test)\n",
    "\n",
    "    x_train = np.linspace(-np.pi, np.pi, 10)\n",
    "    # y_train = np.sin(x_train) + 0.05 * np.random.normal(size=10)\n",
    "    y_train = np.sin(x_train) + sigma * np.random.normal(size=10)\n",
    "\n",
    "    return x_train.reshape(10,1), y_train.reshape(10,1), x_test.reshape(100,1), y_test.reshape(100,1)\n",
    "\n",
    "def fit_relu_implemented(x_train, y_train, x_test, y_test, n_hidden = 10, reg = 0):\n",
    "    \"\"\"\n",
    "    Fit the second layer of the network by solving via linear regression for the given training data and evaluate the performance.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): the size of the hidden layer.\n",
    "    - reg (float, default = 0): regularization term.\n",
    "\n",
    "    Outputs:\n",
    "    - train_err (float): train error value.\n",
    "    - test_err (float): test error value.\n",
    "    - y_pred (np.ndarray): array of predicted values for test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define network architecture\n",
    "    n_inputs = 1  # Number of input features\n",
    "    n_outputs = 1  # Number of output units\n",
    "\n",
    "    # Layer 1 (Input -> Hidden)\n",
    "    W1 = np.random.normal(0, 1, (n_inputs, n_hidden))  # Random weights\n",
    "    b1 = np.random.uniform(-np.pi, np.pi, size = (1, n_hidden))  # Bias\n",
    "\n",
    "    # Layer 2 (Hidden -> Output)\n",
    "    W2 = np.zeros((n_hidden, n_outputs))  # Initialize weights to zero\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward_prop(X):\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU activation\n",
    "        z2 = a1.dot(W2)\n",
    "        return z2\n",
    "\n",
    "    # Fit second layer weights with linear regression\n",
    "    hidden = np.maximum(0, x_train.dot(W1) + b1)  # Hidden layer activations\n",
    "    if reg == 0:\n",
    "        # Pseudo-inverse solution\n",
    "        hidden_pinv = np.linalg.pinv(hidden)\n",
    "        W2 = hidden_pinv.dot(y_train)\n",
    "    else:\n",
    "        # We use linalg.solve to find the solution to (H'H + reg*I) * W2 = H'y,\n",
    "        # equivalent to W2 = (H'H + reg*I)^(-1) * H'y\n",
    "        W2 = np.linalg.solve(hidden.T @ hidden + reg * np.eye(n_hidden), hidden.T @ y_train)\n",
    "\n",
    "    # Train Error\n",
    "    y_pred = forward_prop(x_train)\n",
    "    train_err = np.mean((y_train-y_pred)**2/2)\n",
    "\n",
    "    # Test Error\n",
    "    y_pred = forward_prop(x_test)\n",
    "    test_err = np.mean((y_test-y_pred)**2/2)\n",
    "\n",
    "    return train_err, test_err, y_pred\n",
    "\n",
    "\n",
    "def fit_relu_init_scale(x_train, y_train, x_test, y_test, init_scale = 0, n_hidden = 10, reg = 0):\n",
    "    \"\"\"\n",
    "    Fit the second layer of the network by solving for linear regression with pseudo-inverse for the given training data and evaluate the performance on test one.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - init_scale (float, default = 0): initial standard deviation for weights in the second layer.\n",
    "    - n_hidden (int, default = 10): the size of the hidden layer.\n",
    "    - reg (float, default = 0): regularization term.\n",
    "\n",
    "    Outputs:\n",
    "    - train_err (float): train error value.\n",
    "    - test_err (float): test error value.\n",
    "    - y_pred (np.ndarray): array of predicted values for test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define network architecture\n",
    "    n_inputs = 1  # Number of input features\n",
    "    n_outputs = 1  # Number of output units\n",
    "\n",
    "    # Layer 1 (Input -> Hidden)\n",
    "    W1 = np.random.normal(0,1, (n_inputs, n_hidden))  # Random weights\n",
    "    b1 = np.random.uniform(-np.pi, np.pi, size=(1, n_hidden))  # Bias\n",
    "\n",
    "    # Layer 2 (Hidden -> Output)\n",
    "    W2 = np.random.normal(0,init_scale,(n_hidden, n_outputs))  # Initialize weights to zero\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward_prop(X):\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU activation\n",
    "        z2 = a1.dot(W2)\n",
    "        return z2\n",
    "\n",
    "    # Fit second layer weights with linear regression\n",
    "    hidden = np.maximum(0, x_train.dot(W1) + b1)  # Hidden layer activations\n",
    "    if reg == 0:\n",
    "        hidden_pinv = np.linalg.pinv(hidden)\n",
    "        W2 = hidden_pinv.dot(y_train) + (np.eye(n_hidden) - hidden_pinv @ hidden) @ W2   # Pseudo-inverse solution plus component in data nullspace\n",
    "    else:\n",
    "        hidden_pinv = np.dot(np.linalg.pinv(np.dot(hidden.T, hidden) + reg*np.eye(n_hidden)), hidden.T)\n",
    "        W2 = hidden_pinv.dot(y_train) + (np.eye(n_hidden) - hidden_pinv @ hidden) @ W2   # Pseudo-inverse solution plus component in data nullspace\n",
    "\n",
    "    # Train Error\n",
    "    y_pred = forward_prop(x_train)\n",
    "    train_err = np.mean((y_train-y_pred)**2/2)\n",
    "\n",
    "    # Test Error\n",
    "    y_pred = forward_prop(x_test)\n",
    "    test_err = np.mean((y_test-y_pred)**2/2)\n",
    "\n",
    "    return train_err, test_err, y_pred\n",
    "\n",
    "def sweep_test_init_scale(x_train, y_train, x_test, y_test, init_scale = 0, n_hidden = 10, n_reps = 20):\n",
    "    \"\"\"\n",
    "    Calculate the mean test error for fitting the second layer of the network for a defined number of repetitions.\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - init_scale (float, default = 0): initial standard deviation for weights in the second layer.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "\n",
    "    Outputs:\n",
    "    - (float): mean error for test data.\n",
    "    \"\"\"\n",
    "    return np.mean(np.array([fit_relu_init_scale(x_train, y_train, x_test, y_test, init_scale=init_scale, n_hidden=n_hidden)[1] for i in range(n_reps)]))\n",
    "\n",
    "def sweep_train(x_train, y_train, x_test, y_test, n_hidden = 10, n_reps = 20):\n",
    "    \"\"\"\n",
    "    Calculate the mean train error for fitting the second layer of the network for a defined number of repetitions.\n",
    "    Notice that `init_scale` is always set to be 0 in this case.\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "\n",
    "    Outputs:\n",
    "    - (float): mean error for train data.\n",
    "    \"\"\"\n",
    "    return np.mean(np.array([fit_relu(x_train, y_train, x_test, y_test, n_hidden=n_hidden)[0] for i in range(n_reps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38960eb6-2e03-4c36-b73e-abaac92ab5f9",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19288124-5e02-43e1-bef5-649d5b65750f",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Introduction\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'BAQYcMpqtYU'), ('Bilibili', 'BV1Q4421S7St')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43176a-d80f-4106-8df1-c33c2cee6be1",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6830bc7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Overfitting in overparameterized models\n",
    "\n",
    "In this section we will observe the classical behaviour of overparametrized networks - overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2a90f-1af4-445a-85fb-b4ebb7d56bae",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Overfitting in Overparameterized Models\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'z3n1fT76gHs'), ('Bilibili', 'BV1gi421i7DP')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fa0e6-2848-490f-891b-82f126867170",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_overfitting_in_overparameterized_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3553a90",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: Learning with a simple neural network\n",
    "\n",
    "We start by generating a simple sinusoidal dataset.\n",
    "\n",
    "This dataset contains 100 datapoints. We've selected a subset of 10 points for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2198760-a59c-4fab-be27-555590220c63",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Sample and plot sinusoidal dataset\n",
    "\n",
    "set_seed(42)\n",
    "x_train, y_train, x_test, y_test = sin_dataset()\n",
    "\n",
    "plot_fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e91cf-db0a-4992-8bc3-96395bb1b801",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll learn this task with a simple neural network: a one-hidden-layer ReLU network, where the first layer is fixed and random.\n",
    "\n",
    "That is, the network computes its hidden activity as $h = ReLU(W_1x + b)$, and its output as $\\hat y = W_2h$. \n",
    "\n",
    "The input $x\\in R$ is a scalar. There are $N_h$ hidden units, and the output $\\hat y\\in R$ is a scalar.\n",
    "\n",
    "We will initialize $W_1$ with i.i.d. random Gaussian values with a variance of one and $b$ with values drawn i.i.d. uniformly between $-\\pi$ and $\\pi$. Finally, we will initialize the weights $W_2$ to zero.\n",
    "\n",
    "We only train $W_2$, leaving $W_1$ and $b$ fixed. We can train $W_2$ to minimize the mean squared error between the training labels $y$ and the network's output on those datapoints. \n",
    "\n",
    "Usually, we would train this network using gradient descent, but here, we've taken advantage of the fixed first layer to compute the solution in closed form using linear regression. This is much faster than gradient descent, allowing us to us rapidly train many networks.\n",
    "\n",
    "<details open>\n",
    "<summary> Closed-form solution derivation (optional)</summary>\n",
    "<br>\n",
    "\n",
    "To find $W_2$, we will rewrite the problem in matrix form. Let:\n",
    "- $X$ be the matrix of input data (shape $ n \\times 1 $)\n",
    "- $H$ be the matrix of hidden activations after ReLU (shape $ n \\times N_h $)\n",
    "- $y$ be the vector of target outputs (shape $n \\times 1$)\n",
    "\n",
    "$W_1$, $W_2$ and $b$ are of shape $N_h \\times 1$.\n",
    "\n",
    "Compute the hidden activations:\n",
    "\n",
    "$$\n",
    "H = \\text{ReLU}(X W_1^\\top + b)\n",
    "$$\n",
    "\n",
    "The predicted outputs can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = H W_2\n",
    "$$\n",
    "\n",
    "We aim to minimize the mean squared error (MSE) between the predicted output $\\hat{y}$ and the true output $y$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{MSE} = (H W_2 - y)^2\n",
    "$$\n",
    "\n",
    "By taking the derivative of this expression by $W_2$, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2} = 0 = 2H^\\top HW_2 - 2H^\\top y\n",
    "$$\n",
    "\n",
    "Meaning that the optimal solution $W_2$ is:\n",
    "\n",
    "$$\n",
    "W_2 = H^+ y\n",
    "$$\n",
    "\n",
    "Here, $H^+$ is the [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "Below is the skeleton code for this network. Your task is to implement its weight initialization and forward pass function. In the cell below, you can test your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a367ca7",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "def fit_relu(x_train, y_train, x_test, y_test, n_hidden = 10, reg = 0):\n",
    "    \"\"\"\n",
    "    Fit the second layer of the network by solving via linear regression for the given training data and evaluate the performance.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): the size of the hidden layer.\n",
    "    - reg (float, default = 0): regularization term.\n",
    "\n",
    "    Outputs:\n",
    "    - train_err (float): train error value.\n",
    "    - test_err (float): test error value.\n",
    "    - y_pred (np.ndarray): array of predicted values for test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define network architecture\n",
    "    n_inputs = 1  # Number of input features\n",
    "    n_outputs = 1  # Number of output units\n",
    "\n",
    "    ###################################################################\n",
    "    ## Fill out the following, then remove\n",
    "    raise NotImplementedError(\"Student exercise: complete initialization of layers as well as forward propagation for the network (follow the description in the text above).\")\n",
    "    ###################################################################\n",
    "\n",
    "    # Layer 1 (Input -> Hidden)\n",
    "    W1 = np.random.normal(0, ..., (n_inputs, ...))  # Random weights\n",
    "    b1 = np.random.uniform(-np.pi, ..., size = (1, n_hidden))  # Bias\n",
    "\n",
    "    # Layer 2 (Hidden -> Output)\n",
    "    W2 = np.zeros((..., ...))  # Initialize weights to zero\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward_prop(X):\n",
    "        z1 = X.dot(...) + ...\n",
    "        a1 = np.maximum(0, ...)  # ReLU activation\n",
    "        z2 = a1.dot(...)\n",
    "        return z2\n",
    "\n",
    "    # Fit second layer weights with linear regression\n",
    "    hidden = np.maximum(0, x_train.dot(W1) + b1)  # Hidden layer activations\n",
    "    if reg == 0:\n",
    "        # Pseudo-inverse solution\n",
    "        hidden_pinv = np.linalg.pinv(hidden)\n",
    "        W2 = hidden_pinv.dot(y_train)\n",
    "    else:\n",
    "        # We use linalg.solve to find the solution to (H'H + reg*I) * W2 = H'y,\n",
    "        # equivalent to W2 = (H'H + reg*I)^(-1) * H'y\n",
    "        W2 = np.linalg.solve(hidden.T @ hidden + reg * np.eye(n_hidden), hidden.T @ y_train)\n",
    "\n",
    "    # Train Error\n",
    "    y_pred = forward_prop(x_train)\n",
    "    train_err = np.mean((y_train-y_pred)**2/2)\n",
    "\n",
    "    # Test Error\n",
    "    y_pred = forward_prop(x_test)\n",
    "    test_err = np.mean((y_test-y_pred)**2/2)\n",
    "\n",
    "    return train_err, test_err, y_pred\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae96a50-9b12-4b17-9416-ef1e2fb18e54",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "def fit_relu(x_train, y_train, x_test, y_test, n_hidden = 10, reg = 0):\n",
    "    \"\"\"\n",
    "    Fit the second layer of the network by solving via linear regression for the given training data and evaluate the performance.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): the size of the hidden layer.\n",
    "    - reg (float, default = 0): regularization term.\n",
    "\n",
    "    Outputs:\n",
    "    - train_err (float): train error value.\n",
    "    - test_err (float): test error value.\n",
    "    - y_pred (np.ndarray): array of predicted values for test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define network architecture\n",
    "    n_inputs = 1  # Number of input features\n",
    "    n_outputs = 1  # Number of output units\n",
    "\n",
    "    # Layer 1 (Input -> Hidden)\n",
    "    W1 = np.random.normal(0, 1, (n_inputs, n_hidden))  # Random weights\n",
    "    b1 = np.random.uniform(-np.pi, np.pi, size = (1, n_hidden))  # Bias\n",
    "\n",
    "    # Layer 2 (Hidden -> Output)\n",
    "    W2 = np.zeros((n_hidden, n_outputs))  # Initialize weights to zero\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward_prop(X):\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU activation\n",
    "        z2 = a1.dot(W2)\n",
    "        return z2\n",
    "\n",
    "    # Fit second layer weights with linear regression\n",
    "    hidden = np.maximum(0, x_train.dot(W1) + b1)  # Hidden layer activations\n",
    "    if reg == 0:\n",
    "        # Pseudo-inverse solution\n",
    "        hidden_pinv = np.linalg.pinv(hidden)\n",
    "        W2 = hidden_pinv.dot(y_train)\n",
    "    else:\n",
    "        # We use linalg.solve to find the solution to (H'H + reg*I) * W2 = H'y,\n",
    "        # equivalent to W2 = (H'H + reg*I)^(-1) * H'y\n",
    "        W2 = np.linalg.solve(hidden.T @ hidden + reg * np.eye(n_hidden), hidden.T @ y_train)\n",
    "\n",
    "    # Train Error\n",
    "    y_pred = forward_prop(x_train)\n",
    "    train_err = np.mean((y_train-y_pred)**2/2)\n",
    "\n",
    "    # Test Error\n",
    "    y_pred = forward_prop(x_test)\n",
    "    test_err = np.mean((y_test-y_pred)**2/2)\n",
    "\n",
    "    return train_err, test_err, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ed2cd-f3d4-47ed-82c6-9765113847a8",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_learning_with_simple_neural_network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054304e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 2: The bias-variance tradeoff\n",
    "\n",
    "With the network implemented, we now investigate how the size of the network (the number of hidden units it has, $N_h$) relates to its ability to generalize. \n",
    "\n",
    "Ultimately, the true measure of a learning system is how well it performs on novel inputs, that is, its ability to generalize. The classical story of how model size relates to generalization is the bias-variance tradeoff.\n",
    "\n",
    "To start, complete the code below to train several small networks with just two hidden neurons and plot their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d0e55",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "set_seed(42)\n",
    "\n",
    "n_hid = 2\n",
    "\n",
    "n_reps = 10 # Number of networks to train\n",
    "\n",
    "def plot_predictions(n_hid, n_reps):\n",
    "    \"\"\"\n",
    "    Generate train and test data for `n_reps` times, fit it for a network with hidden size `n_hid`, and plot prediction values.\n",
    "\n",
    "    Inputs:\n",
    "    - n_hid (int): size of hidden layer.\n",
    "    - n_reps (int): number of data regenerations.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "        plt.plot(x_train, y_train,'o',label='Training data')\n",
    "\n",
    "        train_err, test_err, y_pred = fit_relu(x_train, y_train, x_test, y_test, n_hidden=n_hid)\n",
    "        plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "\n",
    "        for rep in range(n_reps-1):\n",
    "            ###################################################################\n",
    "            ## Fill out the following then remove\n",
    "            raise NotImplementedError(\"Student exercise: complete generation of new train / test data and visualize it.\")\n",
    "            ###################################################################\n",
    "            train_err, test_err, y_pred = ...\n",
    "            plt.plot(..., ..., color='g', alpha=.5, label='_')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Input Feature')\n",
    "        plt.ylabel('Target Output')\n",
    "        plt.title('Number of Hidden Units = {}'.format(n_hid))\n",
    "        plt.show()\n",
    "\n",
    "plot_predictions(n_hid, n_reps)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ecfe6-a49a-44f6-8a91-7aaa71db8e55",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "\n",
    "n_hid = 2\n",
    "\n",
    "n_reps = 10 # Number of networks to train\n",
    "\n",
    "def plot_predictions(n_hid, n_reps):\n",
    "    \"\"\"\n",
    "    Generate train and test data for `n_reps` times, fit it for a network with hidden size `n_hid`, and plot prediction values.\n",
    "\n",
    "    Inputs:\n",
    "    - n_hid (int): size of hidden layer.\n",
    "    - n_reps (int): number of data regenerations.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "        plt.plot(x_train, y_train,'o',label='Training data')\n",
    "\n",
    "        train_err, test_err, y_pred = fit_relu(x_train, y_train, x_test, y_test, n_hidden=n_hid)\n",
    "        plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "\n",
    "        for rep in range(n_reps-1):\n",
    "            train_err, test_err, y_pred = fit_relu(x_train, y_train, x_test, y_test, n_hidden=n_hid)\n",
    "            plt.plot(x_test, y_pred, color='g', alpha=.5, label='_')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Input Feature')\n",
    "        plt.ylabel('Target Output')\n",
    "        plt.title('Number of Hidden Units = {}'.format(n_hid))\n",
    "        plt.show()\n",
    "\n",
    "plot_predictions(n_hid, n_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc554ad",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With just two hidden units, the model cannot fit the training data, nor can it do well on the test data. A network of this size has a high bias.\n",
    "\n",
    "Now, let's train a network with five hidden units.\n",
    "\n",
    "### Coding Exercise 2 Discussion\n",
    "\n",
    "1. Do you expect the network to fit the data better with five hidden units than with two units? Check your intuition by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b974f",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the performance on 5 hidden units\n",
    "set_seed(42)\n",
    "\n",
    "n_hid = 5\n",
    "n_reps = 10\n",
    "\n",
    "plot_predictions(n_hid, n_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6b6f0",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With five hidden units, the model can do a better job of fitting the training data, and also follows the test data more closely - though still with errors.\n",
    "\n",
    "Next let's try 10 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd9a94",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the performance on 10 hidden units\n",
    "set_seed(42)\n",
    "\n",
    "n_hid = 10\n",
    "n_reps = 10\n",
    "\n",
    "plot_predictions(n_hid, n_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a60d5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With 10 hidden units, the network often fits every training datapoint, but generalizes poorly--sometimes catastrophically so. We say that this size network has high variance. Intuitively, it is so complex that it can fit the training data perfectly, but this same complexity means it can take many different shapes in between datapoints.\n",
    "\n",
    "We have just traced out the bias-variance tradeoff: the models with 2 hidden units had high bias, while the models with 10 hidden units had high variance. The models with 5 hidden units struck a balance--they were complex enough to achieve relatively low error on the training datapoints, but simple enough to be well constrained by the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18751454-e61d-41d9-854d-8ce60023d512",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_bias_variance_tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbf633-30cb-48b4-9b45-e9144f73c5a3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: The modern regime\n",
    "\n",
    "Estimated timing to here from start of tutorial: 20 minutes\n",
    "\n",
    "In this section, we will go beyond ten hidden units, which might sound unreasonable, but it will result in an unexpected plot twist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7e0cf-91a0-4ddc-a8a6-e48b6c18993c",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Modern Regime\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'uYqWdoC0TrM'), ('Bilibili', 'BV13r421w7gS')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a9a2f-8738-4869-a3f4-1e1f716e93ba",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_modern_regime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da7c78-6314-4d40-9b29-41867e5c691e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We just saw that a network with 10 hidden units trained on 10 training datapoints could fail to generalize. If we add even more hidden units, it seems unlikely that the network could perform well. How could hundreds of weights be correctly constrained with just these ten datapoints?\n",
    "\n",
    "But let's try it. Throw caution to the wind and train a network with 500 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19980d47",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the performance on 500 hidden units\n",
    "set_seed(42)\n",
    "\n",
    "n_hid = 500\n",
    "n_reps = 10\n",
    "\n",
    "plot_predictions(n_hid, n_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d90bd7-de67-417a-a0f1-1daaac38cf7a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 3: Observing double descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82826383",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Remarkably, this very large network fits the training datapoints and generalizes well.\n",
    "\n",
    "This network has fifty times as many parameters as datapoints. How can this be?\n",
    "\n",
    "We've tested four different network sizes and seen the qualitative behavior of the predictions. Now, let's systematically compute the average test error for different network sizes.\n",
    "\n",
    "For each network size in the array below, train 100 networks and plot their mean test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a836bc-561a-42fa-8731-25cb911804b1",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "set_seed(42)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 20))).astype(int)\n",
    "\n",
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete calculation of mean test error for different sizes of hidden units defined above.\")\n",
    "###################################################################\n",
    "\n",
    "def sweep_test(x_train, y_train, x_test, y_test, n_hidden = 10, n_reps = 100, reg = 0.0):\n",
    "    \"\"\"\n",
    "    Calculate the mean test error for fitting the second layer of the network for a defined number of repetitions.\n",
    "    Notice that `init_scale` is always set to 0 in this case.\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "    - n_reps (int, default = 100): number of resamples for data.\n",
    "    - reg (float, default = 0): regularization constant.\n",
    "\n",
    "    Outputs:\n",
    "    - (float): mean error for train data.\n",
    "    \"\"\"\n",
    "    return np.mean(...)\n",
    "\n",
    "test_errs = [sweep_test(x_train, y_train, x_test, y_test, n_hidden=..., n_reps=100, reg = 0.0) for n_hid in ...]\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.loglog(n_hids,test_errs,'o-',label='Test')\n",
    "    plt.xlabel('Number of Hidden Units')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af8c6a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 20))).astype(int)\n",
    "\n",
    "def sweep_test(x_train, y_train, x_test, y_test, n_hidden = 10, n_reps = 100, reg = 0.0):\n",
    "    \"\"\"\n",
    "    Calculate the mean test error for fitting the second layer of the network for a defined number of repetitions.\n",
    "    Notice that `init_scale` is always set to 0 in this case.\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "    - n_reps (int, default = 100): number of resamples for data.\n",
    "    - reg (float, default = 0): regularization constant.\n",
    "\n",
    "    Outputs:\n",
    "    - (float): mean error for train data.\n",
    "    \"\"\"\n",
    "    return np.mean(np.array([fit_relu(x_train, y_train, x_test, y_test, n_hidden=n_hidden, reg = reg)[1] for i in range(n_reps)]))\n",
    "\n",
    "test_errs = [sweep_test(x_train, y_train, x_test, y_test, n_hidden=n_hid, n_reps=100, reg = 0.0) for n_hid in n_hids]\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.loglog(n_hids,test_errs,'o-',label='Test')\n",
    "    plt.xlabel('Number of Hidden Units')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e73af4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This curve is an example of the **double descent phenomenon**: between 1 hidden unit and 10 hidden units, we see the bias-variance tradeoff (containing the first descent). The test error reaches its peak at 10 hidden units. Then it descends again (hence double descent), eventually outperforming the smaller models.\n",
    "\n",
    "Hence, in this scenario, larger models perform better--even when they contain many more parameters than datapoints.\n",
    "\n",
    "The peak (worst generalization) is at an intermediate model size when the number of hidden units is equal to the number of examples in this case. More generally, it turns out the peak occurs when the model first becomes complex enough to reach zero training error. This point is known as the interpolation point.\n",
    "\n",
    "The trend for deep learning models to grow in size is in part due to this phenomenon of double descent. Let's now see its limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480bec8-40a3-41c5-b159-be41d1cffa9a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interactive Demo 1: Interpolation point & predictions\n",
    "\n",
    "In this interactive demo, you can move the slider for the number of hidden units in the network to be trained on and observe one representative trial of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888a891-b47e-47bd-87eb-5f9729366a9d",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Execute this cell to observe interactive plot\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "@widgets.interact\n",
    "def interactive_predict(n_hid = widgets.IntSlider(description=\"Hidden Units\", min=0, max=300, step=1, value=5)):\n",
    "    predict(x_train, y_train, x_test, y_test, n_hidden = n_hid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0ea29-4691-4018-a0b7-46daf0625299",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The trend for deep learning models to grow in size is in part due to the phenomenon of double descent. Let's now see its limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fad9c-b469-4864-8316-82d0b1bf660a",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_observing_double_descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e59b8b-1cc8-4634-81ca-ac0b07f61b31",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "---\n",
    "# Section 3: Double descent, noise & regularization\n",
    "\n",
    "Estimated timing to here from start of tutorial: 35 minutes\n",
    "\n",
    "In this section, we are going to explore the effect of noise and regularization on double descent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321e9bb-ece8-47b7-b87e-0d38c04ea024",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Noise & Regularization Effects\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'lCeJmCDcSc0'), ('Bilibili', 'BV1ms421u7Zc')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6e654-a9f1-404b-88cc-e81935220ce6",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_noise_regularization_effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24b97f-adee-4e7b-9b34-c40495cb6607",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 4: Noise & regularization impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20493de1-ad04-4e40-aed5-5c40f18f53c4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So far, our training datapoints have been noiseless. Intuitively, a noisy training dataset might hurt the ability of complex models to generalize. In this section, we are going to explore the effect of noise on double descent behavior.\n",
    "\n",
    "Let's test this. Add i.i.d. Gaussian noise of different standard deviations to the training labels, and plot the resulting double descent curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e3caa-09ff-46a0-842e-2855106db53e",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "set_seed(42)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 10))).astype(int)\n",
    "\n",
    "std_devs = np.linspace(0, 1.0, 3)\n",
    "\n",
    "def plot_error(x_train, y_train, x_test, y_test, std_devs, n_hids, n_hidden = 10, n_reps = 100, reg = 0.0):\n",
    "    \"\"\"\n",
    "    Plot mean test error for distinct values of noise added to train dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - std_devs (np.ndarray): different standard deviation values for noise.\n",
    "    - n_hids (np.ndarray): different values for hidden layer size.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "    - n_reps (int, default = 100): number of resamples for data.\n",
    "    - reg (float, default = 0): regularization constant.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        for sd in tqdm(std_devs):\n",
    "            ###################################################################\n",
    "            ## Fill out the following then remove\n",
    "            raise NotImplementedError(\"Student exercise: complete calculation of mean test error for different noise values.\")\n",
    "            ###################################################################\n",
    "            test_errs = [sweep_test(x_train, y_train + ..., x_test, y_test, n_hidden = n_hid, n_reps = n_reps, reg = reg * (1 + sd)) for n_hid in n_hids]\n",
    "            plt.loglog(n_hids,test_errs,'o-',label=\"std={}\".format(sd))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Number of Hidden Units')\n",
    "        plt.ylabel('Test Error')\n",
    "        plt.show()\n",
    "\n",
    "plot_error(x_train, y_train, x_test, y_test, std_devs, n_hids)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b1cd7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 10))).astype(int)\n",
    "\n",
    "std_devs = np.linspace(0, 1.0, 3)\n",
    "\n",
    "def plot_error(x_train, y_train, x_test, y_test, std_devs, n_hids, n_hidden = 10, n_reps = 100, reg = 0.0):\n",
    "    \"\"\"\n",
    "    Plot mean test error for distinct values of noise added to train dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - x_train (np.ndarray): train input data.\n",
    "    - y_train (np.ndarray): train target data.\n",
    "    - x_test (np.ndarray): test input data.\n",
    "    - y_test (np.ndarray): test target data.\n",
    "    - std_devs (np.ndarray): different standard deviation values for noise.\n",
    "    - n_hids (np.ndarray): different values for hidden layer size.\n",
    "    - n_hidden (int, default = 10): size of hidden layer.\n",
    "    - n_reps (int, default = 100): number of resamples for data.\n",
    "    - reg (float, default = 0): regularization constant.\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "        for sd in tqdm(std_devs):\n",
    "            test_errs = [sweep_test(x_train, y_train + np.random.normal(0,sd,y_train.shape), x_test, y_test, n_hidden = n_hid, n_reps = n_reps, reg = reg * (1 + sd)) for n_hid in n_hids]\n",
    "            plt.loglog(n_hids,test_errs,'o-',label=\"std={}\".format(sd))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Number of Hidden Units')\n",
    "        plt.ylabel('Test Error')\n",
    "        plt.show()\n",
    "\n",
    "plot_error(x_train, y_train, x_test, y_test, std_devs, n_hids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510874f0-3e57-42ea-a2ea-7bcd47a41467",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Though we are still able to observe the double descent effect, its strength is reduced with the increase in noise level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05678383",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Thus far, we have examined the impact of number of parameters without any explicit regularization (in which case we saw that all models with > 10 parameters fit all training data including noise). What happens if we add in explicit regularization to prevent overfitting? Let's take a look. We'll add L2 regularization to the weights of the network, also known as weight decay or Tikhonov regularization. The closed form solution for $\\mathbf{W}_2$ with L2 regularization is:\n",
    "\n",
    "$$\\mathbf{W}_2 = (\\mathbf{H}^\\top \\mathbf{H} + \\lambda \\mathbf{I})^{-1} \\mathbf{H}^\\top \\mathbf{y}$$\n",
    "\n",
    "where $\\lambda$ is the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018db9f",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observe the error plots with regularization term included\n",
    "set_seed(42)\n",
    "\n",
    "plot_error(x_train, y_train, x_test, y_test, std_devs, n_hids, reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174f801",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We observe that the \"peak\" disappears, and the test error roughly monotonically decreases, although it is generally higher for higher noise levels in the training data.\n",
    "\n",
    "The word *regularization* is commonly used in statistics/ML parlance in two different contexts to ensure the good generalization of overparameterized models:\n",
    "\n",
    "- The first context, which is emphasized throughout the tutorial, is explicit regularization which means that the model is not trained to completion (zero training error) in order to avoid overfitting of noise. Without explicit regularization, we observe the double descent behavior – i.e. catastrophic overfitting when the number of model parameters is too close to the number of training examples – but also a vast reduction in this overfitting effect as we heavily overparameterize the model. With explicit regularization (when tuned correctly), the double descent behavior disappears because we no longer run the risk of overfitting to noise at all.\n",
    "- The second context is the one of inductive bias – overparameterized models, when trained with popular optimization algorithms like gradient descent, tend to converge to a particularly “simple” solution that perfectly fits the data. By “simple”, we usually mean that the size of the parameters (in terms of magnitude) is very small. This inductive bias is a big reason why double descent occurs as well, in particular, the benefit of overparameterization in reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebecb9a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When our training labels contain noise, are larger models still always better than smaller ones? You may have to run the cell a few times because even with 100 repetitions, these estimates can have high variance.\n",
    "\n",
    "Edit the code below to visualize a 500-hidden unit network's predictions for the case where the noise standard deviation is 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8c1d4-c518-4743-8c58-a5ecd06a8276",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "set_seed(42)\n",
    "\n",
    "std_dev = .2\n",
    "\n",
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete noise calculation and add it in appropriate places.\")\n",
    "###################################################################\n",
    "\n",
    "noise = np.random.normal(..., ..., y_train.shape)\n",
    "\n",
    "n_hid = 500\n",
    "n_reps = 10\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "    plt.plot(x_train, y_train + ...,'o',label='Training data')\n",
    "    train_err, test_err, y_pred = fit_relu(x_train, y_train + ..., x_test, y_test, n_hidden = n_hid)\n",
    "    plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Input Feature')\n",
    "    plt.ylabel('Target Output')\n",
    "    plt.title('Number of Hidden Units = {}'.format(n_hid))\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2643c1",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "\n",
    "std_dev = .2\n",
    "\n",
    "noise = np.random.normal(0, std_dev, y_train.shape)\n",
    "\n",
    "n_hid = 500\n",
    "n_reps = 10\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "    plt.plot(x_train, y_train + noise,'o',label='Training data')\n",
    "    train_err, test_err, y_pred = fit_relu(x_train, y_train + noise, x_test, y_test, n_hidden = n_hid)\n",
    "    plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Input Feature')\n",
    "    plt.ylabel('Target Output')\n",
    "    plt.title('Number of Hidden Units = {}'.format(n_hid))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd7bc2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The network smoothly interpolates between the training datapoints. Even when noisy, these can still somewhat track the test data. Depending on the noise level, though, a smaller and more constrained model can be better.\n",
    "\n",
    "From this, we might expect that large models will work particularly well for datasets with little label noise. Many real-world datasets fit this requirement: image classification datasets strive to have accurate labels for all datapoints, for instance. Other datasets may not. For instance, predicting DSM-V diagnoses from structural MRI data is a noisy task, as the diagnoses themselves are noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b285d-5a6b-4a14-9eb6-88a67db91bdd",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_noise_regularization_impact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14395dc-1e14-4ae6-b84b-5dc19ba997f6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Double descent and initialization\n",
    "\n",
    "Estimated timing to here from start of tutorial: 50 minutes\n",
    "\n",
    "So far, we have considered one important aspect of architecture, namely the size or number of hidden neurons. A second critical aspect is initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ea5df-a8d3-45c8-aa60-4e8f2fc130ca",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Initialization Scale\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'dVhk5orM9_c'), ('Bilibili', 'BV1i1421C7Z6')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272fbe2-04a9-44af-90a5-44f0459e5c76",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_initialization_scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2160cbd-d16f-48bc-99ab-085d5bebb3af",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 5: Initialization scale impact\n",
    "\n",
    "Instead of initializing the second layer weights $W_2$ to zero, we'll now initialize them with i.i.d. Gaussian elements with a specified standard deviation. \n",
    "\n",
    "### Coding Exercise 5 Discussion\n",
    "\n",
    "1. How does this change the double descent curve?\n",
    "   \n",
    "Notice that executing this cell will take around 3 minutes. Thus, we invite you to guess / predict what you are going to see while the cell is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53888d5b-9d0b-48d5-80bf-c3b60ea9813d",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "set_seed(42)\n",
    "init_scales = np.linspace(0, 3, 5)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 10))).astype(int)\n",
    "\n",
    "###################################################################\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: complete initial scale parameter assignment.\")\n",
    "###################################################################\n",
    "with plt.xkcd():\n",
    "    for sd in tqdm(init_scales):\n",
    "        test_errs = [sweep_test_init_scale(x_train, y_train, x_test, y_test, init_scale = ..., n_hidden=n_hid, n_reps=100) for n_hid in n_hids]\n",
    "        plt.loglog(n_hids,test_errs,'o-',label=\"Init Scale={}\".format(sd))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Number of Hidden Units')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193a40a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "set_seed(42)\n",
    "init_scales = np.linspace(0, 3, 5)\n",
    "\n",
    "n_hids = np.unique(np.round(np.logspace(0, 3, 10))).astype(int)\n",
    "\n",
    "with plt.xkcd():\n",
    "    for sd in tqdm(init_scales):\n",
    "        test_errs = [sweep_test_init_scale(x_train, y_train, x_test, y_test, init_scale = sd, n_hidden=n_hid, n_reps=100) for n_hid in n_hids]\n",
    "        plt.loglog(n_hids,test_errs,'o-',label=\"Init Scale={}\".format(sd))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Number of Hidden Units')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2627a3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We see that for overparametrized models, where the number of parameters is larger than the number of training examples, the initialization scale strongly impacts the test error. The good performance of these large models thus depends on our choice of initializing $W_2$ equal to zero.\n",
    "\n",
    "Intuitively, this is because directions of weight space in which we have no training data are not changed by gradient descent, so poor initialization can continue to affect the model even after training. Large initializations implement random functions that generalize poorly.\n",
    "\n",
    "Let's see what the predictions of a large-variance-initialization network with 500 hidden neurons look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290be6",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Execute the cell to observe the plot\n",
    "init_scale = 1\n",
    "\n",
    "n_hid = 500\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(x_test, y_test,linewidth=4,label='Test data')\n",
    "    plt.plot(x_train, y_train,'o',label='Training data')\n",
    "    train_err, test_err, y_pred = fit_relu_init_scale(x_train, y_train, x_test, y_test, init_scale=init_scale, n_hidden=n_hid)\n",
    "    plt.plot(x_test, y_pred, color='g', label='Prediction')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Input Feature')\n",
    "    plt.ylabel('Target Output')\n",
    "    plt.title('Number of Hidden Units = {}'.format(n_hid))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d1dae",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The network perfectly fits every training datapoint, but now connects them with more complex functions.\n",
    "\n",
    "Intuitively, in a large network, there are many ways to achieve zero training error. Gradient descent tends to find solutions that are near the initialization. So simple initializations (here, small variance) yield less complex models that can generalize well.\n",
    "\n",
    "Therefore, proper initialization is critical for good generalization in large networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da546cf-bd9e-4818-8321-156d118795fa",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There is one more mathematical trick to highlight, used in `fit_relu_init_scale` to find the appropriate weights. This passage is optional for those who would like to dive into implementation specifics.\n",
    "<details open>\n",
    "<summary> Solution in case of non-zero weights initialization</summary>\n",
    "    \n",
    "The tutorial is trying to study what happens when you train the network with gradient descent for a very long time. However, if we actually trained these networks that way, it'd indeed take a very long time. So, we have picked a case where we know where training will end up and can calculate this endpoint in a different way. This case is basically a linear regression from the hidden layer activity. We have covered mathematical derivation at the very beginning of the tutorial.\n",
    "\n",
    "Still, there's one case that is a bit trickier: what if the network is overparametrized? That is, what if we have fewer data samples than weights? Then, there are many possible solutions that can result in zero training errors. Starting from some particular initialization $w^0$, which one of these solutions will gradient descent end at? It turns out that the gradient descent dynamics only change the weights in the subspace spanned by the input training data. The component of the weights that start in the orthogonal complement of that subspace, the data null space, i.e., the subspace in which we have no data at all, doesn't change at all during gradient descent learning. So, whatever initial values the weights take in that subspace will persist throughout training. To see this, note that the gradient update is $\\Delta w \\propto \\text{error} H^T$, so every update is in the direction of a training example. Therefore, to calculate $w^{\\text{gd}}$, the weights that gradient descent reaches convergence, we need to add the component of the weight initialization that lies in this data nullspace to the minimum norm linear regression solution given by the earlier equation for $w$^. That is, $w^{gd} = w^ + (I-\\text{pinv}(H)H)w^0$, where $w^0$ is the initial weight vector and $(I-\\text{pinv}(H)H)$ is one way of calculating the projection of that vector into the data nullspace. \n",
    "\n",
    "To recap the intuition: shallow gradient descent learning in overparameterized models has a *frozen subspace* in which weight components do not change during learning, and so initial values in this subspace persist forever. We have to add the projection of our initialization on this frozen subspace to the minimum norm linear regression solution to obtain the solution that gradient descent would find. New test examples can then overlap with this frozen subspace, making the network's test performance depend on the initial weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c96724-892c-41f0-beff-41aca7a7d6de",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_double_descent_initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6bd6c-de46-4418-bb8d-79f46dc9e12e",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Summary\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', '6V1MZBRxVlo'), ('Bilibili', 'BV1Gr421w745')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954837a-af3f-470f-9fe4-0caad95bc7df",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b83c94-1ab5-4a7d-8e3a-1b5d843afce8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "Estimated timing of tutorial: 1 hour\n",
    "\n",
    "In this tutorial, we observed the phenomenon of double descent: the situation when the overparameterized network was expected to behave as overfitted but instead generalized better to the unseen data. Moreover, we discovered how noise, regularization & initial scale impact the effect of double descent and, in some cases, can fully cancel it.\n",
    "\n",
    "Combining with the findings from the first tutorial of the previous day on sparse connections and activity, brain architecture suggests it might operate in over parameterized regime, which enables the interpolation of vast amounts of sensory and experiential data smoothly, leading to effective generalization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial2",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
