{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6221f0d3-4a18-416b-b548-3d9a5b82dedf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Contrastive learning for object recognition\n",
    "\n",
    "**Week 1, Day 2: Comparing Tasks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Andrew F. Luo, Leila Wehbe\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734dc52-6dc9-4669-a7c7-99ef87e47b8b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 20 minutes*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "1. Understand why we want to do contrastive learning.\n",
    "2. Understand the losses in contrastive learning.\n",
    "3. Run an example on contrastive learning using MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceddeef-ea13-4224-8ca8-70563edbcf00",
   "metadata": {
    "cellView": "form",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download//\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io//?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ae64d-bc44-4e0e-b077-487da8391334",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0ea4d",
   "metadata": {},
   "source": [
    "##  Install and import feedback gadget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeb07c-5de3-428e-a422-3a58f1124b43",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install numpy matplotlib torch torchvision tqdm ipysankeywidget ipywidgets --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt - leave this as is\n",
    "        notebook_section,\n",
    "        {\n",
    "        \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "        \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "        \"user_key\": \"y1x3mpx5\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "feedback_prefix = \"W1D2_T2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56685b9-3f17-4a55-acca-73dad5623992",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# Set up PyTorch backend configurations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn for machine learning utilities\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f242f",
   "metadata": {},
   "source": [
    "## Figure settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5798-a8dd-4985-b0ec-d5facf4ee700",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perform high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471a7f",
   "metadata": {},
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cc45f-c4b7-406c-a80b-6adfae387583",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d728a",
   "metadata": {},
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b49a0-9409-496a-8656-579ca3e4af5f",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pIBjtdIDeeVg",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Overview of the tutorial\n",
    "\n",
    "To begin, we will start by importing all the necessary packages that we'll need throughout our session. This initial step ensures that all the tools and functions required for our computations are readily available.\n",
    "\n",
    "### Speeding up training and inference\n",
    "\n",
    "We set 'allow tf32' settings in PyTorch, this is designed to enhance computational speed at the expense of precision. This setting is particularly relevant for operations involving tensors. PyTorch automatically enables this for CuDNN-based convolution operations to accelerate processing times. However, it's important to note that for matrix multiplication tasks, this option remains disabled by default to maintain higher numerical precision.\n",
    "\n",
    "### Analysis of the results\n",
    "\n",
    "As we move forward, we'll employ PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) as our primary tools for visualizing data. These techniques are instrumental in reducing the dimensionality of the data, allowing us to observe patterns and relationships that are otherwise difficult to discern in high-dimensional spaces. By visualizing data in this way, we can gain insightful perspectives that are crucial for understanding complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74907a-f462-4acf-835d-340a899a3663",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### What is contrastive learning?\n",
    "\n",
    "Contrastive learning is often referred to as \"self-supervised learning (SSL)\" and has historically been known as \"metric learning.\" The essence of contrastive/metric learning is that instead of outputting a classification one-hot/softmax vector, or a regression value, you directly output a high-dimensional embedding.\n",
    "\n",
    "Here is an example: given multiple data points from a single class (for example, three photos of you from different viewpoints) and different classes (for example, 10 photos from one or multiple people who are not you), you want the three embeddings from your photos to be closer to each other while being farther away from the ten embeddings from the different classes.\n",
    "\n",
    "Hence the name \"metric learning,\" where you seek to learn a metric/distance that fits the constraints of the data.\n",
    "\n",
    "\n",
    "### Why contrastive learning?\n",
    "\n",
    "It may not be immediately obvious why you would want to engage in contrastive/metric learning. Can't you just use a giant 1000-class ImageNet-trained classifier and recognize every image? However, metric learning proves useful when the number of classes is not known ahead of time. For example, if I wanted a network to recognize human faces, there are 7 billion people on this planet, which makes it impossible for you to train a classification network with 7 billion output neurons. Nevertheless, I can train a network that outputs a high-dimensional embedding for each image. Now, given a reference image of a person, your network can decide if the new photo is close to or farther away from the reference image.\n",
    "\n",
    "\n",
    "### Terms defined and other stuff\n",
    "\n",
    "* Positive pair—This refers to two data points that should be close together in embedding space. For example, two photos of you in different lighting conditions.\n",
    "* Negative pair—This refers to two data points that should be far apart in embedding space. For example, a photo of you versus a photo of a dog (assuming you are not a dog). Note that positive pairs/negative pairs don't have to be images. You could have a picture and the matching text be a positive pair as well. Recent work has also moved to positive pairs defined using an older version of the encoder (Google Momentum contrast or EMA contrastive).\n",
    "* Pretext task—In computer vision, this refers to how you augment images to get positive pairs.\n",
    "Hard positive/negative mining—This refers to a practice where positive/negative pairs where the network struggles are used to train the network with more loss in some way.\n",
    "* InfoNCE—This is one of the most common contrastive losses [1,2,3,4], which was proposed in similar ways multiple times by different authors. It is a cross-entropy loss of classifying the correct positive pair out from a pool of pairs. Note there are variants like MIL-NCE, which allow for multiple positive pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AJoELbcmeO_n",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "First, we'll start by outlining the network blocks that we plan to incorporate into our model. This involves defining each component as a class in PyTorch, which requires inheriting from the torch.nn.Module class. It's crucial to remember that after defining your class, you must initialize it properly by calling super().__init__(). This step is essential as it ensures that all network sub-modules are registered correctly. Additionally, PyTorch provides several useful functions such as ModuleList, register_parameter, register_module, and register_buffer to help manage these components effectively.\n",
    "\n",
    "### Mini residual block\n",
    "\n",
    "Our initial focus will be on creating a mini_residual block. This block adopts a modern approach to the residual design, featuring a prenormalization step as suggested by Kaiming He. We will also incorporate the LeakyReLU activation function. LeakyReLU is particularly favored in generative adversarial networks (GANs) due to its ability to maintain non-zero gradients, which helps in the training process by avoiding the vanishing gradient problem. Residual connections are widely used in deep learning, as they prevent vanishing gradient problems.\n",
    "\n",
    "### Full model construction\n",
    "\n",
    "Following the mini_residual block, we will construct the full model. This model will consist of a series of residual blocks stacked together. In PyTorch, the components of a model are organized in a sequence using nn.Sequential, which executes the blocks from the first to the last. This sequential arrangement simplifies the process of defining forward pass operations, ensuring that data flows through the blocks in the intended order. By stacking these blocks, the model can learn complex patterns from the data, enhancing its predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c432bf-1de5-43f6-b887-087d17e9ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_residual(nn.Module):\n",
    "    # Follows \"Identity Mappings in Deep Residual Networks\", uses LayerNorm instead of BatchNorm, and LeakyReLU instead of ReLU\n",
    "    def __init__(self, feat_in=128, feat_out=128, feat_hidden=256, use_norm=True):\n",
    "        super().__init__()\n",
    "        # Define the residual block with or without normalization\n",
    "        if use_norm:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LayerNorm(feat_in),  # Layer normalization on input features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LayerNorm(feat_hidden),  # Layer normalization on hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "        \n",
    "        # Define the bypass connection\n",
    "        if feat_in != feat_out:\n",
    "            self.bypass = nn.Linear(feat_in, feat_out)  # Linear layer to match dimensions if they differ\n",
    "        else:\n",
    "            self.bypass = nn.Identity()  # Identity layer if input and output dimensions are the same\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Forward pass: apply the block and add the bypass connection\n",
    "        return self.block(input_data) + self.bypass(input_data)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, num_blocks=4):\n",
    "        super().__init__()\n",
    "        # Initial linear projection from input dimension to hidden dimension\n",
    "        self.in_proj = nn.Linear(in_dim, hidden_dim)\n",
    "        # Sequence of residual blocks\n",
    "        self.hidden = nn.Sequential(\n",
    "            *[mini_residual(feat_in=hidden_dim, feat_out=hidden_dim, feat_hidden=hidden_dim) for i in range(num_blocks)]\n",
    "        )\n",
    "        # Output linear projection from hidden dimension to output dimension\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: input projection, passing through residual blocks, and final output projection\n",
    "        in_proj_out = self.in_proj(x)\n",
    "        hidden_out = self.hidden(in_proj_out)\n",
    "        return self.out(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C29eZ1BkfuIr",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's move on to defining the loss function for our model, using an approach extracted from the PyTorch metric learning package for better clarity. We will be implementing a variant of the InfoNCE loss function, which is widely recognized as one of the most effective contrastive or metric learning losses. It has been prominently used in various models, including OpenAI's CLIP, due to its ability to enhance feature discrimination by contrasting positive pairs against negative pairs.\n",
    "\n",
    "InfoNCE typically requires substantial batch sizes—commonly 128 or even larger—to perform optimally. This requirement is due to the need for diverse negative samples in the batch to effectively learn the contrasts. However, large batch sizes can be impractical in resource-constrained settings or when data availability is limited.\n",
    "\n",
    "To address this, we will implement a modified version of InfoNCE as described in the \"Decoupled Contrastive Learning\" paper. This variant adapts the loss to be more suitable for smaller batch sizes by modifying the denominator of the InfoNCE formula. Specifically, it removes the positive example from the denominator, which reduces the computational demand and stabilizes training when fewer examples are available. This adjustment not only makes the loss function more flexible but also maintains robustness in learning discriminative features even with smaller batch sizes.\n",
    "\n",
    "Here is what the default infoNCE loss looks like. Note that prior to the dot product, the vectors are normalized to unit norm (photo credit: pytorch metric library).\n",
    "\n",
    "![Picture which shows infoNCE loss.](https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/infoNCEloss.png?raw=true)\n",
    "\n",
    "\n",
    "Remember, a loss is being minimized! So if you ignore the negative sign, you are trying to maximize everything else. The loss tries to maximize the dot-product similarity between embeddings of positive pairs. This can be seen above the line, where the numerator only consists of the positive pair similarity.\n",
    "\n",
    "On the bottom in the denominator, you are computing the sum of dot-product similarities for all the elements.\n",
    "\n",
    "Basically, we want the similarity of positive pairs to be higher as a ratio than the similaritiy of all pairs. DCL loss modifies this slightly to remove the positive pair from the denominator, please see their paper for a justification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f70265-2738-4ec3-b07f-b1d06507cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is code from the pytorch metric learning package\n",
    "# Extracted out so it is clear what it is doing\n",
    "\n",
    "def neg_inf(dtype):\n",
    "    # Returns the smallest possible value for the given data type\n",
    "    return torch.finfo(dtype).min\n",
    "\n",
    "def small_val(dtype):\n",
    "    # Returns the smallest positive value greater than zero for the given data type\n",
    "    return torch.finfo(dtype).tiny\n",
    "\n",
    "def to_dtype(x, tensor=None, dtype=None):\n",
    "    # Converts tensor `x` to the specified `dtype`, or to the same dtype as `tensor`\n",
    "    if not torch.is_autocast_enabled():\n",
    "        dt = dtype if dtype is not None else tensor.dtype\n",
    "        if x.dtype != dt:\n",
    "            x = x.type(dt)\n",
    "    return x\n",
    "\n",
    "def get_matches_and_diffs(labels, ref_labels=None):\n",
    "    # Returns tensors indicating matches and differences between pairs of labels\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)  # Expand dimensions for comparison\n",
    "    labels2 = ref_labels.unsqueeze(0)  # Expand dimensions for comparison\n",
    "    matches = (labels1 == labels2).byte()  # Byte tensor of matches\n",
    "    diffs = matches ^ 1  # Byte tensor of differences (inverse of matches)\n",
    "    if ref_labels is labels:\n",
    "        matches.fill_diagonal_(0)  # Remove self-matches\n",
    "    return matches, diffs\n",
    "\n",
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    matches, diffs = get_matches_and_diffs(labels, ref_labels)\n",
    "    a1_idx, p_idx = torch.where(matches)  # Indices for positive pairs\n",
    "    a2_idx, n_idx = torch.where(diffs)  # Indices for negative pairs\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n",
    "def cos_sim(input_embeddings):\n",
    "    # Computes cosine similarity matrix for input embeddings\n",
    "    normed_embeddings = torch.nn.functional.normalize(input_embeddings, dim=-1)  # Normalize embeddings\n",
    "    return normed_embeddings @ normed_embeddings.t()  # Cosine similarity matrix\n",
    "\n",
    "def dcl_loss(pos_pairs, neg_pairs, indices_tuple, temperature=0.07):\n",
    "    # This is the modified InfoNCE loss called \"Decoupled Contrastive Learning\" for small batch sizes\n",
    "    # Basically You remove the numerator from the sum to the denominator\n",
    "\n",
    "    a1, p, a2, _ = indices_tuple  # Unpack indices\n",
    "\n",
    "    if len(a1) > 0 and len(a2) > 0:\n",
    "        dtype = neg_pairs.dtype\n",
    "        pos_pairs = pos_pairs.unsqueeze(1) / temperature  # Scale positive pairs by temperature\n",
    "        neg_pairs = neg_pairs / temperature  # Scale negative pairs by temperature\n",
    "        n_per_p = to_dtype(a2.unsqueeze(0) == a1.unsqueeze(1), dtype=dtype)  # Indicator matrix for matching pairs\n",
    "        neg_pairs = neg_pairs * n_per_p  # Zero out non-matching pairs\n",
    "        neg_pairs[n_per_p == 0] = neg_inf(dtype)  # Replace non-matching pairs with negative infinity\n",
    "\n",
    "        # Compute the maximum value for numerical stability\n",
    "        max_val = torch.max(\n",
    "            pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0]\n",
    "        ).detach()\n",
    "        # Compute numerator and denominator for the loss\n",
    "        numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "        denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1)\n",
    "        log_exp = torch.log((numerator / denominator) + small_val(dtype))\n",
    "        return -log_exp  # Return the negative log of the exponential\n",
    "    return 0\n",
    "\n",
    "def pair_based_loss(mat, indices_tuple, lossfunc):\n",
    "    # Computes pair-based loss using the provided loss function\n",
    "    a1, p, a2, n = indices_tuple  # Unpack indices\n",
    "    pos_pair, neg_pair = [], []\n",
    "    if len(a1) > 0:\n",
    "        pos_pair = mat[a1, p]  # Extract positive pairs\n",
    "    if len(a2) > 0:\n",
    "        neg_pair = mat[a2, n]  # Extract negative pairs\n",
    "    return lossfunc(pos_pair, neg_pair, indices_tuple)  # Apply loss function\n",
    "\n",
    "# Example usage:\n",
    "# dummy_labels = torch.from_numpy(np.array([1,1,2,3,2,4]))\n",
    "# demo_matches, demo_diffs = get_matches_and_diffs(labels=dummy_labels)\n",
    "# results = get_all_pairs_indices(labels=dummy_labels)\n",
    "# final_loss = pair_based_loss(torch.randn(6,6),results, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAajOxFCgYFK",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we will make the Pytorch dataset object, this defines how data is loaded from disk for each batch, and what transform you want to apply. Note that you do not have to use torchvision transforms. It is very common to write your own transform code in the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d9231-35a5-405e-b2f6-135e134e0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations for the MNIST dataset\n",
    "mnist_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # Convert images to tensor\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Normalize the images with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Load the MNIST test dataset with the defined transformations\n",
    "test_dset = torchvision.datasets.MNIST(\"./\", train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "# Calculate the height and width of the MNIST images (28x28)\n",
    "height = int(784**0.5)\n",
    "width = height\n",
    "\n",
    "# Select the first image from the test dataset\n",
    "idx = 0\n",
    "data_point = test_dset[idx]\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(data_point[0][0].numpy(), cmap='gray')  # Display the image in grayscale\n",
    "plt.show()\n",
    "\n",
    "# Print the label of the selected image\n",
    "print(data_point[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CkhXcVrGhUM5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we will make the model using the definition we wrote previously. And we will move it to the device you want. Note that in pytorch, calling `.to(device)` on a module acts on the module itself, as in it is an inplace operation. However for pytorch tensors directly (if you don't call this function on a module) it is not inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3ab89-ffd9-408c-8396-25b88e0930fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with specified input, output, and hidden dimensions\n",
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "\n",
    "# Automatically select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "_ = mynet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pi1Seq52ho57",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us create a test dataloader, and see what the untrained network gives us in terms of representations for each number. We will compute the cosine similarity for each written character WITHIN the same class (we set the diagonal to np.nan to avoid comparing a written character to itself).\n",
    "\n",
    "We will also compute the cosine similarity for each written character across the classes.\n",
    "\n",
    "You should have a habit of calling `network.eval()` before evaluating a network, this is also an inplace operation. This will tell pytorch to freeze some buffers (like in batchnorm) and disable dropout.\n",
    "\n",
    "We use `torch.inference_mode()` here, this disables gradient computation and speeds up the testing process. However this may break some features, if it fails you can replace it with `torch.no_grad()`. Note that `inference_mode` does not automatically enable `eval`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9122b-ce83-464f-b6c9-5af2feceac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try with untrained network, find the cosine similarities within a class and across classes\n",
    "\n",
    "# Create a DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Initialize a similarity matrix of size 10x10 for 10 classes\n",
    "sim_matrix = np.zeros((10, 10))\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch  # Get images and labels from the batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)  # Flatten the images and move to device\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()  # Get embeddings from the model and move to CPU\n",
    "        test_embeddings.extend(pred_embeddings)  # Store the embeddings\n",
    "        test_labels.extend(test_label.numpy().tolist())  # Store the labels\n",
    "\n",
    "# Convert embeddings and labels to numpy arrays\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Convert test labels to numpy array\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GtbiyBRmi-e2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Visualizing the cosine similarity of embeddings within the same class and across different classes before training\n",
    "\n",
    "Ideally, you should see a very high cosine similarity for images within the same class (the diagonal), but very low cosine similarity for images not within the same class (the non-diagonal).\n",
    "\n",
    "But since our network is untrained, *you* will see there is not much difference, this is expected as the network is untrained. If you look at the plot, there is no clear structure to the similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0JcR1VwliL1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store normalized embeddings for each class\n",
    "embeddings = {}\n",
    "for i in range(10):\n",
    "    embeddings[i] = test_embeddings_normed[test_labels == i]\n",
    "\n",
    "# Within class cosine similarity:\n",
    "for i in range(10):\n",
    "    sims = embeddings[i] @ embeddings[i].T  # Compute cosine similarity matrix within the class\n",
    "    np.fill_diagonal(sims, np.nan)  # Ignore diagonal values (self-similarity)\n",
    "    cur_sim = np.nanmean(sims)  # Calculate the mean similarity excluding diagonal\n",
    "    sim_matrix[i, i] = cur_sim  # Store the within-class similarity in the matrix\n",
    "\n",
    "    print(\"Within class {} cosine similarity\".format(i, cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "# Between class cosine similarity:\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            continue  # Skip if same class (already computed)\n",
    "        elif i > j:\n",
    "            continue  # Skip if already computed (matrix symmetry)\n",
    "        else:\n",
    "            sims = embeddings[i] @ embeddings[j].T  # Compute cosine similarity between different classes\n",
    "            cur_sim = np.mean(sims)  # Calculate the mean similarity\n",
    "            sim_matrix[i, j] = cur_sim  # Store the similarity in the matrix\n",
    "            sim_matrix[j, i] = cur_sim  # Ensure symmetry in the matrix\n",
    "            print(\"{} and {} cosine similarity {}\".format(i, j, cur_sim))\n",
    "\n",
    "# Plotting the similarity matrix\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"Untrained Network Cosine Similarity Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iAScidA3iXpZ",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we will train the network!\n",
    "\n",
    "Note how we decay the learning rate, so the final learning rate will be half that of the inital learning rate. AdamW is the Adam optimizer with decoupled weight decay. A learning rate of 3e-4 and a weight decay of 1e-2 in AdamW are pretty typical. Note that weight decay in AdamW and SGD work differently in the pytorch implementations. In pytorch, the adamw weight decay is further scaled by learning rate (real weight decay = weight decay * lr) but in SGD, it is not scaled by learning rate. So in AdamW, it is common to use higher weight decay values than SGD.\n",
    "\n",
    "Also note how we call `mynet.train()` before we start training. That sets mynet to training mode, and enables the buffers and dropout layers (if they were present in the network architecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b62ecc-c0c4-4b2e-8d93-ab4ea9a952d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training\n",
    "epochs = 10\n",
    "\n",
    "# Automatically select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the MNIST training dataset with the defined transformations\n",
    "train_dset = torchvision.datasets.MNIST(\"./\", train=True, transform=mnist_transforms)\n",
    "train_loader = DataLoader(train_dset, batch_size=50, shuffle=True)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Cleanup: delete the optimizer and free up memory if this block is re-run\n",
    "try:\n",
    "    del optimizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cleanup: delete the network and free up memory if this block is re-run\n",
    "try:\n",
    "    del mynet\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Initialize the model with specified input, output, and hidden dimensions\n",
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "_ = mynet.to(device)  # Move the model to the selected device\n",
    "\n",
    "# Enable training mode, which may affect dropout and other layers\n",
    "mynet.train(mode=True)\n",
    "print(\"Is the network in training mode?\", mynet.training)\n",
    "\n",
    "# Initial learning rate and decay factor for the optimizer\n",
    "init_lr = 3e-4\n",
    "lr_decay_factor = 0.5\n",
    "\n",
    "# Initialize the optimizer with model parameters and learning rate\n",
    "optimizer = torch.optim.AdamW(mynet.parameters(), lr=init_lr, weight_decay=1e-2)\n",
    "\n",
    "# Tracker to keep track of loss values during training\n",
    "loss_tracker = []\n",
    "\n",
    "# Training loop over the specified number of epochs\n",
    "for epoch_id in range(1, epochs+1):\n",
    "    loss_epoch_tracker = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    # Adjust learning rate for the current epoch\n",
    "    new_lrate = init_lr * (lr_decay_factor ** (epoch_id / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "\n",
    "    batches_in_epoch = len(train_loader)\n",
    "    for data_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "\n",
    "        # Get images and labels from the batch\n",
    "        train_img, train_label = data_batch\n",
    "        batch_size = train_img.shape[0]\n",
    "\n",
    "        # Flatten images and move data to the selected device\n",
    "        flat = train_img.reshape(batch_size, -1).to(device, non_blocking=True)\n",
    "        train_label = train_label.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass through the network\n",
    "        predicted_results = mynet(flat)\n",
    "\n",
    "        # Compute cosine similarity matrix for the batch\n",
    "        similarities = cos_sim(predicted_results)\n",
    "\n",
    "        # Get pairs of indices for positive and negative pairs\n",
    "        label_pos_neg = get_all_pairs_indices(train_label)\n",
    "\n",
    "        # Compute the loss using the decoupled contrastive learning loss function\n",
    "        final_loss = torch.mean(pair_based_loss(similarities, label_pos_neg, dcl_loss))\n",
    "\n",
    "        # Compute gradients from the loss\n",
    "        final_loss.backward()\n",
    "\n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert the loss to a single CPU scalar\n",
    "        loss_cpu_number = final_loss.item()\n",
    "\n",
    "        # Keep track of the losses for visualization\n",
    "        loss_epoch_tracker += loss_cpu_number\n",
    "        batch_counter += 1\n",
    "\n",
    "        # Print the current epoch, batch number, and loss every 500 batches\n",
    "        if batch_counter % 500 == 0:\n",
    "            print(\"Epoch {}, Batch {}/{}, loss: {}\".format(epoch_id, batch_counter, batches_in_epoch, loss_cpu_number))\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(\"Epoch average loss {}\".format(loss_epoch_tracker / batch_counter))\n",
    "\n",
    "# Set the model to test mode (optional, not used here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvQ4MdHKjY1t",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us now extract the features from the trained network!\n",
    "\n",
    "Again, please make it a habit to set the network into eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f11e3-07e8-4104-9ae4-6a9fd6f037eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch  # Get images and labels from the batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)  # Flatten images and move to device\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()  # Get embeddings from the model and move to CPU\n",
    "        test_embeddings.extend(pred_embeddings)  # Store the embeddings\n",
    "        test_labels.extend(test_label.numpy().tolist())  # Store the labels\n",
    "\n",
    "# Convert test labels to numpy array for further processing\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Indicate that feature extraction is complete\n",
    "print(\"Feature extraction done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4GGxdeiWjkIr",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As the network was trained using infoNCE, we will normalize each feature to unit norm. PCA further expects the features to be centered and 1 std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e0f93-3f68-4db0-9edf-4891a3858f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of embeddings to a numpy array\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Center the normalized embeddings by subtracting the mean\n",
    "test_embeddings_normed = test_embeddings_normed - np.mean(test_embeddings_normed, axis=1, keepdims=True)\n",
    "\n",
    "# Standardize the centered embeddings by dividing by the standard deviation\n",
    "test_embeddings_normed = test_embeddings_normed / np.std(test_embeddings_normed, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the embeddings and transform them to 2D\n",
    "pca_embeddings = pca.fit_transform(test_embeddings)\n",
    "\n",
    "# Optional: Print the shape of the resulting PCA embeddings to verify\n",
    "print(\"PCA embeddings shape:\", pca_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8UXxSAmfjoB8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For TSNE, we just normalize each feature to unit norm due to infoNCE. We will not further center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdd26a-15c9-4c5f-82f7-4a65ca0bd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of embeddings to a numpy array\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings to unit length by dividing each embedding by its L2 norm\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize t-SNE with 2 components for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# Fit t-SNE on the normalized embeddings and transform them to 2D\n",
    "tsne_embeddings = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "# Notify that the t-SNE transformation may take some time\n",
    "print(\"t-SNE transformation in progress... This may take a minute. Go grab a coffee or something.\")\n",
    "\n",
    "# Optional: Print the shape of the resulting t-SNE embeddings to verify\n",
    "print(\"t-SNE embeddings shape:\", tsne_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de073f-d514-4156-98e2-d7aac5620ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape, tsne_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqJJWmx7j1kF",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Look at how the features for each number are distributed! Note how well separated all the embeddings for different characters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe3c0d-7eed-4b2b-bdaa-bb258fc92d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE embeddings for visualization\n",
    "my_embeddings = tsne_embeddings\n",
    "# TSNE or PCA? TSNE is nicer to look at.\n",
    "\n",
    "# Plot embeddings for digit '0' in red\n",
    "num = 0\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"red\")\n",
    "\n",
    "# Plot embeddings for digit '1' in green\n",
    "num = 1\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"green\")\n",
    "\n",
    "# Plot embeddings for digit '2' in blue\n",
    "num = 2\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"blue\")\n",
    "\n",
    "# Plot embeddings for digit '3' in orange\n",
    "num = 3\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0KXB1wpmMp1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Visualizing the cosine similarity AFTER training\n",
    "\n",
    "Note now that diagonal is strongly more positive than the off-diagonal, this means within the same class, the similarity is much stronger than outside a class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a8b40-faa7-4071-b070-0ca192e06d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Initialize a similarity matrix of size 10x10 for 10 classes\n",
    "sim_matrix = np.zeros((10, 10))\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        # Get images and labels from the batch\n",
    "        test_img, test_label = data_batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "\n",
    "        # Flatten images and move data to the selected device\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)\n",
    "\n",
    "        # Get embeddings from the model and move to CPU\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()\n",
    "\n",
    "        # Store the embeddings and labels\n",
    "        test_embeddings.extend(pred_embeddings)\n",
    "        test_labels.extend(test_label.numpy().tolist())\n",
    "\n",
    "# Convert embeddings and labels to numpy arrays for further processing\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings to unit length by dividing each embedding by its L2 norm\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Convert test labels to a numpy array\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Dictionary to store normalized embeddings for each class\n",
    "embeddings = {}\n",
    "for i in range(10):\n",
    "    embeddings[i] = test_embeddings_normed[test_labels == i]\n",
    "\n",
    "# Calculate within-class cosine similarity\n",
    "for i in range(10):\n",
    "    # Compute cosine similarity matrix within the class\n",
    "    sims = embeddings[i] @ embeddings[i].T\n",
    "\n",
    "    # Ignore diagonal values (self-similarity)\n",
    "    np.fill_diagonal(sims, np.nan)\n",
    "\n",
    "    # Calculate the mean similarity excluding diagonal\n",
    "    cur_sim = np.nanmean(sims)\n",
    "\n",
    "    # Store the within-class similarity in the matrix\n",
    "    sim_matrix[i, i] = cur_sim\n",
    "\n",
    "    # Print the within-class cosine similarity\n",
    "    print(\"Within class {} cosine similarity\".format(cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "# Calculate between-class cosine similarity\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            pass  # Skip if same class (already computed)\n",
    "        elif i > j:\n",
    "            pass  # Skip if already computed (matrix symmetry)\n",
    "        else:\n",
    "            # Compute cosine similarity between different classes\n",
    "            sims = embeddings[i] @ embeddings[j].T\n",
    "\n",
    "            # Calculate the mean similarity\n",
    "            cur_sim = np.mean(sims)\n",
    "\n",
    "            # Store the similarity in the matrix\n",
    "            sim_matrix[i, j] = cur_sim\n",
    "            sim_matrix[j, i] = cur_sim  # Ensure symmetry in the matrix\n",
    "\n",
    "            # Print the between-class cosine similarity\n",
    "            print(\"{} and {} cosine similarity {}\".format(i, j, cur_sim))\n",
    "\n",
    "# Plot the similarity matrix using matplotlib\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"Trained Network Cosine Similarity Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8wRjpsrmva7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Using the network to identify nearest neighbors in the test set.\n",
    "\n",
    "But how do people actually use a contrastive learned network? In Person Re-ID (reidentification), you will use a network to compute the embeddings for two images, and check if the embeddings have cosine/euclidean similarity above some threshold to decide if they are the same person.\n",
    "\n",
    "In foundation model training (CLIP), people typically fine tune the entire network or train a linear probe or small network on the last layer outputs.\n",
    "\n",
    "Here, we will follow the person Re-ID setup, and try to find the most similar image in a test set and decide if they represent the same character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013D0NbXowjj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity matrix for all embeddings\n",
    "sims_all = test_embeddings_normed @ test_embeddings_normed.T\n",
    "\n",
    "# Set diagonal elements to a large negative value to avoid self-matching\n",
    "np.fill_diagonal(sims_all, -1000.0)  # Set to a small value so it doesn't give us the same number for argmax\n",
    "\n",
    "# Index of the embedding to check for the most similar embedding\n",
    "idx_to_check = 3029\n",
    "\n",
    "# Find the index of the most similar embedding (excluding itself)\n",
    "best_idx = np.argmax(sims_all[idx_to_check])\n",
    "\n",
    "# Plot the image corresponding to the index to check\n",
    "plt.imshow(test_dset[idx_to_check][0][0].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "# Plot the image corresponding to the most similar embedding\n",
    "plt.imshow(test_dset[best_idx][0][0].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444cc4a-fd26-471c-ac34-7e515dbe946a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### How is contrastive learning used in practice?\n",
    "\n",
    "Nearly all vision foundation models (DINO and DINOv2, CLIP and all CLIP derivatives including OpenCLIP/EVA-CLIP) are trained using contrastive losses. DINO/v2 is trained on images alone, while CLIP is trained on a combination of images and text.\n",
    "\n",
    "When only images are used, the contrastive learning loss is applied to augmentations of the same image. Examples include crops/flips/rotations of images -- when used in the way, the augmentations are called a \"pretext task\". Typically, augmentations of the same image are treated as instances where the embeddings should be the same. So for example, a network should recognize a photo of you, and a photo of you flipped (or brightness changed, or noise added, or changed to black and white) as a photo of the same person.\n",
    "\n",
    "When images and text are used together as in CLIP, you have images and captions of those images -- for example the caption \"A photo of a dog\" may be matched to a picture of a blue heeler puppy. These captions are typicalled scraped from online sources, and collected into datasets like LAION-2B/COYO-700M/CommonCrawl. While these captions are typically not high quality, having billions of them do seem to mitigate this issue. When contrastive learning is used in this case, typically a dual encoder is used -- one for text, and one for the image. The network is trained using a loss which minimizes the distance between the correct text and image pair, while distances between incorrect text and image pairs are maximized. For example -- the caption \"A photo of a dog\" and the picture of the blue heeler puppy should have embeddings that are close, while the caption should be far away from a picture of a cat. Often times, normalized dot-product (cosine similarity), angular distance (Universal Sentence Encoder) euclidean distance, or squared euclidean distance are applied to compute the \"distance\" of the embeddings.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[1] Unsupervised feature learning via non-parametric instance discrimination (2018)\n",
    "\n",
    "[2] Representation learning with contrastive predictive coding (2018)\n",
    "\n",
    "[3] A simple framework for contrastive learning of visual representations (2020)\n",
    "\n",
    "[4] Improved Deep Metric Learning with Multi-class N-pair Loss Objective (2016)\n",
    "\n",
    "[4] Noise-contrastive estimation: A new estimation principle for unnormalize statistical models (2010)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D2_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
