{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tutorial 3: Reinforcement learning across temporal scales\n",
    "\n",
    "**Week 1, Day 2: Comparing Tasks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Leila Wehbe & Swapnil Kumar\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Lily Chamakura, RyeongKyung Yoon, Yizhou Chen, Ruiyi Zhang\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 20-30min*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- Review how a reinforcement learning algorithm can be evaluated.\n",
    "- Formulate a simple meta-reinforcement learning framework where an algorithm learns how to learn in different episodes.\n",
    "- Investigate learning at different temporal scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1717168540208,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "\n",
    "link_id = \"x4pa5\"\n",
    "\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "##  Install and import feedback gadget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck requests matplotlib ipython numpy --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_neuroai\",\n",
    "            \"user_key\": \"wb2cxze8\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D2_T3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard library imports\n",
    "import logging\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import IFrame, display, Image\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# URL of the file on OSF\n",
    "url = \"https://osf.io/swrmj/download\"\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the response content to a file\n",
    "with open('trials.py', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Execute the downloaded Python file\n",
    "%run trials.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perform high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Reinforcement learning across temporal scales\n",
    "\n",
    "## Outline:\n",
    "\n",
    "This tutorial:\n",
    "- Reviews the basics of reinforcement learning\n",
    "- Introduces meta-reinforcement learning (meta-RL)\n",
    "- Studies how \"learning to learn\" is implemented in the context of a binary bandit task\n",
    "- Studies how learning is affected by changing temporal scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "##  Video 1: meta-RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1717210414472,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: meta-RL\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "video_ids = [('Youtube', 'BOGX0KvpnEU')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reinforcement learning review\n",
    "\n",
    "Reinforcement learning is a learning paradigm where an agent interacts with an environment and learns a policy to maximize its cumulative rewards. We focus here on a simple environment: a binary \"two-armed bandit\" (Figure A), a name for a gambling machine found in casinos (because it steals your money). In this task, at each opportunity the agent chooses arm $i$ (either Left $L$ or Right $R$) and gets a reward with probability $p_i$ that is unknown to the agent. On each trial, only one of the arms will return a reward, so $p_L + p_R = 1$ (which the agent does know).\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/two_armed_bandit.png?raw=true\" width=600 />\n",
    "\n",
    "*Figure A. Example of binary bandit. The left arm has a probability $p_L$ of giving a reward, and the right arm a probability $p_R = 1-p_L$ of giving a reward. At a given trial, the agent pulls one of the arms and receives a reward with the corresponding probability.*\n",
    "\n",
    "The aim of the agent is to maximize its cumulative reward over a series of trials (e.g., 100 trials). This would correspond to correctly identifying the arm that returns more reward (has a higher probability) and choosing it for subsequent trials once it's sure about its choice. Thus, while solving this problem, the agent will need to balance exploration and exploitation. At a high level, more exploration is useful while the agent is not certain of which arm has more reward, and once the agent is more certain, it is more advantageous to exploit the arm that has a higher probability of reward.\n",
    "\n",
    "There are different methods that are usually used to solve this problem. An optimal method to solve some instances of bandit problems is [Gittin indices](https://www.statslab.cam.ac.uk/~rrw1/oc/ocgittins.pdf). This method incorporates the estimated rewards and their uncertainty, and involves solving an expensive dynamical programing problem. The simplest method is [epsilon greedy](https://medium.com/@ym1942/exploring-multi-armed-bandit-problem-epsilon-greedy-epsilon-decreasing-ucb-and-thompson-02ad0ec272ee), where exploration and exploitation are balanced by choosing a random action with probability $\\epsilon$ and the best known action with probability $1-\\epsilon$. In the [Upper Confidence Bound (UCB)](https://medium.com/@ym1942/exploring-multi-armed-bandit-problem-epsilon-greedy-epsilon-decreasing-ucb-and-thompson-02ad0ec272ee), the choice of action also incorporates both the estimated rewards and the estimated uncertainty of the rewards. In [Thompson sampling](https://medium.com/@ym1942/exploring-multi-armed-bandit-problem-epsilon-greedy-epsilon-decreasing-ucb-and-thompson-02ad0ec272ee), the choice is made according to a Bayesian model that estimates the posterior probabilities of the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point 1\n",
    "Imagine training an RL algorithm on the binary bandit problem above, with a method that depends on uncertainty about the reward probabilities (like UCB). Assume that $p_L=0.05$ and $p_R=0.95$. How rapidly will the agent become certain of its estimates of the expected reward of each arm? Do you expect the exploration phase to take a long time or a short time?\n",
    "\n",
    "What about the case when $p_L=0.51$ and $p_R=0.49$? How rapidly will the agent become certain of its estimates, and how will that affect the exploration phase?\n",
    "\n",
    "#### Answer\n",
    "When one of the arms has a high reward, it is easier to identify it. The uncertainty of the agent thus reduces quickly, and they spend less time in exploration. When the probabilities of rewards are more equal (close to 0.5), a lot of trials are needed to reduce the uncertainty, and the exploration phase lasts a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Evaluating decision-making strategies\n",
    "\n",
    "One key metric for evaluating an agent's strategies is to compute the cumulative regret metric. Regret at a specific time step refers to the difference in reward between the chosen strategy and the reward that could have been obtained by selecting the best possible action. The cumulative regret refers to the sum of the regret from the start of the episode until the current time step $T$. For our example of binary bandit, it corresponds to:\n",
    "\n",
    "$$R(T) = \\sum_{t = 1}^T (p^* - \\mathbb{E}(p_{a_t})),$$\n",
    "\n",
    "where $p^*$ is the probability of the reward for the best arm, i.e., max($p_L$, $p_r$). $\\mathbb{E}(p_{a_t})$ corresponds to the expected probability of reward for the action that was chosen at previous time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point 2\n",
    "\n",
    "Consider the setting where $p_L=0.25$ and $p_R=0.75$. Let's derive a lower bound and an upper bound for cumulative regret over 100 trials of one episode.\n",
    "\n",
    "- Plot the lower bound of the cumulative regret, i.e. the cumulative regret in the best case scenario where the agent choses the optimal arm at every trial, plotted against the number of trials.\n",
    "- In the same figure, plot the upper bound of the cummulative regret, i.e. the cumulative regret in the worst case scenario where the agent choses the non-optimal arm at every trial, plotted against the number of trials.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1717168547712,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "t = np.arange(1,101) # Array representing trials from 1 to 100\n",
    "### In the best case scenario, regret is 0 at every trial, resulting in cumulative regret of zero\n",
    "cr_lb = np.zeros(100)\n",
    "### In the worst case scenario, regret is 0.5 at every trial\n",
    "regret_up = np.zeros(100) + 0.5\n",
    "cr_up = np.cumsum(regret_up)\n",
    "\n",
    "plt.plot(t,cr_lb, label = 'lower bound')\n",
    "plt.plot(t,cr_up, label = 'upper bound')\n",
    "\n",
    "plt.xlabel('trial')\n",
    "plt.ylabel('cumulative regret')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 2: Meta-learning and meta-reinforcement learning\n",
    "\n",
    "Meta-learning is a sub-field of machine learning that corresponds to a process in which an algorithm \"learns to learn\". The idea is that the algorithm will learn during training how to adapt itself to different tasks. When faced with a new task, the trained algorithm will quickly adapt to this new task without requiring additional training (the weights of the algorithm are not changed).\n",
    "\n",
    "In meta-reinforcement learning (meta-RL), this concept is applied to a reinforcement learning agent. During training, the agent is exposed to different episodes with different environment, and gains meta-learning strategies. At deployment, the agent is faced with a new environment and uses the trials from that episode (the combination of actions, states and rewards) to learn a policy on the fly. Meta-RL algorithms are typically implemented using a deep learning architecture such as an LSTM. Once they are trained, the weights of the LSTM are fixed. When faced with a new episode, the activations in the LSTM implement the nested learning in which the algorithm identifies the best arm for this specific episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Prefrontal cortex as a meta-reinforcement learning system\n",
    "\n",
    "In recent years, the field of neuroAI has opened new avenues for understanding and developing AI systems that emulate the complexity and efficiency of the human brain to better model the human brain or to gain new functionality in AI systems. An important work in this domain is the following paper:\n",
    "\n",
    "[1] Wang, Jane X., Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis, and Matthew Botvinick. **\"Prefrontal cortex as a meta-reinforcement learning system.\"** Nature neuroscience 21, no. 6 (2018): 860-868.\n",
    "\n",
    "Available at: https://www.nature.com/articles/s41593-018-0147-8\n",
    "\n",
    "This paper aims to reconciliate two seemingly disparate ideas about learning in the prefrontal cortex (PFC). The activations in the PFC have been shown to be correlated to actions, values and rewards, indicating that the PFC is engaged in RL. Another set of findings, including the fact that PFC activity is correlated with the recent history of rewards and actions, also suggests that the PFC is acting as a meta-RL agent. The paper proposes a model of the PFC, the weights of which are optimized during training to allow it to learn a second RL algorithm dynamically, as the properties of the environment changes. The second RL algorithm is implemented by the activations of the PFC, and is independent of the original training task. This allows the PFC to adapt to new situations using prior learning experiences, essentially learning to learn.\n",
    "\n",
    "We will explore here the demo in Figure 1 of the paper. Figure C belows shows the architecture of the meta-RL model, which is implemented as an LSTM. We will explore how, after training, the weights of the LSTM implement an RL procedure (e.g., we will see if this procedure deals with the exploration-exploitation tradeoff).\n",
    "\n",
    "We will also explore how learning can occur at different time scales (see figure B below):\n",
    "\n",
    "**Latent world state (fast)**: In this demo, at the start of each episode, a new set of probabilities $p_L$ and $p_R = 1-p_L$ are sampled. This corresponds to latent world state parameters that change on a slower time scale than the observations level (i.e., trials), but still at a relatively fast scale (each episode here has 100 trials).\n",
    "\n",
    "**Context (slow)**: We think of the context level as a slower time scale at which the world is changing and affecting how the parameters of the latent world state are sampled. In this paper, the authors train their meta-RL model in two contexts. In the first (independent bandit context), the parameters of the successive latent world state (i.e., $p_L$ and $p_R$ from two successive episodes) are not correlated. There is no specific knowledge about the parameters that can be generalized from one episode to the next. In the second (correlated bandit context), the parameters are anti-correlated. Thus, there is information in the parameters of the current episode that the model can use for faster learning in the next episode. We will explore how training in these two different contexts affects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1717168548113,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/learning_temporal_scales.png?raw=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Figure B - Learning across temporal scales*\n",
    "\n",
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1717168548113,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/model_architecture.png?raw=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "*Figure C - Model architecture. Adapted from Figure 1 of Wang et al. The PFC, along with the basal ganglia and directly connected thalamus, are modeled as a recurrent network. The synaptic weights are adjusted during training. a = action, r = reward, v = state value. Specifically, the algorithm is implemented as an LSTM (part b above shows the model in more detail, along with an inset of one LSTM model unit illustrating the maintenance mechanism).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Training an agent in an uncorrelated setting - exploration/exploitation tradeoff\n",
    "\n",
    "We first focus on showing how the meta-RL algorithm implements an RL procedure. To do so, we look at how this procedure balances exploration and exploitation under different episode parameters.\n",
    "\n",
    "In figure D, you can see the chosen actions of the LSTM agent across 101 different episodes. The episodes span the range of probabilities $p_L \\in \\{0,0.01,0.02,...,0.99,1\\}$. Each row corresponds to an episode, and the episodes are ordered in terms of the reward probability of the left arm. For each episode, the action taken by the agent in the 100 trials are shown. In the top half of the matrix, the optimal choice is the left arm (Arm 1), and in the bottom half, the optimal choice is the right arm (Arm 2). Here, the agent is trained in the independent bandit context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1717168548113,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/exploration_exploitation_tradeoff.png?raw=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Figure D - Exploration exploitation tradeoff. From the https://github.com/nathanwispinski/meta-rl repo which adapts the Wang et al. model. The episodes are ranked on the y-axis according to the probability of the left arm. The x-axis corresponds to the 100 successive trials in one episode. The color indicates the choice of the agent. In the lower half of the square, the right arm (Arm 2) is optimal, and in the upper half, the left arm (Arm 1) is optimal.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point 3\n",
    "\n",
    "Observe the actions of the model for different episodes. How do they vary as the probabilities vary? What sets of probabilities are typically easy to learn? And which are harder to learn?\n",
    "\n",
    "In the cells below, you can find an array in which each row corresponds to a row of Figure D. Group the rows into groups of 25 rows and average them, so you get the average arm that is selected at each time point. How do these curves showcase a longer vs shorter exploration phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Answer\n",
    "The transition from exploration to exploitation occurs more slowly in more difficult problems, as indicated in the provided figure D. In episodes with reward parameters such as 0.6 and 0.4, where the difference in reward probability between actions is smaller, it's tougher for the agent to ascertain which action is better. Hence, more exploration is needed before a confident shift to exploitation can occur.\n",
    "\n",
    "In contrast, with a larger discrepancy in rewards (e.g., 0.75 vs. 0.25), the agent can more quickly determine the more rewarding action and thus switch to exploitation sooner.\n",
    "\n",
    "Interestingly, in these examples, there are still instances in which 100 trials are not enough for the algorithm to converge on the correct arm even when $p_L$ is relatively small, and some in which the algorithm converges quickly to the right arm even though $p_L$ is close to 0.5. For a more robust estimation of the average duration of exploration, it would be useful to look at the behavior of the network for many episodes while using the same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Loading image info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1717210367452,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "normalize = mcolors.Normalize(vmin=0, vmax=11)\n",
    "colormap = cm.jet\n",
    "\n",
    "print(Trials.shape)\n",
    "\n",
    "for i in np.arange(0,100,25):\n",
    "    plt.plot(Trials[i:i+25].mean(0), color = colormap(normalize(i/10)),\n",
    "             label = 'p_L = {:.2f} - {:.2f}'.format(i*0.01,(i+25)*0.01))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Uncorrelated vs. correlated settings, seen through cumulative regret\n",
    "In the paper, the average cumulative regret (defined above) of the meta-RL agent under the independent bandit and the correlated bandits contexts is computed over 300 episodes for the setting $p_L =0.25$ and $p_R = 0.75$ (see figure E). It is compared to the cumulative regret for three common algorithms (Gittin indices, Thompson sampling and UCB, also discussed above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1717168550287,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/cummulative_regret.png?raw=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Figure E - Cumulative Regret tested on a probability setting of 0.25, 0.75. Adapted from Figure 1 of Wang et al.*\n",
    "\n",
    "Notice that the meta-RL agent trained in the correlated bandit context is able to capitalize on the information present in successive episodes and obtain a better performance than the one trained in the independent bandit context, matching the optimal performance of the Gittins indices algorithm. Both meta-RL algorithms perform better than the Thompson sampling and UCB algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### The learned model dynamics implement an RL algorithm\n",
    "\n",
    "We explore here in more depth how the activations in the LSTM implement the RL algorithm, by visualizing the top principal components (PCs) projections of the internal states of the LSTM during the different trials of an episode. Specifically, we look at the meta-RL agent trained in the correlated bandit setting (Figure F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1717168550288,
     "user": {
      "displayName": "Leila Wehbe",
      "userId": "11453012797362280188"
     },
     "user_tz": 240
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/static/evolution.png?raw=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Figure F - Evolution of model activations during different episodes. Adapted from Figure 1 of Wang et al. Each subfigure is a different episode, with the arm probabilities shown at the top. The activations at the 100 trials of an episode are projected to their top two PCs and scatterplotted using an asterix when the action decided by the model is left and a triangle when the action is right. The color indicates the number of a trial.  Adapted from Figure 1 of Wang et al.*\n",
    "\n",
    "### Discussion point 4\n",
    "In Figure F, the PCs of the model activations are obtained and the 100 trials in four episodes with specific probabilities are shown, projected onto PC1 and PC2. Observe the different patterns:\n",
    "- Which two episodes show a fast convergence to an optimal action? How does this manifest?\n",
    "- In the other two episodes, describe the trajectory of the activations, does the model change its estimation of the optimal arm? What leads to this change?\n",
    "- What does the PC1 dimension appear to represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### A hint\n",
    "\n",
    "The adaptation to fast-changing latent variables, as described in the changes in activation patterns over trials, shows the algorithm's capability to adjust its internal representations based on the feedback from the environment.\n",
    "\n",
    "- The first and last episode show a very fast convergence towards the optimal arm. The trajectory is very fast towards one or the other side of the PC space.\n",
    "\n",
    "- In the middle two episodes, it takes longer to converge (they also correspond to more difficult settings). The model actually first decide on left, but then after sampling a right action, end up converging on right.\n",
    "\n",
    "- The first PC appears to correspond to the certainty of the algorithm about the optimal arm (low PC value = left arm, high PC value = right arm).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
