{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d34f18-51d4-46f6-b002-6e6d80efec7e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W2D5_Mysteries/student/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/blob/main/tutorials/W2D5_Mysteries/student/W2D5_Tutorial1.ipynb\"  target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed61a3-87d2-4e76-83f6-4b786c101af2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Consciousness\n",
    "\n",
    "**Week 2, Day 5: Mysteries**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Samuele Bolotta, Steve Fleming, Juan David Vargas, Guillaume Dumas\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366c740-80c7-4545-b688-d547ef79613e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "\n",
    "1. Understand and distinguish various aspects of consciousness including the hard problem of consciousness, the difference between phenomenal consciousness and access consciousness, as well as the distinctions between consciousness and sentience or intelligence\n",
    "\n",
    "2. Explore core frameworks for analyzing consciousness, including diagnostic criteria, and will compare objective probabilities with subjective credences\n",
    "\n",
    "3. Delve into reductionist theories of consciousness, such as Global Workspace Theory (GWT), theories of metacognition, and Higher-Order Thought (HOT) theories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4fa9f7-d9cc-4594-be41-b31f02af1bd4",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "link_id = \"wm9dp\"\n",
    "\n",
    "print(f\"If you want to download the slides: 'https://osf.io/download/{link_id}'\")\n",
    "\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d1a7f-452c-436d-90a8-409571405e62",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762c382-3622-4178-8e19-da783bac0a57",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "# !pip install numpy matplotlib Pillow torch torchvision transformers ipywidgets gradio trdg scikit-learn networkx pickleshare seaborn tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b676d-d8de-41ad-80c6-3516e25f0fba",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown Enhanced organization and clarity in import statements for better readability\n",
    "\n",
    "# Standard Libraries\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "\n",
    "# Data Handling and Visualization Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Scientific Computing and Statistical Libraries\n",
    "from numpy.linalg import inv\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "from torch import nn, optim, save, load\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Image Processing Libraries\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Interactive Elements and Web Applications\n",
    "from IPython.display import IFrame\n",
    "import gradio as gr\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# Graph Analysis Libraries\n",
    "import networkx as nx\n",
    "\n",
    "# Progress Monitoring Libraries\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utilities and Miscellaneous Libraries\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960e64d-fe33-4276-8da0-e450e2649bcc",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497f919-a995-4704-81cc-13799ba5c6ba",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def initialize_global():\n",
    "    global Input_Size_1, Hidden_Size_1, Output_Size_1, Input_Size_2\n",
    "    global num_units, patterns_number\n",
    "    global learning_rate_1, learning_rate_2, n_epochs, momentum, temperature , Threshold\n",
    "    global First_set, Second_set, Third_set\n",
    "    global First_set_targets, Second_set_targets, Third_set_targets\n",
    "    global epoch_list, epoch_1_order, epoch_2_order, patterns_matrix1\n",
    "    global Testing_graph_names\n",
    "\n",
    "    # Network sizes\n",
    "    Input_Size_1 = 100\n",
    "    Hidden_Size_1 = 60\n",
    "    Output_Size_1 = 100\n",
    "    Input_Size_2 = 100\n",
    "\n",
    "    # Patterns\n",
    "    num_units = 100\n",
    "    patterns_number = 200\n",
    "\n",
    "    # Pre-training and hyperparameters\n",
    "    learning_rate_1 = 0.9\n",
    "    learning_rate_2 = 0.1\n",
    "    n_epochs = 200\n",
    "    momentum = 0.0\n",
    "    temperature = 1.0\n",
    "    Threshold=0.5\n",
    "\n",
    "    # Testing\n",
    "    First_set = []\n",
    "    Second_set = []\n",
    "    Third_set = []\n",
    "    First_set_targets = []\n",
    "    Second_set_targets = []\n",
    "    Third_set_targets = []\n",
    "\n",
    "    Testing_graph_names = [\"Suprathreshold stimulus\", \"Subthreshold stimulus\", \"Low Vision\"]\n",
    "\n",
    "\n",
    "    # Graphic of pretraining\n",
    "    epoch_list = list(range(1, n_epochs + 1))\n",
    "    epoch_1_order = np.zeros(n_epochs)\n",
    "    epoch_2_order = np.zeros(n_epochs)\n",
    "    patterns_matrix1 =  torch.zeros((n_epochs, patterns_number), device=device)  # Initialize patterns_matrix as a PyTorch tensor on the GPU\n",
    "\n",
    "class SecondOrderNetwork(nn.Module):\n",
    "    def __init__(self, use_gelu):\n",
    "        super(SecondOrderNetwork, self).__init__()\n",
    "        # Define a linear layer for comparing the difference between input and output of the first-order network\n",
    "        self.comparison_layer = nn.Linear(100, 100)\n",
    "\n",
    "        # Linear layer for determining wagers, mapping from 100 features to a single output\n",
    "        self.wager = nn.Linear(100, 1)\n",
    "\n",
    "        # Dropout layer to prevent overfitting by randomly setting input units to 0 with a probability of 0.3 during training\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Select activation function based on the `use_gelu` flag\n",
    "        self.activation = torch.nn.GELU() if use_gelu else torch.relu\n",
    "\n",
    "        # Additional activation functions for potential use in network operations\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.softmax = torch.nn.Softmax(dim=1)  # Softmax for multi-class classification problems\n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "        # Initialize the weights of the network\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Uniformly initialize weights for the comparison and wager layers\n",
    "        init.uniform_(self.comparison_layer.weight, -1.0, 1.0)\n",
    "        init.uniform_(self.wager.weight, 0.0, 0.1)\n",
    "\n",
    "    def forward(self, first_order_input, first_order_output):\n",
    "        # Calculate the difference between the first-order input and output\n",
    "        comparison_matrix = first_order_input - first_order_output\n",
    "\n",
    "        # Pass the difference through the comparison layer and apply the chosen activation function\n",
    "        comparison_out = self.comparison_layer(comparison_matrix)\n",
    "\n",
    "        # Calculate the wager value, applying dropout and sigmoid activation to the output of the wager layer\n",
    "        wager = self.dropout(self.sigmoid(self.wager(comparison_out)))\n",
    "\n",
    "        return wager\n",
    "\n",
    "def compute_metrics(TP, FP, FN):\n",
    "    \"\"\"Compute precision, recall, and F1 score.\"\"\"\n",
    "    precision = round(TP / (TP + FP), 2) if (TP + FP) > 0 else 0\n",
    "    recall = round(TP / (TP + FN), 2) if (TP + FN) > 0 else 0\n",
    "    f1_score = round(2 * (precision * recall) / (precision + recall), 2) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def create_patterns(stimulus):\n",
    "    \"\"\"\n",
    "    Generates neural network input patterns based on specified stimulus conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - stimulus (int): Determines the type of patterns to generate.\n",
    "                      Acceptable values:\n",
    "                      - 0: Suprathreshold stimulus\n",
    "                      - 1: Subthreshold stimulus\n",
    "                      - 2: Low vision condition\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of generated patterns.\n",
    "    - torch.Tensor: Tensor of target values corresponding to the generated patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate initial patterns and target tensors for base condition.\n",
    "    patterns_tensor, stim_present_tensor, stim_absent_tensor, order_2_tensor = Generate_Patterns(patterns_number, num_units)\n",
    "\n",
    "    if stimulus == 0:  # Suprathreshold stimulus condition\n",
    "        # Convert pattern tensors for processing on specified device (CPU/GPU).\n",
    "        patterns = torch.Tensor(patterns_tensor).to(device)\n",
    "        targets = torch.Tensor(stim_present_tensor).to(device)\n",
    "\n",
    "    elif stimulus == 1:  # Subthreshold stimulus condition, simulating blindsight with added noise\n",
    "        patterns, targets = generate_subthreshold_patterns(patterns_number, num_units, device)\n",
    "\n",
    "    elif stimulus == 2:  # Low vision condition, reducing stimulus activation\n",
    "        patterns, targets = generate_low_vision_patterns(patterns_number, num_units, device)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stimulus ID for testing patterns creation.\")\n",
    "\n",
    "    return patterns, targets\n",
    "\n",
    "def generate_subthreshold_patterns(patterns_number, num_units, device):\n",
    "    \"\"\"\n",
    "    Generates patterns and targets for the subthreshold stimulus condition by adding noise.\n",
    "\n",
    "    Parameters:\n",
    "    - patterns_number (int): Number of patterns to generate.\n",
    "    - num_units (int): Number of units in each pattern.\n",
    "    - device: The device (CPU/GPU) for tensor operations.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two lists of torch.Tensor: (patterns, targets)\n",
    "    \"\"\"\n",
    "    patterns, targets = [], []\n",
    "    for i in range(patterns_number):\n",
    "        pattern = np.random.uniform(0.0, 0.02, num_units) + 0.0012  # Base pattern with noise\n",
    "        if i >= 100:\n",
    "            stimulus_number = random.randint(0, 99)  # Selecting unit for stimulus\n",
    "            pattern[stimulus_number] = np.random.uniform(0.0, 1.0) + 0.0012\n",
    "            present = np.zeros(num_units)\n",
    "            present[stimulus_number] = 1.0 if pattern[stimulus_number] >= 0.5 else 0.0\n",
    "            targets.append(present)\n",
    "        patterns.append(pattern)\n",
    "    # Convert lists to tensors for device processing.\n",
    "    return torch.Tensor(patterns).to(device), torch.Tensor(targets).to(device)\n",
    "\n",
    "def generate_low_vision_patterns(patterns_number, num_units, device):\n",
    "    \"\"\"\n",
    "    Generates patterns and targets for the low vision condition by reducing stimulus activation.\n",
    "\n",
    "    Parameters:\n",
    "    - patterns_number (int): Number of patterns to generate.\n",
    "    - num_units (int): Number of units in each pattern.\n",
    "    - device: The device (CPU/GPU) for tensor operations.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two lists of torch.Tensor: (patterns, targets)\n",
    "    \"\"\"\n",
    "    patterns, targets = [], []\n",
    "    for i in range(patterns_number):\n",
    "        pattern = np.random.uniform(0.0, 0.02, num_units)\n",
    "        if i >= 100:\n",
    "            stimulus_number = random.randint(0, 99)\n",
    "            pattern[stimulus_number] = np.random.uniform(0.0, 0.3)\n",
    "            present = np.zeros(num_units)\n",
    "            present[stimulus_number] = 1.0 if pattern[stimulus_number] >= 0.15 else 0.0\n",
    "            targets.append(present)\n",
    "        patterns.append(pattern)\n",
    "    # Convert lists to tensors for device processing.\n",
    "    return torch.Tensor(patterns).to(device), torch.Tensor(targets).to(device)\n",
    "\n",
    "  #define the architecture, optimizers, loss functions, and schedulers for pre training\n",
    "def prepare_pre_training(hidden,factor,gelu,stepsize, gam):\n",
    "\n",
    "  first_order_network = FirstOrderNetwork(hidden,factor,gelu).to(device)\n",
    "  second_order_network = SecondOrderNetwork(gelu).to(device)\n",
    "\n",
    "  criterion_1 = nn.MSELoss()\n",
    "  criterion_2 = nn.MSELoss()\n",
    "\n",
    "  optimizer_1 = optim.SGD(first_order_network.parameters(), lr=learning_rate_1, momentum=momentum)\n",
    "  optimizer_2 = optim.SGD(second_order_network.parameters(), lr=learning_rate_2, momentum=momentum)\n",
    "\n",
    "\n",
    "  # Learning rate schedulers\n",
    "  scheduler_1 = StepLR(optimizer_1, step_size=stepsize, gamma=gam)\n",
    "  scheduler_2 = StepLR(optimizer_2, step_size=stepsize, gamma=gam)\n",
    "\n",
    "  return first_order_network, second_order_network, criterion_1, criterion_2, optimizer_1, optimizer_2, scheduler_1, scheduler_2\n",
    "\n",
    "def title(string):\n",
    "  #plot the title of the currently trained model, inside a rectangle\n",
    "  fig, ax = plt.subplots()\n",
    "  rectangle = patches.Rectangle((0.05, 0.1), 0.9 , 0.4, linewidth=1, edgecolor='r', facecolor='blue', alpha=0.5)\n",
    "  ax.add_patch(rectangle)\n",
    "  plt.text(0.5, 0.3, string , horizontalalignment='center', verticalalignment='center', fontsize=26, color='white')\n",
    "  ax.set_xlim(0, 1)\n",
    "  ax.set_ylim(0, 1)\n",
    "  ax.axis('off')\n",
    "  plt.show()\n",
    "  plt.close(fig)\n",
    "\n",
    "# Function to configure the training environment and load the models\n",
    "def config_training(first_order_network, second_order_network, hidden, factor, gelu):\n",
    "    \"\"\"\n",
    "    Configures the training environment by saving the state of the given models and loading them back.\n",
    "    Initializes testing patterns for evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - first_order_network: The first order network instance.\n",
    "    - second_order_network: The second order network instance.\n",
    "    - hidden: Number of hidden units in the first order network.\n",
    "    - factor: Factor influencing the network's architecture.\n",
    "    - gelu: Activation function to be used in the network.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of testing patterns, number of samples in the testing patterns, and the loaded model instances.\n",
    "    \"\"\"\n",
    "    # Paths where the models' states will be saved\n",
    "    PATH = './cnn1.pth'\n",
    "    PATH_2 = './cnn2.pth'\n",
    "\n",
    "    # Save the weights of the pretrained networks to the specified paths\n",
    "    torch.save(first_order_network.state_dict(), PATH)\n",
    "    torch.save(second_order_network.state_dict(), PATH_2)\n",
    "\n",
    "    # Generating testing patterns for three different sets\n",
    "    First_set, First_set_targets = create_patterns(0)\n",
    "    Second_set, Second_set_targets = create_patterns(1)\n",
    "    Third_set, Third_set_targets = create_patterns(2)\n",
    "\n",
    "    # Aggregate testing patterns and their targets for ease of access\n",
    "    Testing_patterns = [[First_set, First_set_targets], [Second_set, Second_set_targets], [Third_set, Third_set_targets]]\n",
    "\n",
    "    # Determine the number of samples from the first set (assumed consistent across all sets)\n",
    "    n_samples = len(Testing_patterns[0][0])\n",
    "\n",
    "    # Initialize and load the saved states into model instances\n",
    "    loaded_model = FirstOrderNetwork(hidden, factor, gelu)\n",
    "    loaded_model_2 = SecondOrderNetwork(gelu)\n",
    "\n",
    "    loaded_model.load_state_dict(torch.load(PATH))\n",
    "    loaded_model_2.load_state_dict(torch.load(PATH_2))\n",
    "\n",
    "    # Ensure the models are moved to the appropriate device (CPU/GPU) and set to evaluation mode\n",
    "    loaded_model.to(device)\n",
    "    loaded_model_2.to(device)\n",
    "\n",
    "    loaded_model.eval()\n",
    "    loaded_model_2.eval()\n",
    "\n",
    "    return Testing_patterns, n_samples, loaded_model, loaded_model_2\n",
    "\n",
    "# Function to test the model using the configured testing patterns\n",
    "def testing(Testing_patterns, n_samples, loaded_model, loaded_model_2):\n",
    "    \"\"\"\n",
    "    Tests the model on provided testing patterns and calculates F1 scores and other metrics for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - Testing_patterns: A list of testing datasets and their targets.\n",
    "    - n_samples: Number of samples in each testing dataset.\n",
    "    - loaded_model: The loaded first order network model for testing.\n",
    "    - loaded_model_2: The loaded second order network model for testing.\n",
    "\n",
    "    Returns:\n",
    "    - F1 scores for high and low wager scenarios, and results for plotting.\n",
    "    \"\"\"\n",
    "    results_for_plotting = []\n",
    "\n",
    "    f1_scores_high_wager = []\n",
    "    f1_scores_low_wager = []\n",
    "\n",
    "    # Iterate through each set of testing patterns and targets\n",
    "    for i in range(len(Testing_patterns)):\n",
    "        with torch.no_grad():  # Ensure no gradients are computed during testing\n",
    "            # Obtain output from the first order model\n",
    "            output_first_order = loaded_model(Testing_patterns[i][0])\n",
    "            # Focus on the last 100 elements for wagering analysis\n",
    "            last_100_elements_wager = output_first_order[-100:].cpu()\n",
    "            max_values = last_100_elements_wager.max(dim=1).values\n",
    "\n",
    "            _, targets_2 = torch.max(Testing_patterns[i][1], 1)\n",
    "            targets_2 = targets_2[-100:].cpu()\n",
    "            # Convert targets to binary classification for wagering scenario\n",
    "            targets_2 = (targets_2 > 0).int()\n",
    "\n",
    "            # Convert tensors to NumPy arrays for metric calculations\n",
    "            predicted_np = max_values.numpy()\n",
    "            targets_2_np = targets_2.numpy()\n",
    "\n",
    "            # Calculate True Positives, True Negatives, False Positives, and False Negatives\n",
    "            TP = np.sum((predicted_np > 0.5) & (targets_2_np > 0.5))\n",
    "            TN = np.sum((predicted_np < 0.5) & (targets_2_np < 0.5))\n",
    "            FP = np.sum((predicted_np > 0.5) & (targets_2_np < 0.5))\n",
    "            FN = np.sum((predicted_np < 0.5) & (targets_2_np > 0.5))\n",
    "\n",
    "            # Compute precision, recall, and F1 score for both high and low wager scenarios\n",
    "            precision_h, recall_h, f1_score_h = compute_metrics(TP, FP, FN)\n",
    "            precision_l, recall_l, f1_score_l = compute_metrics(TP, FP, FN)\n",
    "\n",
    "            f1_scores_high_wager.append(f1_score_h)\n",
    "            f1_scores_low_wager.append(f1_score_l)\n",
    "\n",
    "            # Collect results for plotting\n",
    "            results_for_plotting.append({\n",
    "                \"counts\": [[TP, FP, TP + FP], [FN, TN, FN + TN], [TP + FN, FP + TN, TP + FP + FN + TN]],\n",
    "                \"metrics\": [[precision_h, recall_h, f1_score_h], [precision_l, recall_l, f1_score_l]],\n",
    "                \"title_results\": f\"Results Table - Set {i+1}\",\n",
    "                \"title_metrics\": f\"Metrics Table - Set {i+1}\"\n",
    "            })\n",
    "\n",
    "    return f1_scores_high_wager, f1_scores_low_wager, results_for_plotting\n",
    "\n",
    "def Generate_Patterns(patterns_number, num_units):\n",
    "    # Generates patterns and targets for training the networks\n",
    "    # patterns_number: Number of patterns to generate\n",
    "    # num_units: Number of units in each pattern\n",
    "    # Returns lists of patterns, stimulus present/absent indicators, and second order targets\n",
    "\n",
    "    patterns = []  # Store generated patterns\n",
    "    stim_present = []  # Indicators for when a stimulus is present in the pattern\n",
    "    stim_absent = []  # Indicators for when no stimulus is present\n",
    "    order_2_pr = []  # Second order network targets based on the presence or absence of stimulus\n",
    "\n",
    "    # Generate patterns, half noise and half potential stimuli\n",
    "    for i in range(patterns_number):\n",
    "        # First half: Noise patterns\n",
    "        if i < patterns_number // 2:\n",
    "            pattern = np.random.uniform(0.0, 0.02, num_units)  # Generate a noise pattern\n",
    "            patterns.append(pattern)\n",
    "            stim_present.append(np.zeros(num_units))  # Stimulus absent\n",
    "            stim_absent.append(np.zeros(num_units))  # Redundant, consider removing\n",
    "            order_2_pr.append([0.0 , 1.0])  # No stimulus, low wager\n",
    "        # Second half: Stimulus patterns\n",
    "        else:\n",
    "            stimulus_number = random.randint(0, num_units - 1)  # Choose a unit for potential stimulus\n",
    "            pattern = np.random.uniform(0.0, 0.02, num_units)\n",
    "            pattern[stimulus_number] = np.random.uniform(0.0, 1.0)  # Set stimulus intensity\n",
    "            patterns.append(pattern)\n",
    "            present = np.zeros(num_units)\n",
    "            # Determine if stimulus is above discrimination threshold\n",
    "            if pattern[stimulus_number] >= 0.5:\n",
    "                order_2_pr.append([1.0 , 0.0])  # Stimulus detected, high wager\n",
    "                present[stimulus_number] = 1.0\n",
    "            else:\n",
    "                order_2_pr.append([0.0 , 1.0])  # Stimulus not detected, low wager\n",
    "                present[stimulus_number] = 0.0\n",
    "            stim_present.append(present)\n",
    "            stim_absent.append(np.zeros(num_units))  # Redundant, consider removing\n",
    "\n",
    "    patterns_tensor = torch.Tensor(patterns).to(device)\n",
    "    stim_present_tensor = torch.Tensor(stim_present).to(device)\n",
    "    stim_absent_tensor= torch.Tensor(stim_absent).to(device)\n",
    "    order_2_tensor = torch.Tensor(order_2_pr).to(device)\n",
    "\n",
    "    return patterns_tensor, stim_present_tensor , stim_absent_tensor, order_2_tensor\n",
    "\n",
    "def create_patterns(stimulus):\n",
    "    \"\"\"\n",
    "    Generates neural network input patterns based on specified stimulus conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - stimulus (int): Determines the type of patterns to generate.\n",
    "                      Acceptable values:\n",
    "                      - 0: Suprathreshold stimulus\n",
    "                      - 1: Subthreshold stimulus\n",
    "                      - 2: Low vision condition\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of generated patterns.\n",
    "    - torch.Tensor: Tensor of target values corresponding to the generated patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate initial patterns and target tensors for base condition.\n",
    "    patterns_tensor, stim_present_tensor, stim_absent_tensor, order_2_tensor = Generate_Patterns(patterns_number, num_units)\n",
    "\n",
    "    if stimulus == 0:  # Suprathreshold stimulus condition\n",
    "        # Convert pattern tensors for processing on specified device (CPU/GPU).\n",
    "        patterns = torch.Tensor(patterns_tensor).to(device)\n",
    "        targets = torch.Tensor(stim_present_tensor).to(device)\n",
    "\n",
    "    elif stimulus == 1:  # Subthreshold stimulus condition, simulating blindsight with added noise\n",
    "        patterns, targets = generate_subthreshold_patterns(patterns_number, num_units, device)\n",
    "\n",
    "    elif stimulus == 2:  # Low vision condition, reducing stimulus activation\n",
    "        patterns, targets = generate_low_vision_patterns(patterns_number, num_units, device)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stimulus ID for testing patterns creation.\")\n",
    "\n",
    "    return patterns, targets\n",
    "\n",
    "def generate_subthreshold_patterns(patterns_number, num_units, device):\n",
    "    \"\"\"\n",
    "    Generates patterns and targets for the subthreshold stimulus condition by adding noise.\n",
    "\n",
    "    Parameters:\n",
    "    - patterns_number (int): Number of patterns to generate.\n",
    "    - num_units (int): Number of units in each pattern.\n",
    "    - device: The device (CPU/GPU) for tensor operations.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two lists of torch.Tensor: (patterns, targets)\n",
    "    \"\"\"\n",
    "    patterns, targets = [], []\n",
    "    for i in range(patterns_number):\n",
    "        pattern = np.random.uniform(0.0, 0.02, num_units) + 0.0012  # Base pattern with noise\n",
    "        if i >= 100:\n",
    "            stimulus_number = random.randint(0, 99)  # Selecting unit for stimulus\n",
    "            pattern[stimulus_number] = np.random.uniform(0.0, 1.0) + 0.0012\n",
    "            present = np.zeros(num_units)\n",
    "            present[stimulus_number] = 1.0 if pattern[stimulus_number] >= 0.5 else 0.0\n",
    "            targets.append(present)\n",
    "        patterns.append(pattern)\n",
    "    # Convert lists to tensors for device processing.\n",
    "    return torch.Tensor(patterns).to(device), torch.Tensor(targets).to(device)\n",
    "\n",
    "def generate_low_vision_patterns(patterns_number, num_units, device):\n",
    "    \"\"\"\n",
    "    Generates patterns and targets for the low vision condition by reducing stimulus activation.\n",
    "\n",
    "    Parameters:\n",
    "    - patterns_number (int): Number of patterns to generate.\n",
    "    - num_units (int): Number of units in each pattern.\n",
    "    - device: The device (CPU/GPU) for tensor operations.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two lists of torch.Tensor: (patterns, targets)\n",
    "    \"\"\"\n",
    "    patterns, targets = [], []\n",
    "    for i in range(patterns_number):\n",
    "        pattern = np.random.uniform(0.0, 0.02, num_units)\n",
    "        if i >= 100:\n",
    "            stimulus_number = random.randint(0, 99)\n",
    "            pattern[stimulus_number] = np.random.uniform(0.0, 0.3)\n",
    "            present = np.zeros(num_units)\n",
    "            present[stimulus_number] = 1.0 if pattern[stimulus_number] >= 0.15 else 0.0\n",
    "            targets.append(present)\n",
    "        patterns.append(pattern)\n",
    "    # Convert lists to tensors for device processing.\n",
    "    return torch.Tensor(patterns).to(device), torch.Tensor(targets).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d683a-d65e-4395-ae17-a3b77715e44a",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def plot_signal_max_and_indicator(patterns_tensor, plot_title=\"Training Signals\"):\n",
    "    \"\"\"\n",
    "    Plots the maximum values of signal units and a binary indicator for max values greater than 0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - patterns_tensor: A tensor containing signals, where each signal is expected to have multiple units.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the maximum value of units for each signal within the patterns tensor\n",
    "    max_values_of_units = patterns_tensor.max(dim=1).values.cpu().numpy()  # Ensure it's on CPU and in NumPy format for plotting\n",
    "\n",
    "    # Determine the binary indicators based on the max value being greater than 0.5\n",
    "    binary_indicators = (max_values_of_units > 0.5).astype(int)\n",
    "\n",
    "    # Create a figure with 2 subplots (2 rows, 1 column)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "    fig.suptitle(plot_title, fontsize=16)  # Set the overall title for the plot\n",
    "\n",
    "    # First subplot for the maximum values of each signal\n",
    "    axs[0].plot(range(patterns_tensor.size(0)), max_values_of_units, drawstyle='steps-mid')\n",
    "    axs[0].set_xlabel('Signal Number')\n",
    "    axs[0].set_ylabel('Max Value of Signal Units')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Second subplot for the binary indicators\n",
    "    axs[1].plot(range(patterns_tensor.size(0)), binary_indicators, drawstyle='steps-mid', color='red')\n",
    "    axs[1].set_xlabel('Signal Number')\n",
    "    axs[1].set_ylabel('Indicator (Max > 0.5) in each signal')\n",
    "    axs[1].set_ylim(-0.1, 1.1)  # Adjust y-axis limits for clarity\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pre_train_plots(epoch_1_order, epoch_2_order, title):\n",
    "    # LOSS PLOTS\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # Adjust the figure size as needed\n",
    "\n",
    "    # First graph for 1st Order Network\n",
    "    ax1.plot(epoch_list, epoch_1_order, linestyle='--', marker='o', color='g')\n",
    "    ax1.set_title('1st Order Network Loss')\n",
    "    ax1.set_xlabel('Epochs - Pretraining Phase')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    # Second graph for 2nd Order Network\n",
    "    ax2.plot(epoch_list, epoch_2_order, linestyle='--', marker='o', color='b')\n",
    "    ax2.set_title('2nd Order Network Loss')\n",
    "    ax2.set_xlabel('Epochs - Pretraining Phase')\n",
    "    ax2.set_ylabel('Loss')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16, y=0.95)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Blindsight_Pre_training_Loss_{}.png'.format(title.replace(\" \", \"_\").replace(\"/\", \"_\")), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_testing(results_for_plotting , title):\n",
    "    \"\"\"Plot a table focused on 'PRECISION', 'RECALL', 'F1 SCORE', indicating consciousness level and testing scenario.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, len(results_for_plotting) * 2 + 2))  # Adjusted for added header space\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Define column labels\n",
    "    col_labels = [\"Scenario\", \"Consciousness Level\", \"PRECISION\", \"RECALL\", \"F1 SCORE\"]\n",
    "\n",
    "    # Initialize list to hold all rows of data including headers\n",
    "    full_data = []\n",
    "\n",
    "    # Process each result to extract and prepare the data rows\n",
    "    for i, result in enumerate(results_for_plotting):\n",
    "        for level, metrics in zip([\"High Consciousness\", \"Low Consciousness\"], result[\"metrics\"]):\n",
    "            row = [Testing_graph_names[i], level] + metrics\n",
    "            full_data.append(row)\n",
    "\n",
    "    # Extract metric values for color scaling (excluding the first two columns which are text)\n",
    "    metric_values = np.array(full_data)[:, 2:].astype(float)  # Convert to float for color scaling\n",
    "    max_value = np.max(metric_values)\n",
    "    colors = metric_values / max_value  # Normalize for color mapping\n",
    "\n",
    "    # Prepare colors for all cells, defaulting to white for non-metric cells\n",
    "    cell_colors = [[\"white\", \"white\"] + list(row) for row in plt.cm.RdYlGn(colors)]\n",
    "\n",
    "    # Create the table with cell colors\n",
    "    table = ax.table(cellText=full_data, colLabels=col_labels, loc='center', cellLoc='center', cellColours=cell_colors)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)\n",
    "\n",
    "    plt.title(title, pad=20, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f79032-091e-4f19-88b6-7fb797a1cc31",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "# @markdown\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4a1ee-afa6-4e4a-9cc6-081d802e43de",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Global Neural Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6977aa-7911-4d54-be25-cea647bdc73d",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92d437-a179-45a1-b11f-2f593ded647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'v1jaOTisBGM'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734a181-77f8-4e19-8674-10904df84761",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1a: Modularity Of The Mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b2098-9d42-4060-a470-fad0d748faac",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The crucial idea behind this section is that machine learning aims to capture the modular structure of the physical world, where complexity emerges from simpler, independently evolving subsystems. This concept aligns with causal inference, suggesting that understanding and modeling the world involves identifying and integrating these autonomous mechanisms. These mechanisms, which interact sparsely, maintain their functionality even amidst changes in others, highlighting their robustness. Recurrent Independent Mechanisms (RIMs) embody this principle by operating mostly independently, occasionally interacting through an attention-based mechanism for efficient and dynamic information processing. This approach [https://arxiv.org/pdf/1909.10893.pdf] suggests a preference for models that can capture the independence and sparse interactions of mechanisms, potentially leading to more adaptable and generalizable AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3e1d3-7be8-460b-ad89-17c96274cb2c",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "# URL of the repository to clone\n",
    "!git clone https://github.com/SamueleBolotta/RIMs-Sequential-MNIST\n",
    "%cd RIMs-Sequential-MNIST\n",
    "\n",
    "# Imports\n",
    "from data import MnistData\n",
    "from networks import MnistModel, LSTM\n",
    "\n",
    "# Function to download files\n",
    "def download_file(url, destination):\n",
    "    print(f\"Starting to download {url} to {destination}\")\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    open(destination, 'wb').write(response.content)\n",
    "    print(f\"Successfully downloaded {url} to {destination}\")\n",
    "\n",
    "# Path of the models\n",
    "model_path = {\n",
    "    'LSTM': 'lstm_model_dir/lstm_best_model.pt',\n",
    "    'RIM': 'rim_model_dir/best_model.pt'\n",
    "}\n",
    "\n",
    "# URLs of the models\n",
    "model_urls = {\n",
    "    'LSTM': 'https://osf.io/4gajq/download',\n",
    "    'RIM': 'https://osf.io/3squn/download'\n",
    "}\n",
    "\n",
    "# Check if model files exist, if not, download them\n",
    "for model_key, model_url in model_urls.items():\n",
    "    if not os.path.exists(model_path[model_key]):\n",
    "        download_file(model_url, model_path[model_key])\n",
    "        print(f\"{model_key} model downloaded.\")\n",
    "    else:\n",
    "        print(f\"{model_key} model already exists. No download needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85190a-f57e-41c8-8d4a-3ddc658ae045",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## RIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb7131-c719-47de-98bf-9cfdb59f1abc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The model has been trained on the Sequential MNIST datset with individual image size 14x14. To assess its generalization capabilities, it got tested on 16x16 (Validation Set 3), 19x19 (Validation Set 2) and 24x24 images (Validation Set 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad179715-7c6f-455b-aea4-7b7860430c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "config = {\n",
    "    'cuda': True,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 100,\n",
    "    'input_size': 1,\n",
    "    'model': 'RIM', # Or 'RIM' for the MnistModel\n",
    "    'train': False, # Set to False to load the saved model\n",
    "    'num_units': 6,\n",
    "    'rnn_cell': 'LSTM',\n",
    "    'key_size_input': 64,\n",
    "    'value_size_input': 400,\n",
    "    'query_size_input': 64,\n",
    "    'num_input_heads': 1,\n",
    "    'num_comm_heads': 4,\n",
    "    'input_dropout': 0.1,\n",
    "    'comm_dropout': 0.1,\n",
    "    'key_size_comm': 32,\n",
    "    'value_size_comm': 100,\n",
    "    'query_size_comm': 32,\n",
    "    'k': 4,\n",
    "    'size': 14,\n",
    "    'loadsaved': 1, # Ensure this is 1 to load saved model\n",
    "    'log_dir': 'rim_model_dir'\n",
    "}\n",
    "\n",
    "# Choose the model\n",
    "model = MnistModel(config)  # Instantiating MnistModel (RIM) with config\n",
    "model_directory = model_path['RIM']\n",
    "\n",
    "# Set device\n",
    "device = set_device()\n",
    "model.to(device)\n",
    "\n",
    "# Set the map_location based on whether CUDA is available\n",
    "map_location = 'cuda' if torch.cuda.is_available() and config['cuda'] else 'cpu'\n",
    "\n",
    "# Use torch.load with the map_location parameter\n",
    "saved = torch.load(model_directory, map_location=map_location)\n",
    "model.load_state_dict(saved['net'])\n",
    "\n",
    "# Data\n",
    "data = MnistData(config['batch_size'], (config['size'], config['size']), config['k'])\n",
    "\n",
    "# Evaluation function\n",
    "def test_model(model, loader, func):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Total validation samples: {loader.val_len()}\")  # Print total number of validation samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(loader.val_len())):\n",
    "            test_x, test_y = func(i)\n",
    "            test_x = model.to_device(test_x)\n",
    "            test_y = model.to_device(test_y).long()\n",
    "            probs  = model(test_x)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            correct = preds == test_y\n",
    "            accuracy += correct.sum().item()\n",
    "\n",
    "    accuracy /= 100  # Use the total number of items in the validation set for accuracy calculation\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate on all three validation sets\n",
    "validation_functions = [data.val_get1, data.val_get2, data.val_get3]\n",
    "validation_accuracies_rim = []\n",
    "\n",
    "print(f\"Model: {config['model']}, Device: {device}\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "for func in validation_functions:\n",
    "    accuracy = test_model(model, data, func)\n",
    "    validation_accuracies_rim.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303ab3d-10e3-4f17-98cd-d1cc8bec2244",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7acc2d-4aa5-4e4c-82ae-43ec4a3c2bd8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's now repeat the same process with LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceb84b-e550-48ed-a612-a5e5155ebf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "config = {\n",
    "    'cuda': True,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 100,\n",
    "    'input_size': 1,\n",
    "    'model': 'LSTM',\n",
    "    'train': False, # Set to False to load the saved model\n",
    "    'num_units': 6,\n",
    "    'rnn_cell': 'LSTM',\n",
    "    'key_size_input': 64,\n",
    "    'value_size_input': 400,\n",
    "    'query_size_input': 64,\n",
    "    'num_input_heads': 1,\n",
    "    'num_comm_heads': 4,\n",
    "    'input_dropout': 0.1,\n",
    "    'comm_dropout': 0.1,\n",
    "    'key_size_comm': 32,\n",
    "    'value_size_comm': 100,\n",
    "    'query_size_comm': 32,\n",
    "    'k': 4,\n",
    "    'size': 14,\n",
    "    'loadsaved': 1, # Ensure this is 1 to load saved model\n",
    "    'log_dir': 'rim_model_dir'\n",
    "}\n",
    "\n",
    "model = LSTM(config)  # Instantiating LSTM with config\n",
    "model_directory = model_path['LSTM']\n",
    "\n",
    "# Set device\n",
    "device = set_device()\n",
    "model.to(device)\n",
    "\n",
    "# Set the map_location based on whether CUDA is available\n",
    "map_location = 'cuda' if torch.cuda.is_available() and config['cuda'] else 'cpu'\n",
    "\n",
    "# Use torch.load with the map_location parameter\n",
    "saved = torch.load(model_directory, map_location=map_location)\n",
    "model.load_state_dict(saved['net'])\n",
    "\n",
    "# Data\n",
    "data = MnistData(config['batch_size'], (config['size'], config['size']), config['k'])\n",
    "\n",
    "# Evaluation function\n",
    "def test_model(model, loader, func):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Total validation samples: {loader.val_len()}\")  # Print total number of validation samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(loader.val_len())):\n",
    "            test_x, test_y = func(i)\n",
    "            test_x = model.to_device(test_x)\n",
    "            test_y = model.to_device(test_y).long()\n",
    "            probs  = model(test_x)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            correct = preds == test_y\n",
    "            accuracy += correct.sum().item()\n",
    "\n",
    "    accuracy /= 100  # Use the total number of items in the validation set for accuracy calculation\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate on all three validation sets\n",
    "validation_functions = [data.val_get1, data.val_get2, data.val_get3]\n",
    "validation_accuracies_lstm = []\n",
    "\n",
    "print(f\"Model: {config['model']}, Device: {device}\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "for func in validation_functions:\n",
    "    accuracy = test_model(model, data, func)\n",
    "    validation_accuracies_lstm.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b188f-b335-4ffb-b723-4782aa18af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracies for all validation sets (RIMs)\n",
    "for i, accuracy in enumerate(validation_accuracies_rim, 1):\n",
    "    print(f'Validation Set {i} Accuracy (RIMs): {accuracy:.2f}%')\n",
    "\n",
    "# Print accuracies for all validation sets (LSTM)\n",
    "for i, accuracy in enumerate(validation_accuracies_lstm, 1):\n",
    "    print(f'Validation Set {i} Accuracy (LSTM): {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f514c-7da3-4f7b-bea3-adcada11ed47",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As you can see, the accuracy on 16x16 images is not extremely different. However, RIMs generalize way more robustly to 19x19 and 24x24 images. To achieve this, the authors built recurrent networks that are modular in nature, with each module being independent of the other modules and only interacting sparsely through attention. In this way, each module can learn different aspects of the environment and is only responsible for ensuring similar performance on the same aspect of a different environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe47fe-3df3-469f-ae78-b1e2c9ad584b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Discussion point\n",
    "\n",
    "10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45788f3e-bcba-4a40-8836-833d1d06803d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 1b: A Shared Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ea49e-f052-42c0-8b1a-c37135584646",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As we have seen, deep learning has shifted towards structured models with specialized modules that enhance scalability and generalization. But we can go one step further. Inspired by the 1980s AI focus on modular architectures and the Global Workspace Theory from cognitive neuroscience, the approach [https://arxiv.org/pdf/2103.01197.pdf] we are going to analyse in this section employs a shared global workspace for module coordination. It promotes flexibility and systematic generalization by allowing dynamic interactions among specialized modules. This model emphasizes the importance of having a number of sparsely communicating specialist modules interact via a shared working memory, aiming to achieve coherent and efficient behavior across the system. \n",
    "\n",
    "RIMs leverage a self-attention mechanism to enable information sharing among specialist modules, traditionally through pairwise interactions where each module attends to every other. This new approach, however, introduces a shared workspace with limited capacity to streamline this process. At each computational step, specialist modules compete for the opportunity to write to this shared workspace. Subsequently, the information stored in the workspace is broadcasted to all specialists simultaneously, enhancing coordination and information flow among the modules without the need for direct pairwise communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22f777-19ce-4858-a1dd-93d98bbf9035",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise: Creating a Shared Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf09def-0d69-4c45-b418-0d956789de07",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Specialists compete to write their information into the shared workspace. This process is guided by a key-query-value attention mechanism, where the competition is realized through attention scores determining which specialists' information is most critical to be updated in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380f39e-ec9f-4981-b0c0-2531f1730db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # Ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73b534-831e-4e4f-b268-02077f8599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedWorkspace(nn.Module):\n",
    "\n",
    "    def __init__(self, num_specialists, hidden_dim, num_memory_slots, memory_slot_dim):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        super().__init__()\n",
    "        self.num_specialists = num_specialists\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_memory_slots = num_memory_slots\n",
    "        self.memory_slot_dim = memory_slot_dim\n",
    "        self.workspace_memory = nn.Parameter(torch.randn(num_memory_slots, memory_slot_dim))\n",
    "\n",
    "        # Attention mechanism components for writing to the workspace\n",
    "        self.key = ...\n",
    "        self.query = ...\n",
    "        self.value = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "\n",
    "    def write_to_workspace(self, specialists_states):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        # Flatten specialists' states if they're not already\n",
    "        specialists_states = specialists_states.view(-1, self.hidden_dim)\n",
    "\n",
    "        # Compute key, query, and value\n",
    "        keys = self.key(specialists_states)\n",
    "        query = self.query(self.workspace_memory)\n",
    "        values = self.value(specialists_states)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        attention_scores = torch.matmul(query, keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "        attention_probs = ...\n",
    "\n",
    "        # Update workspace memory with weighted sum of values\n",
    "        updated_memory = torch.matmul(attention_probs, values)\n",
    "        self.workspace_memory = nn.Parameter(updated_memory)\n",
    "\n",
    "        return self.workspace_memory\n",
    "\n",
    "    def forward(self, specialists_states):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        updated_memory = ...\n",
    "        return updated_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d1740-cc8b-4246-b566-e725e52e54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove solution\n",
    "\n",
    "class SharedWorkspace(nn.Module):\n",
    "\n",
    "    def __init__(self, num_specialists, hidden_dim, num_memory_slots, memory_slot_dim):\n",
    "        super().__init__()\n",
    "        self.num_specialists = num_specialists\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_memory_slots = num_memory_slots\n",
    "        self.memory_slot_dim = memory_slot_dim\n",
    "        self.workspace_memory = nn.Parameter(torch.randn(num_memory_slots, memory_slot_dim))\n",
    "\n",
    "        # Attention mechanism components for writing to the workspace\n",
    "        self.key = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "        self.query = nn.Linear(memory_slot_dim, memory_slot_dim)\n",
    "        self.value = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "\n",
    "    def write_to_workspace(self, specialists_states):\n",
    "        # Flatten specialists' states if they're not already\n",
    "        specialists_states = specialists_states.view(-1, self.hidden_dim)\n",
    "\n",
    "        # Compute key, query, and value\n",
    "        keys = self.key(specialists_states)\n",
    "        query = self.query(self.workspace_memory)\n",
    "        values = self.value(specialists_states)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        attention_scores = torch.matmul(query, keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Update workspace memory with weighted sum of values\n",
    "        updated_memory = torch.matmul(attention_probs, values)\n",
    "        self.workspace_memory = nn.Parameter(updated_memory)\n",
    "\n",
    "        return self.workspace_memory\n",
    "\n",
    "    def forward(self, specialists_states):\n",
    "        updated_memory = self.write_to_workspace(specialists_states)\n",
    "        return updated_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c901689-abca-4a3a-8d14-9cb2644f5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "num_specialists = 5\n",
    "hidden_dim = 10\n",
    "num_memory_slots = 4\n",
    "memory_slot_dim = 6\n",
    "\n",
    "# Generate deterministic specialists' states\n",
    "specialists_states = torch.randn(num_specialists, hidden_dim)\n",
    "\n",
    "workspace = SharedWorkspace(num_specialists, hidden_dim, num_memory_slots, memory_slot_dim)\n",
    "expected_output = workspace.forward(specialists_states)\n",
    "print(\"Expected Output:\", expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed9ace-b3de-4fdc-8996-664dd8e4b3a1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "After updating the shared workspace with the most critical signals, this information is then broadcast back to all specialists. Each specialist updates its state using this broadcast information, which can involve an attention mechanism for consolidation and an update function (like an LSTM or GRU step) based on the new combined state. Let's add this method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03836253-ec99-490b-9921-db1395e5c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_from_workspace(self, specialists_states):\n",
    "    # Broadcast updated memory to specialists\n",
    "    broadcast_query = self.query(specialists_states).view(self.num_specialists, -1, self.memory_slot_dim)\n",
    "    broadcast_keys = self.key(self.workspace_memory).unsqueeze(0).repeat(self.num_specialists, 1, 1)\n",
    "\n",
    "    # Compute attention scores for broadcasting\n",
    "    broadcast_attention_scores = torch.matmul(broadcast_query, broadcast_keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "    broadcast_attention_probs = F.softmax(broadcast_attention_scores, dim=-1)\n",
    "\n",
    "    # Update specialists' states with attention-weighted memory information\n",
    "    broadcast_values = self.value(self.workspace_memory).unsqueeze(0).repeat(self.num_specialists, 1, 1)\n",
    "    updated_states = torch.matmul(broadcast_attention_probs, broadcast_values)\n",
    "\n",
    "    return updated_states.view_as(specialists_states)\n",
    "\n",
    "# Assign the method to the class\n",
    "SharedWorkspace.broadcast_from_workspace = broadcast_from_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc221d3-530e-4835-b542-905ce3e281bf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This approach modularizes the shared workspace functionality, ensuring the specialists' states are first aggregated in a competitive manner into the workspace, followed by an efficient distribution of this consolidated information. This mechanism allows for dynamic filtering based on the current context and enhances the model's ability to generalize from past experiences by focusing on the most relevant signals at each computational step. To integrate this into a full system, you would need to instantiate this SharedWorkspace within your RIM architecture, ensuring that the initial representations of specialists are processed (Step 1), passed to the SharedWorkspace for competition and update (Step 2), and then the updated information is broadcast back to the specialists (Step3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed95b87-e254-41cd-83f5-dc32928480c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71074b-70b5-4446-bf86-00e9f781ecd7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 1c: a toy model for illustrating GNW "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2a7bc-5498-48d4-a1ee-599314124450",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we outline a `SimpleGNWModel` class for simulating node activation within a network. It uses an Erds-Rnyi graph to model connections and includes methods to activate nodes, reset the network, and visualize the results. This setup provides an interactive introduction to network dynamics, making it easy to observe how activations spread across a network.\n",
    "\n",
    "In the network visualization, the colors distinguish between active and inactive nodes. Active nodes are colored green, indicating they have been activated either directly or through their connection to another activated node. Inactive nodes are colored red, showing they have yet to be activated in the simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070b46f-51c0-4416-80ee-728a32f75cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNWModel:\n",
    "    def __init__(self, num_nodes=5):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.network = nx.erdos_renyi_graph(n=num_nodes, p=0.5)\n",
    "        self.activations = {node: False for node in self.network.nodes}\n",
    "\n",
    "    def activate_node(self):\n",
    "        selected_node = random.choice(list(self.network.nodes))\n",
    "        self.activations[selected_node] = True\n",
    "\n",
    "        # Simulate global broadcast\n",
    "        for neighbor in self.network.neighbors(selected_node):\n",
    "            self.activations[neighbor] = True\n",
    "\n",
    "    def reset_activations(self):\n",
    "        self.activations = {node: False for node in self.network.nodes}\n",
    "\n",
    "    def draw_network(self):\n",
    "        color_map = ['green' if self.activations[node] else 'red' for node in self.network.nodes]\n",
    "        nx.draw(self.network, node_color=color_map, with_labels=True, node_size=700)\n",
    "        plt.show()\n",
    "\n",
    "# Create a GNW model instance\n",
    "gnw_model = SimpleGNWModel()\n",
    "\n",
    "# Button to activate a node\n",
    "activate_button = widgets.Button(description='Activate Node')\n",
    "\n",
    "# Button to reset activations\n",
    "reset_button = widgets.Button(description='Reset')\n",
    "\n",
    "# Output area for the network graph\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_activate_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        gnw_model.activate_node()\n",
    "        gnw_model.draw_network()\n",
    "\n",
    "def on_reset_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        gnw_model.reset_activations()\n",
    "        gnw_model.draw_network()\n",
    "\n",
    "activate_button.on_click(on_activate_clicked)\n",
    "reset_button.on_click(on_reset_clicked)\n",
    "\n",
    "display(widgets.VBox([activate_button, reset_button, output_area]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c701cbc-d70b-44e3-8838-7b917ff3ef1a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When you click on the \"Activate Node\" button, the code randomly selects one node in the network and activates it, changing its status to active (if it was inactive). This action also triggers a \"global broadcast\" effect, meaning that all of the selected node's immediate neighbors are activated as well. The network visualization then updates to reflect these changes: the activated nodes are colored green, while any nodes that remain inactive are colored red. This process visually demonstrates how activation can spread through a network, highlighting the connections between nodes and the potential influence of a single node's activation on its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa4601-c636-4999-9b2d-70f1d85057ab",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Metacognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704245fb-56d8-459c-a819-cc4a899d9e44",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2a: Second order model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac885f1-55d5-4dc6-acca-8159386bf884",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This section invites you to engage with a straightforward, auto-generated dataset on blindsight, originally introduced by Pasquali et al. in 2010. Blindsight is a fascinating condition where individuals who are cortically blind due to damage in their primary visual cortex can still respond to visual stimuli without conscious perception. This intriguing phenomenon underscores the intricate nature of sensory processing and the brain's ability to process information without conscious awareness.\n",
    "\n",
    "Your mission involves a set of exercises aimed at examining the blindsight dataset, with a focus on its structure and various test scenarios:\n",
    "\n",
    "1. Explore the blindsight dataset: This involves visualizing the dataset used for training to get a grasp of its characteristics.\n",
    "2. Grasp the 3 testing scenarios and finalize the low vision function: The dataset has been constructed based on the testing scenarios by Pasquali et al. You're tasked with executing the code to visualize the dataset, understanding its design and purpose.\n",
    "3. Plot the testing data: Lastly, you'll plot the testing data using the provided code, drawing parallels with the earlier examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de060c73-ef87-436f-8fcd-e620b8fec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the autogenerated data\n",
    "initialize_global()\n",
    "patterns_tensor, _ , _, _ = Generate_Patterns(patterns_number, num_units)\n",
    "plot_signal_max_and_indicator(patterns_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf53dc1-4351-4da2-8563-e18b875359e2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The pre-training dataset for the network consisted of 200 patterns. These were evenly divided: half were purely noise (with unit activations randomly chosen between 0.0 and 0.02), and the other half represented potential stimuli. In the stimulus patterns, 99 out of 100 units had activations ranging between 0.0 and 0.02, with one unique unit having an activation between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb07ea8-6ce7-4812-bfef-cd4f1777014e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Testing patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da24487-3ef1-4129-9b86-a7b98e3eb198",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The network underwent evaluations under three distinct conditions, each modifying the signal-to-noise ratio in a unique way to explore different degrees and types of blindness.\n",
    "\n",
    "Suprathreshold stimulus condition: here, the network was exposed to the identical set of 200 patterns used during pre-training, testing the network's response to familiar inputs.\n",
    "\n",
    "Subthreshold stimulus condition (blindsight simulation): this condition aimed to mimic blindsight. It was achieved by introducing a slight noise increment (+0.0012) to every input of the first-order network, barring the one designated as the stimulus. This setup tested the network's ability to discern faint signals amidst noise.\n",
    "\n",
    "Low vision condition: to simulate low vision, the activation levels of the stimuli were reduced. Unlike the range from 0.0 to 1.0 used in pre-training, the stimuli's activation levels were adjusted to span from 0.0 to 0.3. This condition examined the network's capability to recognize stimuli with diminished intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c11ef3-e7ca-4958-930f-cb33fefa13a7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's get hands on and plot those auto-generated patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485bda4-edd9-4682-8557-d18660b4cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your results with the patterns generate below\n",
    "set_1, _ = create_patterns(0)\n",
    "set_2, _ = create_patterns(1)\n",
    "set_3, _ = create_patterns(2)\n",
    "\n",
    "# Plot\n",
    "plot_signal_max_and_indicator(set_1,\"Suprathreshold stimulus\")\n",
    "plot_signal_max_and_indicator(set_2,\"Subthreshold stimulus\")\n",
    "plot_signal_max_and_indicator(set_3,\"Low Vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721e8b5-22b0-4f2c-b1e5-72f69c5e6284",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Activity 1: Building a nework for a blindsight situation\n",
    "\n",
    "In this activity, we'll construct a neural network model using our auto-generated dataset, focusing on blindsight scenarios. The model will primarily consist of fully connected layers, establishing a straightforward, first-order network. The aim here is to assess the basic network's performance to later contrast it with the enhancements introduced by a second-order network in the subsequent activity.\n",
    "\n",
    "### Steps to follow\n",
    "\n",
    "1. Examine the network architecture: understand the structure of the neural network you're about to work with.\n",
    "2. Visualize loss metrics: observe and analyze the network's performance during pre-training by visualizing the loss over epochs.\n",
    "3. Evaluate the model: use the provided code snippets to calculate and interpret the model's accuracy, recall, and F1-score, giving you insight into the network's capabilities.\n",
    "\n",
    "### Understanding the process\n",
    "\n",
    "The goal is to gain a thorough comprehension of the network's architecture and to interpret the pre-training results visually. This will provide a clearer picture of the model's potential and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfbe26-cffc-4743-9661-078708c3695f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c74f4d-6b9e-4a55-997d-8c71119898f8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The network is designed as a backpropagation autoassociator. It features a 100-unit input layer, directly linked to a 60-unit hidden layer, which in turn connects to a 100-unit output layer. Initial connection weights are set within the range of -1.0 to 1.0 for the first-order network. To mitigate overfitting, dropout is employed within the network architecture. The architecture includes a configurable activation function. This flexibility allows for adjustments and tuning in Activity 3, aiming for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4a4cc-7971-4e76-bcbd-fbfe4bad6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units, data_factor, use_gelu):\n",
    "        \"\"\"\n",
    "        Initializes the FirstOrderNetwork with specific configurations.\n",
    "\n",
    "        Parameters:\n",
    "        - hidden_units (int): The number of units in the hidden layer.\n",
    "        - data_factor (int): Factor to scale the amount of data processed.\n",
    "                             A factor of 1 indicates the default data amount,\n",
    "                             while 10 indicates 10 times the default amount.\n",
    "        - use_gelu (bool): Flag to use GELU (True) or ReLU (False) as the activation function.\n",
    "        \"\"\"\n",
    "        super(FirstOrderNetwork, self).__init__()\n",
    "\n",
    "        # Define the encoder, hidden, and decoder layers with specified units\n",
    "        self.encoder = nn.Linear(100, hidden_units)\n",
    "        self.hidden = nn.Linear(hidden_units, hidden_units)\n",
    "        self.decoder = nn.Linear(hidden_units, 100)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Temperature parameter for activation sharpness control\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        # Set the data factor\n",
    "        self.data_factor = data_factor\n",
    "\n",
    "        # Choose the activation function based on the use_gelu flag\n",
    "        self.activation = nn.GELU() if use_gelu else nn.ReLU()\n",
    "\n",
    "        # Other activation functions for various purposes\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "        # Initialize network weights\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initializes weights of the encoder, hidden, and decoder layers uniformly.\"\"\"\n",
    "        init.uniform_(self.encoder.weight, -1.0, 1.0)\n",
    "        init.uniform_(self.hidden.weight, -1.0, 1.0)\n",
    "        init.uniform_(self.decoder.weight, -1.0, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to the network.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The output of the network after passing through the layers and activations.\n",
    "        \"\"\"\n",
    "        # Encoder step with dropout and sigmoid activation\n",
    "        x = self.dropout(self.sigmoid(self.encoder(x) / self.temperature))\n",
    "\n",
    "        # Hidden layer step with dropout and sigmoid activation\n",
    "        x = self.dropout(self.sigmoid(self.hidden(x) / self.temperature))\n",
    "\n",
    "        # Decoder step with dropout and sigmoid activation\n",
    "        output = self.dropout(self.sigmoid(self.decoder(x) / self.temperature))\n",
    "\n",
    "        # Adjust output based on a threshold\n",
    "        output = torch.where(output > 0.5, output + 0.12, output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef709703-27e1-43eb-b924-1a2b3eb54773",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### First order network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade7dd7-a679-4949-942c-7b6219bd58cf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For now, we will train the first order network only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43604b-662b-40e4-a155-b40fcd120961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture, optimizers, loss functions, and schedulers for pre training\n",
    "hidden=60\n",
    "factor=1\n",
    "gelu=False\n",
    "gam=0.99\n",
    "stepsize=1\n",
    "\n",
    "initialize_global()\n",
    "\n",
    "# Networks instantiation\n",
    "first_order_network = FirstOrderNetwork(hidden,factor,gelu).to(device)\n",
    "second_order_network = SecondOrderNetwork(gelu).to(device) # We define it, but won't use it until activity 3\n",
    "\n",
    "# Loss function\n",
    "criterion_1 = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer_1 = optim.SGD(first_order_network.parameters(), lr=learning_rate_1, momentum=momentum)\n",
    "\n",
    "# Learning rate schedulers\n",
    "scheduler_1 = StepLR(optimizer_1, step_size=stepsize, gamma=gam)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Generate training patterns and targets for each epoch.\n",
    "    patterns_tensor, stim_present_tensor, stim_absent_tensor, order_2_tensor = Generate_Patterns(patterns_number, num_units)\n",
    "\n",
    "    # Forward pass through the first-order network\n",
    "    output_first_order = first_order_network(patterns_tensor)\n",
    "\n",
    "    # Skip computations for the second-order network\n",
    "    with torch.no_grad():\n",
    "        # Potentially forward pass through the second-order network without tracking gradients\n",
    "        output_second_order = second_order_network(patterns_tensor, output_first_order)\n",
    "\n",
    "    # Calculate the loss for the first-order network (accuracy of stimulus representation)\n",
    "    loss_1 = criterion_1(output_first_order, stim_present_tensor)\n",
    "\n",
    "    # Backpropagate the first-order network's loss\n",
    "    loss_1.backward()\n",
    "\n",
    "    # Update first-order network weights\n",
    "    optimizer_1.step()\n",
    "\n",
    "    # Reset first-order optimizer gradients to zero for the next iteration\n",
    "    optimizer_1.zero_grad()\n",
    "\n",
    "    # Update the first-order scheduler\n",
    "    scheduler_1.step()\n",
    "\n",
    "    epoch_1_order[epoch] = loss_1.item()\n",
    "\n",
    "# Plot training loss curve\n",
    "pre_train_plots(epoch_1_order, epoch_2_order, \"First order network training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cc269-d60c-411f-afae-3c2bf1ec2e70",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Testing under 3 blindsight conditions\n",
    "\n",
    "We will now use the testing auto-generated datasets from activity 1 to test the network's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d07b5e-2623-42ba-96c8-e8c67f0a2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare networks for testing by calling the configuration function\n",
    "Testing_patterns, n_samples, loaded_model, loaded_model_2 = config_training(first_order_network, second_order_network, hidden, factor, gelu)\n",
    "\n",
    "# Perform testing using the defined function and plot the results\n",
    "f1_scores_high_wager, f1_scores_low_wager, results_for_plotting = testing(Testing_patterns, n_samples, loaded_model, loaded_model_2)\n",
    "\n",
    "# Assuming plot_testing is defined, call it to display results\n",
    "plot_testing(results_for_plotting, \"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9105a75-5233-4e33-ac05-83a1ac9ea5df",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Activity 2: Developing a Second-Order Network\n",
    "\n",
    "In this activity, we transition to constructing a more complex model: a second-order network. This network builds upon the foundational first-order network by introducing a meta-cognitive layer that evaluates the first-order network's predictions. The primary novelty here is the introduction of a wagering mechanism, where the network \"bets\" on its confidence level in its predictions.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your mission involves expanding the model architecture to incorporate second-order functionalities. This includes understanding and coding the mechanisms behind a second-order network, followed by evaluating its performance through training loss visualization and testing with provided code snippets. You're also encouraged to explore model fine-tuning to optimize performance.\n",
    "\n",
    "### Steps for completion\n",
    "\n",
    "1. Architectural development: grasp the underlying principles of a second-order network and complete the architectural code.\n",
    "2. Performance evaluation: visualize training losses and test the model using provided code, assessing its initial performance.\n",
    "3. Model fine-tuning: leveraging the provided training function, experiment with fine-tuning the model to enhance its accuracy and efficiency.\n",
    "\n",
    "### Network architecture overview\n",
    "\n",
    "The second-order network is structured as a feedforward backpropagation network.\n",
    "\n",
    "- Input Layer: comprises a 100-unit comparison matrix. This matrix quantifies the discrepancy between each corresponding pair of input and output units from the first-order network. For example, if an input unit and its corresponding output unit have activations of 0.6 and 0.7, respectively, the comparison unit's activation would be -0.1. This setup essentially encodes the prediction error of the first-order network's outputs as an input pattern for the second-order network.\n",
    "- Output Layer: consists of two units representing \"high\" and \"low\" wagers, indicating the network's confidence in its predictions. The initial weights for these output units range between 0.0 and 0.1.\n",
    "- Comparator Weights: set to 1.0 for connections from the first-order input layer to the comparison matrix, and -1.0 for connections from the first-order output layer. This configuration emphasizes the differential error as a critical input for the second-order decision-making process.\n",
    "\n",
    "### Understanding second-order mechanisms\n",
    "\n",
    "The second-order network's novel approach uses the error generated by the first-order network as a direct input for making decisionsspecifically, wagering on the confidence of its outputs. This methodology reflects a meta-cognitive layer of processing, akin to evaluating one's confidence in their answers or predictions.\n",
    "\n",
    "### Fine-tuning insights\n",
    "\n",
    "Fine-tuning involves adjusting various model parameters, such as learning rates, the number of epochs, or even the architecture itself (e.g., adding layers, changing activation functions) to achieve better performance. The goal is to improve the network's prediction accuracy and confidence wagering, thereby making it more adept at tasks resembling the blindsight condition simulation.\n",
    "\n",
    "By exploring these adjustments, you can optimize the network's functionality, making it a powerful tool for understanding and simulating complex cognitive phenomena like blindsight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb66558-ef00-4629-bfb4-54bc62d5cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondOrderNetwork(nn.Module):\n",
    "    def __init__(self, use_gelu):\n",
    "        super(SecondOrderNetwork, self).__init__()\n",
    "        # Define a linear layer for comparing the difference between input and output of the first-order network\n",
    "        self.comparison_layer = nn.Linear(100, 100)\n",
    "\n",
    "        # Linear layer for determining wagers, mapping from 100 features to a single output\n",
    "        self.wager = nn.Linear(100, 1)\n",
    "\n",
    "        # Dropout layer to prevent overfitting by randomly setting input units to 0 with a probability of 0.3 during training\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Select activation function based on the `use_gelu` flag\n",
    "        self.activation = torch.nn.GELU() if use_gelu else torch.relu\n",
    "\n",
    "        # Additional activation functions for potential use in network operations\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.softmax = torch.nn.Softmax(dim=1)  # Softmax for multi-class classification problems\n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "        # Initialize the weights of the network\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Uniformly initialize weights for the comparison and wager layers\n",
    "        init.uniform_(self.comparison_layer.weight, -1.0, 1.0)\n",
    "        init.uniform_(self.wager.weight, 0.0, 0.1)\n",
    "\n",
    "    def forward(self, first_order_input, first_order_output):\n",
    "        # Calculate the difference between the first-order input and output\n",
    "        comparison_matrix = first_order_input - first_order_output\n",
    "\n",
    "        # Pass the difference through the comparison layer and apply the chosen activation function\n",
    "        comparison_out = self.comparison_layer(comparison_matrix)\n",
    "\n",
    "        # Calculate the wager value, applying dropout and sigmoid activation to the output of the wager layer\n",
    "        wager = self.dropout(self.sigmoid(self.wager(comparison_out)))\n",
    "\n",
    "        return wager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce70ae-8fac-4ef7-aab6-ba8fe238d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(first_order_network, second_order_network, criterion_1, criterion_2, optimizer_1, optimizer_2, scheduler_1, scheduler_2, factor, meta):\n",
    "    \"\"\"\n",
    "    Conducts pre-training for first-order and second-order networks.\n",
    "\n",
    "    Parameters:\n",
    "    - first_order_network (torch.nn.Module): Network for basic input-output mapping.\n",
    "    - second_order_network (torch.nn.Module): Network for decision-making based on the first network's output.\n",
    "    - criterion_1, criterion_2 (torch.nn): Loss functions for the respective networks.\n",
    "    - optimizer_1, optimizer_2 (torch.optim): Optimizers for the respective networks.\n",
    "    - scheduler_1, scheduler_2 (torch.optim.lr_scheduler): Schedulers for learning rate adjustment.\n",
    "    - factor (float): Parameter influencing data augmentation or pattern generation.\n",
    "    - meta (bool): Flag indicating the use of meta-learning strategies.\n",
    "\n",
    "    Returns:\n",
    "    Tuple containing updated networks and epoch-wise loss records.\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        # Generate training patterns and targets for each epoch\n",
    "        patterns_tensor, stim_present_tensor, stim_absent_tensor, order_2_tensor = Generate_Patterns(patterns_number, num_units)\n",
    "\n",
    "        # Forward pass through the first-order network\n",
    "        output_first_order = first_order_network(patterns_tensor)\n",
    "\n",
    "        optimizer_1.zero_grad()\n",
    "\n",
    "        # Conditionally execute the second-order network pass and related operations\n",
    "        if meta:\n",
    "            optimizer_2.zero_grad()\n",
    "\n",
    "            # Forward pass through the second-order network with inputs from the first-order network\n",
    "            output_second_order = second_order_network(patterns_tensor, output_first_order)\n",
    "\n",
    "            # Calculate the loss for the second-order network (wagering decision based on comparison)\n",
    "            loss_2 = criterion_2(output_second_order, order_2_tensor)\n",
    "\n",
    "            # Backpropagate the second-order network's loss\n",
    "            loss_2.backward(retain_graph=True)  # Allows further backpropagation for loss_1 after loss_2\n",
    "\n",
    "            # Update second-order network weights\n",
    "            optimizer_2.step()\n",
    "\n",
    "            epoch_2_order[epoch] = loss_2.item()\n",
    "        else:\n",
    "            # Skip computations for the second-order network\n",
    "            with torch.no_grad():\n",
    "                # Potentially forward pass through the second-order network without tracking gradients\n",
    "                output_second_order = second_order_network(patterns_tensor, output_first_order)\n",
    "\n",
    "        # Calculate the loss for the first-order network (accuracy of stimulus representation)\n",
    "        loss_1 = criterion_1(output_first_order, stim_present_tensor)\n",
    "\n",
    "        # Backpropagate the first-order network's loss\n",
    "        loss_1.backward(retain_graph=True)\n",
    "\n",
    "        # Update first-order network weights\n",
    "        optimizer_1.step()\n",
    "\n",
    "        # Reset first-order optimizer gradients to zero for the next iteration\n",
    "\n",
    "        # Update the first-order scheduler\n",
    "        scheduler_1.step()\n",
    "\n",
    "        # Update the second-order scheduler\n",
    "        scheduler_2.step()\n",
    "\n",
    "        epoch_1_order[epoch] = loss_1.item()\n",
    "\n",
    "    return first_order_network, second_order_network, epoch_1_order, epoch_2_order\n",
    "\n",
    "# Hyperparameters\n",
    "hidden=60\n",
    "factor=1\n",
    "gelu=False\n",
    "gam=0.99\n",
    "stepsize=1\n",
    "meta=True\n",
    "\n",
    "# Initialize any global variables or settings, potentially for random seed setting or environment setup\n",
    "initialize_global()\n",
    "\n",
    "# Prepare networks, loss functions, optimizers, and schedulers for pre-training\n",
    "first_order_network, second_order_network, criterion_1, criterion_2, optimizer_1, optimizer_2, scheduler_1, scheduler_2 = prepare_pre_training(hidden, factor, gelu, stepsize, gam)\n",
    "\n",
    "# Conduct pre-training for both the first-order and second-order networks\n",
    "first_order_network_pre, second_order_network_pre, epoch_1_order, epoch_2_order = pre_train(first_order_network, second_order_network, criterion_1, criterion_2, optimizer_1, optimizer_2, scheduler_1, scheduler_2, factor, meta)\n",
    "\n",
    "# Plot the training progress of both networks to visualize performance and learning trends\n",
    "pre_train_plots(epoch_1_order, epoch_2_order, \"2nd-Order-Network\")\n",
    "\n",
    "# Configuration step for the main training phase or evaluation\n",
    "Testing_patterns, n_samples, loaded_model, loaded_model_2 = config_training(first_order_network_pre, second_order_network_pre, hidden, factor, gelu)\n",
    "\n",
    "# Perform testing of the trained models on a separate dataset to evaluate performance\n",
    "f1_scores_high_wager, f1_scores_low_wager, results_for_plotting = testing(Testing_patterns, n_samples, loaded_model, loaded_model_2)\n",
    "\n",
    "# Visualize the testing results, comparing the model performance against a baseline to assess improvements\n",
    "plot_testing(results_for_plotting, \"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b37fa4-44a2-4cfa-b1b6-62198c3c4fe7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Discussion point\n",
    "\n",
    "Let's dive into the outcomes!\n",
    "\n",
    "- Did you notice any variations between the two models?\n",
    "- Can you explain how these differences influenced the performance?\n",
    "- What role does a second-order network play, and in which situations would it be more effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d70aae-6376-48c4-876f-41fcbd4c05dd",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 2b: HOSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85588efd-993d-4125-9859-12c28d600834",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this Section we will combine elements of the previous sections to illustrate an alternative perspective on the origins of conscious awareness in neural systems. This perspective is provided by higher-order theory, and the core idea is that consciousness is enabled by the monitoring of first-order information processing, rather than the global broadcast. This perspective shares with global workspace architectures the need for a global monitor of multiple first-order modules. It also builds on the previous section that highlights the important of a second-order network for monitoring first-order processing to explain blindsight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4faa9-84d6-4093-824c-c17652d68147",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following function is designed for inference within a simplified Bayesian framework, specifically tailored for assessing perceptual states based on observed data. It computes the posterior probabilities of these states and the Kullback-Leibler (KL) divergence between the posterior and prior distributions. This function operates under a model that assumes a flat (or single-layer) Bayesian network, focusing directly on the relationship between perceptual states and observed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa71fb-ce0a-4f12-a7a6-3df4996eb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOSS_evaluate_flat(X, mu, Sigma, Wprior):\n",
    "    \"\"\"\n",
    "    Perform inference on a 2D Bayes net for asymmetric inference on presence vs. absence.\n",
    "\n",
    "    Parameters:\n",
    "    X - Observed data\n",
    "    mu - Means for each perceptual state\n",
    "    Sigma - Covariance matrix\n",
    "    Wprior - Prior probabilities of perceptual states\n",
    "\n",
    "    #Returns:\n",
    "    post_W - Posterior probabilities of perceptual states\n",
    "    KL_W - Kullback-Leibler divergence from posterior to prior\n",
    "    \"\"\"\n",
    "    # Prior on perceptual states W\n",
    "    p_W = Wprior\n",
    "\n",
    "    # Compute likelihood of observed X for each possible W (P(X|W))\n",
    "    log_lik_X_W = np.array([np.log(multivariate_normal.pdf(X, mean=mu[m], cov=Sigma)) for m in range(mu.shape[0])])\n",
    "\n",
    "    # Renormalize to get P(X|W)\n",
    "    log_p_X_W = log_lik_X_W - logsumexp(log_lik_X_W)\n",
    "\n",
    "    # Posterior over W (P(W|X=x))\n",
    "    log_post_W = log_p_X_W + np.log(p_W)\n",
    "    log_post_W = log_post_W - logsumexp(log_post_W)  # Normalize\n",
    "    post_W = np.exp(log_post_W)\n",
    "\n",
    "    # KL divergences\n",
    "    KL_W = np.sum(post_W * (np.log(post_W) - np.log(p_W)))\n",
    "\n",
    "    return post_W, KL_W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfba4a-b48a-48c5-a554-f03e7096af2e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Make our stimulus space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffb999-c213-4400-8f1b-dac5b42ff5e1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The model we are using is grounded in classical \"signal detection theory\", or SDT for short. SDT is in turn a special case of a Bayesian generative model, in which an arbitrary \"evidence\" value is drawn from an unknown distribution, and the task of the observer is to infer which distribution this evidence came from.\n",
    "\n",
    "Let's imagine we have two categories, A and B - for instance, left- and right-tilted visual stimuli. \n",
    "The sensory \"evidence\" can be written as 2D vector, where the first element is evidence for A, and the second element evidence for B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eeba6f-6608-41e1-bfa3-2861bbeb6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the array X with strong evidence for A and weak evidence for B\n",
    "X = np.array([1.5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32b9f0-cd87-43fa-8169-8fdb78fd606d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The origin (0,0) represents low activation of both features, consistent with no stimulus (or noise) being presented. Comparing how the model handles inference on stimulus presence vs. absence - detecting, vs. not detecting a stimulus - allows us to capture the classical conscious vs. unconscious contrast in consciousness science.\n",
    "\n",
    "Let's start by creating our space, and placing three Gaussian distributions on the space that represent the likelihood of observing a pair of features given each of three stimulus classes: leftward tilt (w1), rightward tilt (w2) and noise/nothing (w0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40e73d-d90c-4630-9089-4cb64d22f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "xgrid = np.arange(-4, 6.02, 0.02)\n",
    "X1, X2 = np.meshgrid(xgrid, xgrid)\n",
    "\n",
    "# Mean and covariance of the distributions\n",
    "mu = np.array([[0.5, 0.5], [3.5, 0.5], [0.5, 3.5]])\n",
    "Sigma = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Colors and labels according to the specification\n",
    "colors = ['green', 'blue', 'red']\n",
    "labels = ['w0', 'w1', 'w2']\n",
    "\n",
    "for i, (color, label) in enumerate(zip(colors, labels)):\n",
    "    p = multivariate_normal.pdf(np.dstack((X1, X2)), mean=mu[i], cov=Sigma)\n",
    "    ax.plot_surface(X1, X2, p.reshape(X1.shape), color=color, alpha=0.5, label=label)\n",
    "\n",
    "# Create custom legends\n",
    "legend_elements = [Patch(facecolor=color, edgecolor='k', label=label) for color, label in zip(colors, labels)]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Reverse the X1 axis\n",
    "ax.set_xlim([6, -4])\n",
    "ax.set_ylim([-4, 6])\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_title('2D SDT')\n",
    "ax.view_init(45, -45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f1787-11f0-4c57-8685-dc49e0819540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input parameters\n",
    "mu = np.array([[3.5, 0.5], [0.5, 3.5], [0.5, 0.5]])\n",
    "Sigma = np.array([[1, 0], [0, 1]])\n",
    "Wprior = np.array([1/3, 1/3, 1/3])  # flat priors\n",
    "\n",
    "# High evidence for X1, low evidence for X2\n",
    "X = np.array([3, 0])\n",
    "post_w, KL_W = HOSS_evaluate_flat(X, mu, Sigma, Wprior)\n",
    "print('Posterior probabilities for X = [3, 0]:', post_w)\n",
    "print('KL Divergence for X = [3, 0]:', KL_W)\n",
    "\n",
    "# High evidence for X2, low evidence for X1\n",
    "X = np.array([0, 3])\n",
    "post_w, KL_W = HOSS_evaluate_flat(X, mu, Sigma, Wprior)\n",
    "print('Posterior probabilities for X = [0, 3]:', post_w)\n",
    "print('KL Divergence for X = [0, 3]:', KL_W)\n",
    "\n",
    "# No evidence for either\n",
    "X = np.array([0, 0])\n",
    "post_w, KL_W = HOSS_evaluate_flat(X, mu, Sigma, Wprior)\n",
    "print('Posterior probabilities for X = [0, 0]:', post_w)\n",
    "print('KL Divergence for X = [0, 0]:', KL_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573eaad-da8f-4379-b226-497b7333374d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is as we would expect - the most likely state is recovered in each case. The slightly higher KL divergence in the third scenario indicates a greater degree of \"surprise\" or information gain, as the prior was uniformly distributed across all states, but the posterior is now highly concentrated on the third state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c08c7-ea2b-43bb-b1b4-560ba46089a6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Add in higher-order node for global detection\n",
    "So far, we have considered a \"flat\" architecture in which each state (w1, w2, or absence) is independent. The key addition in HOSS is to allow the model to flexibly answer queries about awareness of any stimulus contents (w1 or w2... or wN). We achieve this by introducing a higher-order node - the \"A\" level - that \"monitors\" for activations in the W-level below.\n",
    "\n",
    "The inputs remain the same - pairs of X's. But now, the outputs give us queries on both the W (content) and A (awareness) levels - where post_A denotes the posterior probability of any content (vs. noise). We now also need to set priors at both the A- and W-levels.\n",
    "\n",
    "In the next sections, we will illustrate the operation of a higher-order state space (HOSS) for monitoring first-order information processing. This higher-order state has a wide purview, enabling the system to know which of its (potentially high-dimensional, rich) first-order states are reliable enough for use in future computation and communication to others. Through this mechanism, the HOSS model not only enhances the accuracy of stimulus awareness but also significantly improves the system's efficiency in processing and utilizing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c6660-231a-4b1c-8c6d-224007e2fad5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12f7e4-1eba-47b2-b49e-1ae0593ced22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOSS_evaluate(X, mu, Sigma, Aprior, Wprior):\n",
    "    \"\"\"\n",
    "    Inference on 2D Bayes net for asymmetric inference on presence vs. absence.\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: fill in the missing variables ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "    #################################################\n",
    "\n",
    "    # Initialise variables and conditional prob tables\n",
    "    p_A = np.array([1 - Aprior, Aprior])  # prior on awareness state A\n",
    "    p_W_a1 = np.append(Wprior, 0)  # likelihood of world states W given aware, last entry is absence\n",
    "    p_W_a0 = np.append(np.zeros(len(Wprior)), 1)  # likelihood of world states W given unaware, last entry is absence\n",
    "    p_W = (p_W_a1 + p_W_a0) / 2  # prior on W marginalising over A (for KL)\n",
    "\n",
    "    # Compute likelihood of observed X for each possible W (P(X|mu_w, Sigma))\n",
    "    lik_X_W = np.array([multivariate_normal.pdf(...) for mu_i in mu])\n",
    "    p_X_W = lik_X_W / lik_X_W.sum()  # normalise to get P(X|W)\n",
    "\n",
    "    # Combine with likelihood of each world state w given awareness state A\n",
    "    lik_W_A = np.vstack((p_X_W * p_W_a0 * p_A[0], p_X_W * p_W_a1 * p_A[1]))\n",
    "    post_A = ...  # sum over W\n",
    "    post_A = post_A / post_A.sum()  # normalise\n",
    "\n",
    "    # Posterior over W (P(W|X=x) marginalising over A)\n",
    "    post_W = ...  # sum over A\n",
    "    post_W = post_W / post_W.sum()  # normalise\n",
    "\n",
    "    # KL divergences\n",
    "    KL_W = (post_W * np.log(post_W / p_W)).sum()\n",
    "    KL_A = (post_A * np.log(post_A / p_A)).sum()\n",
    "\n",
    "    return post_W, post_A, KL_W, KL_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb04c59-a0e2-4595-ac2c-5659242eb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def HOSS_evaluate(X, mu, Sigma, Aprior, Wprior):\n",
    "    \"\"\"\n",
    "    Inference on 2D Bayes net for asymmetric inference on presence vs. absence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise variables and conditional prob tables\n",
    "    p_A = np.array([1 - Aprior, Aprior])  # prior on awareness state A\n",
    "    p_W_a1 = np.append(Wprior, 0)  # likelihood of world states W given aware, last entry is absence\n",
    "    p_W_a0 = np.append(np.zeros(len(Wprior)), 1)  # likelihood of world states W given unaware, last entry is absence\n",
    "    p_W = (p_W_a1 + p_W_a0) / 2  # prior on W marginalising over A (for KL)\n",
    "\n",
    "    # Compute likelihood of observed X for each possible W (P(X|mu_w, Sigma))\n",
    "    lik_X_W = np.array([multivariate_normal.pdf(X, mean=mu_i, cov=Sigma) for mu_i in mu])\n",
    "    p_X_W = lik_X_W / lik_X_W.sum()  # normalise to get P(X|W)\n",
    "\n",
    "    # Combine with likelihood of each world state w given awareness state A\n",
    "    lik_W_A = np.vstack((p_X_W * p_W_a0 * p_A[0], p_X_W * p_W_a1 * p_A[1]))\n",
    "    post_A = lik_W_A.sum(axis=1)  # sum over W\n",
    "    post_A = post_A / post_A.sum()  # normalise\n",
    "\n",
    "    # Posterior over W (P(W|X=x) marginalising over A)\n",
    "    post_W = lik_W_A.sum(axis=0)  # sum over A\n",
    "    post_W = post_W / post_W.sum()  # normalise\n",
    "\n",
    "    # KL divergences\n",
    "    KL_W = (post_W * np.log(post_W / p_W)).sum()\n",
    "    KL_A = (post_A * np.log(post_A / p_A)).sum()\n",
    "\n",
    "    return post_W, post_A, KL_W, KL_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45dfdc-3442-40ea-b116-7daae2d73384",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is now factorised in the code, so we first set the prior on presence (vs. absence), and then set the priors on w1 vs. w2, and the model takes care of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a881f-3103-4fbc-b958-8f29db2bee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input parameters for this specific example\n",
    "X = np.array([0, 3])  # Input observed features\n",
    "Wprior = np.array([0.5, 0.5])  # Prior probabilities of stimuli\n",
    "Aprior = 0.5  # Prior probability of being aware\n",
    "\n",
    "# Call the HOSS_evaluate function with the specified parameters\n",
    "post_W, post_A, KL_W, KL_A = HOSS_evaluate(X, mu, Sigma, Aprior, Wprior)\n",
    "\n",
    "# Print the posterior probabilities\n",
    "print(f\"Posterior probabilities at W level: {post_W}\")\n",
    "print(f\"Posterior probability at A level: {post_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03eb858-2e3e-442e-8f74-746268824a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input parameters for this specific example\n",
    "X = np.array([0, 3])  # Input observed features\n",
    "Wprior = np.array([0.5, 0.5])  # Prior probabilities of stimuli\n",
    "Aprior = 0.5  # Prior probability of being aware\n",
    "\n",
    "# Call the HOSS_evaluate function with the specified parameters\n",
    "post_W, post_A, KL_W, KL_A = HOSS_evaluate(X, mu, Sigma, Aprior, Wprior)\n",
    "\n",
    "# Print the posterior probabilities\n",
    "print(f\"Posterior probabilities at W level: {post_W}\")\n",
    "print(f\"Posterior probability at A level: {post_A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a694f1e-3f32-48fc-bce8-0b544d43ca62",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Plot surfaces for content / awareness inferences\n",
    "To explore the properties of the model, we can simulate inference at different levels of the hierarchy over the full 2D space of possible input X's. The left panel below shows that the probability of awareness (of any stimulus contents) rises in a graded manner from the lower left corner of the graph (low activation of any feature) to the upper right (high activation of both features). In contrast, the right panel shows that confidence in making a discrimination response (e.g. rightward vs. leftward) increases away from the major diagonal, as the model becomes sure that the sample was generated by either a leftward or rightward tilted stimulus. \n",
    "\n",
    "Together, the two surfaces make predictions about the relationships we might see between discrimination confidence and awareness in a simple psychophysics experiment. One notable prediction is that discrimination could still be possible - and lead to some degree of confidence - even when the higher-order node is \"reporting\" unawareness of the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7add05-825c-477e-8051-dd24ecbccc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "xgrid = np.arange(0, 2.01, 0.01)\n",
    "\n",
    "# Define the means for the Gaussian distributions\n",
    "mu = np.array([[0.5, 1.5], [1.5, 0.5], [0.5, 0.5]])\n",
    "\n",
    "# Define the covariance matrix\n",
    "Sigma = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Prior probabilities\n",
    "Wprior = np.array([0.5, 0.5])\n",
    "Aprior = 0.5\n",
    "\n",
    "# Initialize arrays to hold confidence and posterior probability\n",
    "confW = np.zeros((len(xgrid), len(xgrid)))\n",
    "posteriorAware = np.zeros((len(xgrid), len(xgrid)))\n",
    "KL_w = np.zeros((len(xgrid), len(xgrid)))\n",
    "KL_A = np.zeros((len(xgrid), len(xgrid)))\n",
    "\n",
    "# Compute confidence and posterior probability for each point in the grid\n",
    "for i, xi in enumerate(xgrid):\n",
    "    for j, xj in enumerate(xgrid):\n",
    "        X = [xi, xj]\n",
    "        post_w, post_A, KL_w[i, j], KL_A[i, j] = HOSS_evaluate(X, mu, Sigma, Aprior, Wprior)\n",
    "        confW[i, j] = max(post_w[0], post_w[1])\n",
    "        posteriorAware[i, j] = post_A[1]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Posterior probability \"seen\"\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xgrid, xgrid, posteriorAware.T)\n",
    "plt.colorbar()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Posterior probability \"seen\"')\n",
    "plt.axis('square')\n",
    "\n",
    "# Confidence in identity\n",
    "plt.subplot(1, 2, 2)\n",
    "contour_set = plt.contourf(xgrid, xgrid, confW.T)\n",
    "plt.colorbar()\n",
    "plt.contour(xgrid, xgrid, posteriorAware.T, levels=[0.5], linewidths=4, colors=['white'])  # Line contour for threshold\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Confidence in identity')\n",
    "plt.axis('square')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb2c34-fd8e-4525-bf21-aa582657c565",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Simulate KL divergence surfaces\n",
    "\n",
    "We can also simulate K-L divergences (a measure of Bayesian surprise) at each layer in the network, which under predictive coding models of brain has been proposed to scale with neural activation (eg Friston, 2005; Summerfield & de Lange, 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf267c-3acb-4ca4-ab47-4bc99ad7bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean K-L divergence for absent and present awareness states\n",
    "KL_A_absent = np.mean(KL_A[posteriorAware < 0.5])\n",
    "KL_A_present = np.mean(KL_A[posteriorAware >= 0.5])\n",
    "KL_w_absent = np.mean(KL_w[posteriorAware < 0.5])\n",
    "KL_w_present = np.mean(KL_w[posteriorAware >= 0.5])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# K-L divergence, perceptual states\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xgrid, xgrid, KL_w.T, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('K-L divergence, perceptual states')\n",
    "plt.axis('square')\n",
    "\n",
    "# K-L divergence, awareness state\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(xgrid, xgrid, KL_A.T, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('K-L divergence, awareness state')\n",
    "plt.axis('square')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463989fa-0824-4c8c-9bdd-dbb7e4857e46",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "Can you recognise the difference between the K-L divergence for the W-level and the one for the A-level?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230cc74-de5f-4a18-b5ee-fb5f2e4e14c3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### A hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f5470-e848-4508-a426-f097e43b774a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "At the level of perceptual states W, there is substantial asymmetry in the K-L divergence expected when the model says seen vs. unseen (lefthand panel). This is due to the large belief updates invoked in the perceptual layer W by samples that deviate from the lower lefthand corner - from absence. In contrast, when we compute K-L divergence for the A-level (righthand panel), the level of prediction error is symmetric across seen and unseen decisions, leading to \"hot\" zones both at the upper righthand (present) and lower lefthand (absent) corners of the 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c87c0-7067-4e7f-a60f-1ce8caf3ea3c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also sort the K-L divergences as a function of whether the model \"reported\" presence or absence. As can be seen in the bar plots below, there is more asymmetry in the prediction error at the W compared to the A levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9586b-29cc-42cc-bae5-37eb06f611c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with specified size\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# KL divergence for W states\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['unseen', 'seen'], [KL_w_absent, KL_w_present], color='k')\n",
    "plt.ylabel('K-L divergence, W states')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# KL divergence for A states\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['unseen', 'seen'], [KL_A_absent, KL_A_present], color='k')\n",
    "plt.ylabel('K-L divergence, A states')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee04be-c5f5-487e-aaec-79653cf25768",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Simulate ignition (asymmetry vs. symmetry)\n",
    "\n",
    "A notable feature about the HOSS architecture is that it is asymmetric - there are naturally more possible (perceptual) states nested under \"presence\" than under \"absence\". As we saw in the previous section, this asymmetry in the state space suggests there will be greater summed prediction error in the entire network on presence decisions (as summarized by K-L divergence at each node of W). This may be a computational correlate of the global ignition responses often found to track awareness reports, and which is often interpreted as supporting global workspace models (e.g. Del Cul et al. 2007; Dehaene and Changeux 2011). We can simulate this by asking how the divergence in seen vs. unseen prediction error at the two levels seen in the previous plots changes as a function of stimulus strength - modeled here as sensory precision. Importantly, however, the explanation of ignition under HOSS is in terms of prediction error, rather than global broadcast. In other words, ignition is a epiphenomenal consequence of computations taking place elsewhere in the system, rather than constitutive of global information sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f812a-f202-4f3d-ac66-247b322002e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "mu = np.array([[3.5, 0.5], [0.5, 3.5], [0.5, 0.5]])\n",
    "Nsubjects = 30\n",
    "Ntrials = 600\n",
    "cond = np.concatenate((np.ones(Ntrials//3), np.ones(Ntrials//3)*2, np.ones(Ntrials//3)*3))\n",
    "Wprior = [0.5, 0.5]\n",
    "Aprior = 0.5\n",
    "\n",
    "# Sensory precision values\n",
    "gamma = np.linspace(0.1, 10, 6)\n",
    "\n",
    "# Initialize lists for results\n",
    "all_KL_w_yes = []\n",
    "sem_KL_w_yes = []\n",
    "all_KL_w_no = []\n",
    "sem_KL_w_no = []\n",
    "all_KL_A_yes = []\n",
    "sem_KL_A_yes = []\n",
    "all_KL_A_no = []\n",
    "sem_KL_A_no = []\n",
    "all_prob_y = []\n",
    "\n",
    "#################################################\n",
    "## TODO for students: fill in the missing variables ##\n",
    "# Fill out function and remove\n",
    "raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "#################################################\n",
    "\n",
    "for y in ...:\n",
    "    Sigma = np.diag([1./np.sqrt(y)]*2)\n",
    "    mean_KL_w = np.zeros((Nsubjects, 4))\n",
    "    mean_KL_A = np.zeros((Nsubjects, 4))\n",
    "    prob_y = np.zeros(Nsubjects)\n",
    "\n",
    "    for s in range(Nsubjects):\n",
    "        KL_w = np.zeros(len(cond))\n",
    "        KL_A = np.zeros(len(cond))\n",
    "        posteriorAware = np.zeros(len(cond))\n",
    "\n",
    "        # Generate sensory samples\n",
    "        X = np.array([multivariate_normal.rvs(mean=mu[int(c)-1, :], cov=Sigma) for c in cond])\n",
    "\n",
    "        # Model inversion for each trial\n",
    "        for i, x in enumerate(X):\n",
    "            post_w, post_A, KL_w[i], KL_A[i] = HOSS_evaluate(x, mu, Sigma, Aprior, Wprior)\n",
    "            posteriorAware[i] = post_A[1]  # Assuming post_A is a tuple with awareness probability at index 1\n",
    "\n",
    "        binaryAware = posteriorAware > 0.5\n",
    "        for i in range(4):\n",
    "            conditions = [(cond == 3), (cond != 3), (cond != 3), (cond == 3)]\n",
    "            aware_conditions = [(binaryAware == 0), (binaryAware == 0), (binaryAware == 1), (binaryAware == 1)]\n",
    "            mean_KL_w[s, i] = np.mean(KL_w[np.logical_and(aware_conditions[i], conditions[i])])\n",
    "            mean_KL_A[s, i] = np.mean(KL_A[np.logical_and(aware_conditions[i], conditions[i])])\n",
    "\n",
    "        prob_y[s] = np.mean(binaryAware[cond != 3])\n",
    "\n",
    "    # Aggregate results across subjects\n",
    "    all_KL_w_yes.append(np.mean(mean_KL_w[:, 2:4].flatten()))\n",
    "    sem_KL_w_yes.append(np.std(mean_KL_w[:, 2:4].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_w_no.append(np.mean(mean_KL_w[:, :2].flatten()))\n",
    "    sem_KL_w_no.append(np.std(mean_KL_w[:, :2].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_A_yes.append(np.mean(mean_KL_A[:, 2:4].flatten()))\n",
    "    sem_KL_A_yes.append(np.std(mean_KL_A[:, 2:4].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_A_no.append(np.mean(mean_KL_A[:, :2].flatten()))\n",
    "    sem_KL_A_no.append(np.std(mean_KL_A[:, :2].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_prob_y.append(np.mean(prob_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24eab7-5e81-4f04-bdb8-f192058d06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove solution\n",
    "\n",
    "# Experiment parameters\n",
    "mu = np.array([[3.5, 0.5], [0.5, 3.5], [0.5, 0.5]])\n",
    "Nsubjects = 30\n",
    "Ntrials = 600\n",
    "cond = np.concatenate((np.ones(Ntrials//3), np.ones(Ntrials//3)*2, np.ones(Ntrials//3)*3))\n",
    "Wprior = [0.5, 0.5]\n",
    "Aprior = 0.5\n",
    "\n",
    "# Sensory precision values\n",
    "gamma = np.linspace(0.1, 10, 6)\n",
    "\n",
    "# Initialize lists for results\n",
    "all_KL_w_yes = []\n",
    "sem_KL_w_yes = []\n",
    "all_KL_w_no = []\n",
    "sem_KL_w_no = []\n",
    "all_KL_A_yes = []\n",
    "sem_KL_A_yes = []\n",
    "all_KL_A_no = []\n",
    "sem_KL_A_no = []\n",
    "all_prob_y = []\n",
    "\n",
    "for y in gamma:\n",
    "    Sigma = np.diag([1./np.sqrt(y)]*2)\n",
    "    mean_KL_w = np.zeros((Nsubjects, 4))\n",
    "    mean_KL_A = np.zeros((Nsubjects, 4))\n",
    "    prob_y = np.zeros(Nsubjects)\n",
    "\n",
    "    for s in range(Nsubjects):\n",
    "        KL_w = np.zeros(len(cond))\n",
    "        KL_A = np.zeros(len(cond))\n",
    "        posteriorAware = np.zeros(len(cond))\n",
    "\n",
    "        # Generate sensory samples\n",
    "        X = np.array([multivariate_normal.rvs(mean=mu[int(c)-1, :], cov=Sigma) for c in cond])\n",
    "\n",
    "        # Model inversion for each trial\n",
    "        for i, x in enumerate(X):\n",
    "            post_w, post_A, KL_w[i], KL_A[i] = HOSS_evaluate(x, mu, Sigma, Aprior, Wprior)\n",
    "            posteriorAware[i] = post_A[1]  # Assuming post_A is a tuple with awareness probability at index 1\n",
    "\n",
    "        binaryAware = posteriorAware > 0.5\n",
    "        for i in range(4):\n",
    "            conditions = [(cond == 3), (cond != 3), (cond != 3), (cond == 3)]\n",
    "            aware_conditions = [(binaryAware == 0), (binaryAware == 0), (binaryAware == 1), (binaryAware == 1)]\n",
    "            mean_KL_w[s, i] = np.mean(KL_w[np.logical_and(aware_conditions[i], conditions[i])])\n",
    "            mean_KL_A[s, i] = np.mean(KL_A[np.logical_and(aware_conditions[i], conditions[i])])\n",
    "\n",
    "        prob_y[s] = np.mean(binaryAware[cond != 3])\n",
    "\n",
    "    # Aggregate results across subjects\n",
    "    all_KL_w_yes.append(np.mean(mean_KL_w[:, 2:4].flatten()))\n",
    "    sem_KL_w_yes.append(np.std(mean_KL_w[:, 2:4].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_w_no.append(np.mean(mean_KL_w[:, :2].flatten()))\n",
    "    sem_KL_w_no.append(np.std(mean_KL_w[:, :2].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_A_yes.append(np.mean(mean_KL_A[:, 2:4].flatten()))\n",
    "    sem_KL_A_yes.append(np.std(mean_KL_A[:, 2:4].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_KL_A_no.append(np.mean(mean_KL_A[:, :2].flatten()))\n",
    "    sem_KL_A_no.append(np.std(mean_KL_A[:, :2].flatten()) / np.sqrt(Nsubjects))\n",
    "    all_prob_y.append(np.mean(prob_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe9108-9540-4bed-85b3-dc8af529a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(16, 4.67))\n",
    "\n",
    "# First subplot: Probability of reporting \"seen\" for w_1 or w_2\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(gamma, all_prob_y, linewidth=2)\n",
    "plt.xlabel('SOA (precision)')\n",
    "plt.ylabel('Prob. report \"seen\" for w_1 or w_2')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.box(False)\n",
    "\n",
    "# Second subplot: K-L divergence, perceptual states\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.errorbar(gamma, all_KL_w_yes, yerr=sem_KL_w_yes, linewidth=2, label='Seen')\n",
    "plt.errorbar(gamma, all_KL_w_no, yerr=sem_KL_w_no, linewidth=2, label='Unseen')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('SOA (precision)')\n",
    "plt.ylabel('K-L divergence, perceptual states')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.box(False)\n",
    "\n",
    "# Third subplot: K-L divergence, awareness state\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.errorbar(gamma, all_KL_A_yes, yerr=sem_KL_A_yes, linewidth=2, label='Seen')\n",
    "plt.errorbar(gamma, all_KL_A_no, yerr=sem_KL_A_no, linewidth=2, label='Unseen')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('SOA (precision)')\n",
    "plt.ylabel('K-L divergence, awareness state')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.box(False)\n",
    "\n",
    "# Adjust layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df22a0-185b-496f-a2b0-7f2339f1b93c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Discussion point\n",
    "\n",
    "Can you think of experiments that could distinguish between the HOSS and GWS accounts of ignition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d83cef-d96a-4e53-836a-1d33c2f95af2",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283bdf2-27ce-4205-b654-440acf0bfc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'Bc2TT_FrcU8'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7bfbb-fa52-468b-b750-da5bff02889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'BFly8RpKiKk'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa22e6e-7cef-49fb-895b-228cfe2e513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'v3H316M5TDY'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4f657-672f-4931-956c-3039bb70137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'DMs3L-5mCxk'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27d856-8ce7-44a7-8ed6-458fb21688e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'WwY6rWeOCCc'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4243f-fbff-425c-97ec-7a55d42719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', '6fYcB8wZ5Ko'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444ad01-1ab0-4e84-aa32-0456b3aa9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'UzYjWizcZRw'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial1",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
