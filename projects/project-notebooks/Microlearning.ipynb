{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada5523b",
   "metadata": {
    "id": "ada5523b"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Microlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Microlearning.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045624d0",
   "metadata": {
    "execution": {},
    "id": "045624d0"
   },
   "source": [
    "# Microlearning: Credit assignment and local learning rules in artificial and neural systems\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Colleen J. Gillon & Klara Kaleb\n",
    "\n",
    "__Content reviewers:__ Colleen J. Gillon, Klara Kaleb, Eva Dyer\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da422f",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580,
     "referenced_widgets": [
      "02a13bafa5e049c2b5e202879dcea721",
      "00587a334b4e4d46bdb5b5cf59941499",
      "bd1e978893bf4c64baab06fc71290d28",
      "4707c7edbca646f4a703a1a425026f53",
      "4decccb058e44bcd949199d74f04d3f9",
      "77ac927bd67b46cb9c3935000debbd08",
      "52e4ef1a33b3488492e5c516215e93b4",
      "7cd92d2728ad4048b02fe98892f5da18"
     ]
    },
    "id": "64da422f",
    "outputId": "12570ea6-fd92-4c10-97e9-18efa84bda9b"
   },
   "outputs": [],
   "source": [
    "# @title Project Background\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "# video_ids = [('Youtube', '<video_id_1>')]\n",
    "# tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "# tabs = widgets.Tab()\n",
    "# tabs.children = tab_contents\n",
    "# for i in range(len(tab_contents)):\n",
    "#   tabs.set_title(i, video_ids[i][0])\n",
    "# display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fef8b",
   "metadata": {
    "cellView": "form",
    "id": "b52fef8b"
   },
   "outputs": [],
   "source": [
    "# @title Project slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "link_id = \"fjaqp\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b697d5",
   "metadata": {
    "execution": {},
    "id": "63b697d5"
   },
   "source": [
    "**Background:** To learn effectively, our brain must coordinate synaptic updates across its network of neurons. The question of how the brain does this is called the credit assignment problem. Deep neural networks are a leading model of learning in the brain, and are typically trained using gradient descent via backpropagation. However, backpropagation is widely agreed to be biologically implausible. Therefore, to understand how the brain solves the credit assignment problem, we must find learning rules that are both biologically plausible and effective for learning. In this project, we will explore more biologically plausible learning rules proposed as alternatives to backpropagation, compare them to error backpropagation, and test whether we can infer what type of learning rule the brain might be using.\n",
    "\n",
    "**Project setup:** This project builds on a basic feedforward network trained to classify MNIST images (Q1). We then implement biologically plausible rules, compute performance and learning-related metrics (Q2-Q4 & Q7), to then evaluate (1) how consistent and learning-rule specific the metrics are (Q5, Q8-9), or (2) how these rules fare in more complex learning scenarios (Q6, Q10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3kgBXP7RVniQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "3kgBXP7RVniQ",
    "outputId": "4a45cdf0-1474-4a90-effd-36bce6bfa89c"
   },
   "outputs": [],
   "source": [
    "#@title Project Template\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/MicrolearningProjectTemplate.svg?raw=true\"\n",
    "\n",
    "display(Image(url=url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D2BZcAF675Ui",
   "metadata": {
    "id": "D2BZcAF675Ui"
   },
   "source": [
    "### Reference Material\n",
    "- Course materials from Neuromatch's NeuroAI day on Microlearning\n",
    "- Review discussing backpropagation in the brain: [Lillicrap et al., 2020, Nature Reviews Neuroscience](https://www.nature.com/articles/s41583-020-0277-3)\n",
    "- Study on the reliability of different metrics for inferring learning rules: [Nayebi et al., 2020, NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51243f3",
   "metadata": {
    "execution": {},
    "id": "f51243f3"
   },
   "source": [
    "## Section 1: Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e541a40",
   "metadata": {
    "cellView": "form",
    "id": "1e541a40"
   },
   "outputs": [],
   "source": [
    "# @title Importing dependencies\n",
    "\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07545499",
   "metadata": {
    "cellView": "form",
    "id": "07545499"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/cma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ti0I8hj_XiD6",
   "metadata": {
    "id": "Ti0I8hj_XiD6"
   },
   "source": [
    "### 1.1 Download MNIST dataset\n",
    "\n",
    "The first step is to download the [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digits dataset [1], which you will be using in this project. It is provided as a training dataset (60,000 examples) and a test dataset (10,000 examples). We can split the training dataset to obtain a training (e.g., 80%) and a validation set (e.g., 20%). In addition, since the dataset is quite large, we also suggest keeping only half of each subset, as this will make training the models faster.\n",
    "\n",
    "\n",
    "[1] Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6), 141â€“142, https://ieeexplore.ieee.org/document/6296535.\n",
    "\n",
    "**Note:** The download process may try a few sources before succeeding. `HTTP Error 503: Service Unavailable` errors can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FHoIHYDvWAfW",
   "metadata": {
    "cellView": "form",
    "id": "FHoIHYDvWAfW"
   },
   "outputs": [],
   "source": [
    "# @markdown `download_mnist()`: Function to download MNIST.\n",
    "\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "  full_train_set = torchvision.datasets.MNIST(\n",
    "      root=\"./data/\", train=True, download=True, transform=transform\n",
    "      )\n",
    "  full_test_set = torchvision.datasets.MNIST(\n",
    "      root=\"./data/\", train=False, download=True, transform=transform\n",
    "      )\n",
    "\n",
    "  train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set,\n",
    "      [train_prop * keep_prop, valid_prop * keep_prop, discard_prop]\n",
    "      )\n",
    "  test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set,\n",
    "      [keep_prop, discard_prop]\n",
    "      )\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OUZf-8rDduiV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUZf-8rDduiV",
    "outputId": "d0e36529-07ef-41e6-9320-a39b88052f12"
   },
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fXzRit8_Hq6_",
   "metadata": {
    "id": "fXzRit8_Hq6_"
   },
   "source": [
    "#### 1.2 Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FkO3wnAC9FlN",
   "metadata": {
    "cellView": "form",
    "id": "FkO3wnAC9FlN"
   },
   "outputs": [],
   "source": [
    "#@markdown To get started exploring the dataset, here are a few plotting functions:\n",
    "\n",
    "#@markdown `get_plotting_color()`: Returns a color for the specific dataset, e.g. \"train\" or model index.\n",
    "def get_plotting_color(dataset=\"train\", model_idx=None):\n",
    "  if model_idx is not None:\n",
    "    dataset = None\n",
    "\n",
    "  if model_idx == 0 or dataset == \"train\":\n",
    "    color = \"#1F77B4\" # blue\n",
    "  elif model_idx == 1 or dataset == \"valid\":\n",
    "    color = \"#FF7F0E\" # orange\n",
    "  elif model_idx == 2 or dataset == \"test\":\n",
    "    color = \"#2CA02C\" # green\n",
    "  else:\n",
    "    if model_idx is not None:\n",
    "      raise NotImplementedError(\"Colors only implemented for up to 3 models.\")\n",
    "    else:\n",
    "      raise NotImplementedError(\n",
    "          f\"{dataset} dataset not recognized. Expected 'train', 'valid' \"\n",
    "          \"or 'test'.\"\n",
    "          )\n",
    "\n",
    "  return color\n",
    "\n",
    "\n",
    "#@markdown `plot_examples(subset)`: Plot examples from the dataset organized by their predicted class\n",
    "#@markdown (if a model is provided) or by their class label otherwise\n",
    "def plot_examples(subset, num_examples_per_class=8, MLP=None, seed=None,\n",
    "                  batch_size=32, num_classes=10, ax=None):\n",
    "  \"\"\"\n",
    "  Function for visualizing example images from the dataset, organized by their\n",
    "  predicted class, if a model is provided, or by their class, otherwise.\n",
    "\n",
    "  Arguments:\n",
    "  - subset (torch dataset or torch dataset subset): dataset from which to\n",
    "    visualized images.\n",
    "  - num_examples_per_class (int, optional): number of examples to visualize per\n",
    "    class\n",
    "  - MLP (MultiLayerPerceptron or None, optional): model to use to retrieve the\n",
    "    predicted class for each image. If MLP is None, images will be organized by\n",
    "    their class label. Otherwise, images will be organized by their predicted\n",
    "    class.\n",
    "  - seed (int or None, optional): Seed to use to randomly sample images to\n",
    "    visualize.\n",
    "  - batch_size (int, optional): If MLP is not None, number of images to\n",
    "    retrieve predicted class for at one time.\n",
    "  - num_classes (int, optional): Number of classes in the data.\n",
    "  - ax (plt subplot, optional): Axis on which to plot images. If None, a new\n",
    "    axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which images were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  if MLP is None:\n",
    "    xlabel = \"Class\"\n",
    "  else:\n",
    "    MLP.eval()\n",
    "    xlabel = \"Predicted class\"\n",
    "\n",
    "  if ax is None:\n",
    "    fig_wid = min(8, num_classes * 0.6)\n",
    "    fig_hei = min(8, num_examples_per_class * 0.6)\n",
    "    _, ax = plt.subplots(figsize=(fig_wid, fig_hei))\n",
    "\n",
    "  if seed is None:\n",
    "    generator = None\n",
    "  else:\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "  loader = torch.utils.data.DataLoader(\n",
    "      subset, batch_size=batch_size, shuffle=True, generator=generator\n",
    "      )\n",
    "\n",
    "  plot_images = {i: list() for i in range(num_classes)}\n",
    "  with torch.no_grad():\n",
    "    for X, y in loader:\n",
    "      if MLP is not None:\n",
    "        y = MLP(X)\n",
    "        y = torch.argmax(y, axis=1)\n",
    "\n",
    "      done = True\n",
    "      for i in range(num_classes):\n",
    "        num_to_add = int(num_examples_per_class - len(plot_images[i]))\n",
    "        if num_to_add:\n",
    "          add_images = np.where(y == i)[0]\n",
    "          if len(add_images):\n",
    "            for add_i in add_images[: num_to_add]:\n",
    "              plot_images[i].append(X[add_i, 0].numpy())\n",
    "          if len(plot_images[i]) != num_examples_per_class:\n",
    "            done = False\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "  hei, wid = X[0, 0].shape\n",
    "  final_image = np.full((num_examples_per_class * hei, num_classes * wid), np.nan)\n",
    "  for i, images in plot_images.items():\n",
    "    if len(images):\n",
    "      final_image[: len(images) * hei, i * wid: (i + 1) * wid] = np.vstack(images)\n",
    "\n",
    "  ax.imshow(final_image, cmap=\"gray\")\n",
    "\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_xticks((np.arange(num_classes) + 0.5) * wid)\n",
    "  ax.set_xticklabels([f\"{int(i)}\" for i in range(num_classes)])\n",
    "  ax.set_yticks([])\n",
    "  ax.set_title(f\"Examples per {xlabel.lower()}\")\n",
    "\n",
    "  return ax\n",
    "\n",
    "#@markdown `plot_class_distribution(train_set)`: Plots the distribution of classes in each set (train, validation, test).\n",
    "def plot_class_distribution(train_set, valid_set=None, test_set=None,\n",
    "                            num_classes=10, ax=None):\n",
    "  \"\"\"\n",
    "  Function for plotting the number of examples per class in each subset.\n",
    "\n",
    "  Arguments:\n",
    "  - train_set (torch dataset or torch dataset subset): training dataset\n",
    "  - valid_set (torch dataset or torch dataset subset, optional): validation\n",
    "    dataset\n",
    "  - test_set (torch dataset or torch dataset subset, optional): test\n",
    "    dataset\n",
    "  - num_classes (int, optional): Number of classes in the data.\n",
    "  - ax (plt subplot, optional): Axis on which to plot images. If None, a new\n",
    "    axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which images were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  if ax is None:\n",
    "    _, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "  bins = np.arange(num_classes + 1) - 0.5\n",
    "\n",
    "  for dataset_name, dataset in [\n",
    "      (\"train\", train_set), (\"valid\", valid_set), (\"test\", test_set)\n",
    "      ]:\n",
    "    if dataset is None:\n",
    "      continue\n",
    "\n",
    "    if hasattr(dataset, \"dataset\"):\n",
    "      targets = dataset.dataset.targets[dataset.indices]\n",
    "    else:\n",
    "      targets = dataset.targets\n",
    "\n",
    "    outputs = ax.hist(\n",
    "        targets,\n",
    "        bins=bins,\n",
    "        alpha=0.3,\n",
    "        color=get_plotting_color(dataset_name),\n",
    "        label=dataset_name,\n",
    "        )\n",
    "\n",
    "    per_class = len(targets) / num_classes\n",
    "    ax.axhline(\n",
    "        per_class,\n",
    "        ls=\"dashed\",\n",
    "        color=get_plotting_color(dataset_name),\n",
    "        alpha=0.8\n",
    "        )\n",
    "\n",
    "  ax.set_xticks(range(num_classes))\n",
    "  ax.set_title(\"Counts per class\")\n",
    "  ax.set_xlabel(\"Class\")\n",
    "  ax.set_ylabel(\"Count\")\n",
    "  ax.legend(loc=\"center right\")\n",
    "\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oQvgjVmgrd2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "oQvgjVmgrd2x",
    "outputId": "d6c68a14-939d-4074-92c3-cabc8e8479e3"
   },
   "outputs": [],
   "source": [
    "plot_examples(train_set);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DUVRnyEBrA9c",
   "metadata": {
    "id": "DUVRnyEBrA9c"
   },
   "source": [
    "## Section 2: Training a basic model\n",
    "\n",
    "### 2.1 Defining a basic model\n",
    "\n",
    "Next, we can define a basic model to train on this MNIST classification task.\n",
    "\n",
    "First, it is helpful to define a **few hyperparameters** (`NUM_INPUTS` and `NUM_OUTPUTS`) to store the input size and output size the model needs to have for this task.\n",
    "\n",
    "The `MultiLayerPerceptron` class, provided here, initializes a **multilayer perceptron (MLP) with one hidden layer**. Feel free to expand or change the class, if you would like to use a different or more complex model, or add functionalities.\n",
    "\n",
    "This class has several **basic methods**:\n",
    "- `__init__(self)`: To initialize the model.\n",
    "- `_set_activation(self)`: To set the specified activation function for the hidden layer. (The output layer has a softmax activation.)\n",
    "- `forward(self, X)`: To define how activity is passed through the model.\n",
    "\n",
    "It also has additional methods that will be **helpful later on** for collecting metrics from the models:\n",
    "- `_store_initial_weights_biases(self)`: To store the initial weights and biases.\n",
    "- `forward_backprop(self, X)`: For when we will be comparing the gradients computed by alternative learning rules to the gradients computed by error backpropagation.\n",
    "- `list_parameters(self)`: For convenience in retrieving a list of the model's parameters.\n",
    "- `gather_gradient_dict(self)`: For gathering the gradients of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6MM59QBslKf",
   "metadata": {
    "id": "x6MM59QBslKf"
   },
   "outputs": [],
   "source": [
    "NUM_INPUTS = np.product(train_set.dataset.data[0].shape) # size of an MNIST image\n",
    "NUM_OUTPUTS = 10 # number of MNIST classes\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Simple multilayer perceptron model class with one hidden layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_inputs=NUM_INPUTS,\n",
    "      num_hidden=100,\n",
    "      num_outputs=NUM_OUTPUTS,\n",
    "      activation_type=\"sigmoid\",\n",
    "      bias=False,\n",
    "      ):\n",
    "    \"\"\"\n",
    "    Initializes a multilayer perceptron with a single hidden layer.\n",
    "\n",
    "    Arguments:\n",
    "    - num_inputs (int, optional): number of input units (i.e., image size)\n",
    "    - num_hidden (int, optional): number of hidden units in the hidden layer\n",
    "    - num_outputs (int, optional): number of output units (i.e., number of\n",
    "      classes)\n",
    "    - activation_type (str, optional): type of activation to use for the hidden\n",
    "      layer ('sigmoid', 'tanh', 'relu' or 'linear')\n",
    "    - bias (bool, optional): if True, each linear layer will have biases in\n",
    "      addition to weights\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_inputs = num_inputs\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_outputs = num_outputs\n",
    "    self.activation_type = activation_type\n",
    "    self.bias = bias\n",
    "\n",
    "    # default weights (and biases, if applicable) initialization is used\n",
    "    # see https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py\n",
    "    self.lin1 = torch.nn.Linear(num_inputs, num_hidden, bias=bias)\n",
    "    self.lin2 = torch.nn.Linear(num_hidden, num_outputs, bias=bias)\n",
    "\n",
    "    self._store_initial_weights_biases()\n",
    "\n",
    "    self._set_activation() # activation on the hidden layer\n",
    "    self.softmax = torch.nn.Softmax(dim=1) # activation on the output layer\n",
    "\n",
    "\n",
    "  def _store_initial_weights_biases(self):\n",
    "    \"\"\"\n",
    "    Stores a copy of the network's initial weights and biases.\n",
    "    \"\"\"\n",
    "\n",
    "    self.init_lin1_weight = self.lin1.weight.data.clone()\n",
    "    self.init_lin2_weight = self.lin2.weight.data.clone()\n",
    "    if self.bias:\n",
    "      self.init_lin1_bias = self.lin1.bias.data.clone()\n",
    "      self.init_lin2_bias = self.lin2.bias.data.clone()\n",
    "\n",
    "  def _set_activation(self):\n",
    "    \"\"\"\n",
    "    Sets the activation function used for the hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    if self.activation_type.lower() == \"sigmoid\":\n",
    "      self.activation = torch.nn.Sigmoid() # maps to [0, 1]\n",
    "    elif self.activation_type.lower() == \"tanh\":\n",
    "      self.activation = torch.nn.Tanh() # maps to [-1, 1]\n",
    "    elif self.activation_type.lower() == \"relu\":\n",
    "      self.activation = torch.nn.ReLU() # maps to positive\n",
    "    elif self.activation_type.lower() == \"identity\":\n",
    "      self.activation = torch.nn.Identity() # maps to same\n",
    "    else:\n",
    "      raise NotImplementedError(\n",
    "          f\"{self.activation_type} activation type not recognized. Only \"\n",
    "          \"'sigmoid', 'relu' and 'identity' have been implemented so far.\"\n",
    "          )\n",
    "\n",
    "  def forward(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through the network.\n",
    "\n",
    "    Arguments:\n",
    "    - X (torch.Tensor): Batch of input images.\n",
    "    - y (torch.Tensor, optional): Batch of targets. This variable is not used\n",
    "      here. However, it may be needed for other learning rules, to it is\n",
    "      included as an argument here for compatibility.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred (torch.Tensor): Predicted targets.\n",
    "    \"\"\"\n",
    "\n",
    "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
    "    y_pred = self.softmax(self.lin2(h))\n",
    "    return y_pred\n",
    "\n",
    "  def forward_backprop(self, X):\n",
    "    \"\"\"\n",
    "    Identical to forward(). Should not be overwritten when creating new\n",
    "    child classes to implement other learning rules, as this method is used\n",
    "    to compare the gradients calculated with other learning rules to those\n",
    "    calculated with backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
    "    y_pred = self.softmax(self.lin2(h))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "  def list_parameters(self):\n",
    "    \"\"\"\n",
    "    Returns a list of model names for a gradient dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - params_list (list): List of parameter names.\n",
    "    \"\"\"\n",
    "\n",
    "    params_list = list()\n",
    "\n",
    "    for layer_str in [\"lin1\", \"lin2\"]:\n",
    "      params_list.append(f\"{layer_str}_weight\")\n",
    "      if self.bias:\n",
    "        params_list.append(f\"{layer_str}_bias\")\n",
    "\n",
    "    return params_list\n",
    "\n",
    "\n",
    "  def gather_gradient_dict(self):\n",
    "    \"\"\"\n",
    "    Gathers a gradient dictionary for the model's parameters. Raises a\n",
    "    runtime error if any parameters have no gradients.\n",
    "\n",
    "    Returns:\n",
    "    - gradient_dict (dict): A dictionary of gradients for each parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    params_list = self.list_parameters()\n",
    "\n",
    "    gradient_dict = dict()\n",
    "    for param_name in params_list:\n",
    "      layer_str, param_str = param_name.split(\"_\")\n",
    "      layer = getattr(self, layer_str)\n",
    "      grad = getattr(layer, param_str).grad\n",
    "      if grad is None:\n",
    "        raise RuntimeError(\"No gradient was computed\")\n",
    "      gradient_dict[param_name] = grad.detach().clone().numpy()\n",
    "\n",
    "    return gradient_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waoWxi2otR9C",
   "metadata": {
    "id": "waoWxi2otR9C"
   },
   "source": [
    "### 2.2 Initializing the model\n",
    "\n",
    "We can now **initialize an MLP**. Feel free to change the number of hidden units, change the activation function or include biases in the model.  \n",
    "\n",
    "Currently, the `\"sigmoid\"`, `\"TanH\"`, `\"ReLU\"` and `\"identity\"` activation functions are implemented, but you can add more by editing the `_set_activation(self)` method of the `MultiLayerPerceptron` class defined above.\n",
    "\n",
    "We have set `BIAS=False` for simplicity.\n",
    "\n",
    "We will also initialize the dataloaders. Feel free to select a different batch size (`BATCH_SIZE`) when training your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cFV5yFA7tCwa",
   "metadata": {
    "id": "cFV5yFA7tCwa"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "NUM_HIDDEN = 100\n",
    "ACTIVATION = \"sigmoid\" # output constrained between 0 and 1\n",
    "BIAS = False\n",
    "\n",
    "MLP = MultiLayerPerceptron(\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    activation_type=ACTIVATION,\n",
    "    bias=BIAS,\n",
    "    )\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PurpMt6J01hv",
   "metadata": {
    "id": "PurpMt6J01hv"
   },
   "source": [
    "### 2.3 Defining and initializing an optimizer\n",
    "\n",
    "Here, we define a **basic optimizer** that updates the weights and biases of the model based on the gradients saved. This optimizer is equivalent to a simple [Stochastic Gradient Descent optimizer](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) (`torch.optim.SGD()`) applied to mini-batch data.\n",
    "\n",
    "It has two methods:\n",
    "- `__init__(self)`: To initialize the optimizer. Any arguments passed after `params` should be added to the `defaults` dictionary, which is passed to the parents class. These arguments are then added to each parameter group's dictionary, allowing them to be accessed in `step(self)`.\n",
    "- `step(self)`: Makes an update to the model parameters.\n",
    "\n",
    "This optimizer can be extended later, if needed, when implementing more complex learning rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aNovmyCx0zUd",
   "metadata": {
    "id": "aNovmyCx0zUd"
   },
   "outputs": [],
   "source": [
    "class BasicOptimizer(torch.optim.Optimizer):\n",
    "  \"\"\"\n",
    "  Simple optimizer class based on the SGD optimizer.\n",
    "  \"\"\"\n",
    "  def __init__(self, params, lr=0.01, weight_decay=0):\n",
    "    \"\"\"\n",
    "    Initializes a basic optimizer object.\n",
    "\n",
    "    Arguments:\n",
    "    - params (generator): Generator for torch model parameters.\n",
    "    - lr (float, optional): Learning rate.\n",
    "    - weight_decay (float, optional): Weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    if lr < 0.0:\n",
    "        raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "    if weight_decay < 0.0:\n",
    "        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "\n",
    "    defaults = dict(\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "    super().__init__(params, defaults)\n",
    "\n",
    "  def step(self):\n",
    "      \"\"\"\n",
    "      Performs a single optimization step.\n",
    "      \"\"\"\n",
    "\n",
    "      for group in self.param_groups:\n",
    "        for p in group[\"params\"]:\n",
    "\n",
    "          # only update parameters with gradients\n",
    "          if p.grad is not None:\n",
    "\n",
    "            # apply weight decay to gradient, if applicable\n",
    "            if group[\"weight_decay\"] != 0:\n",
    "              p.grad = p.grad.add(p, alpha=group[\"weight_decay\"])\n",
    "\n",
    "            # apply gradient-based update\n",
    "            p.data.add_(p.grad, alpha=-group[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cx06CK5G2SVa",
   "metadata": {
    "id": "cx06CK5G2SVa"
   },
   "source": [
    "We can now **initialize an optimizer**. Feel free to change to learning rate (`LR`) that the optimizer is initialized with, or to add a weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-c2PQOC006r",
   "metadata": {
    "id": "0-c2PQOC006r"
   },
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "backprop_optimizer = BasicOptimizer(MLP.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FOsSwkV9tsoV",
   "metadata": {
    "id": "FOsSwkV9tsoV"
   },
   "source": [
    "### 2.4 Training a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LHnydloQuU3F",
   "metadata": {
    "cellView": "form",
    "id": "LHnydloQuU3F"
   },
   "outputs": [],
   "source": [
    "#@markdown `train_model(MLP, train_loader, valid_loader, optimizer)`: Main function.\n",
    "#@markdown Trains the model across epochs. Aggregates loss and accuracy statistics\n",
    "#@markdown from the training and validation datasets into a results dictionary which is returned.\n",
    "def train_model(MLP, train_loader, valid_loader, optimizer, num_epochs=5):\n",
    "  \"\"\"\n",
    "  Train a model for several epochs.\n",
    "\n",
    "  Arguments:\n",
    "  - MLP (torch model): Model to train.\n",
    "  - train_loader (torch dataloader): Dataloader to use to train the model.\n",
    "  - valid_loader (torch dataloader): Dataloader to use to validate the model.\n",
    "  - optimizer (torch optimizer): Optimizer to use to update the model.\n",
    "  - num_epochs (int, optional): Number of epochs to train model.\n",
    "\n",
    "  Returns:\n",
    "  - results_dict (dict): Dictionary storing results across epochs on training\n",
    "    and validation data.\n",
    "  \"\"\"\n",
    "\n",
    "  results_dict = {\n",
    "      \"avg_train_losses\": list(),\n",
    "      \"avg_valid_losses\": list(),\n",
    "      \"avg_train_accuracies\": list(),\n",
    "      \"avg_valid_accuracies\": list(),\n",
    "  }\n",
    "\n",
    "  for e in tqdm(range(num_epochs)):\n",
    "    no_train = True if e == 0 else False # to get a baseline\n",
    "    latest_epoch_results_dict = train_epoch(\n",
    "        MLP, train_loader, valid_loader, optimizer=optimizer, no_train=no_train\n",
    "        )\n",
    "\n",
    "    for key, result in latest_epoch_results_dict.items():\n",
    "      if key in results_dict.keys() and isinstance(results_dict[key], list):\n",
    "        results_dict[key].append(latest_epoch_results_dict[key])\n",
    "      else:\n",
    "        results_dict[key] = result # copy latest\n",
    "\n",
    "  return results_dict\n",
    "\n",
    "\n",
    "def train_epoch(MLP, train_loader, valid_loader, optimizer, no_train=False):\n",
    "  \"\"\"\n",
    "  Train a model for one epoch.\n",
    "\n",
    "  Arguments:\n",
    "  - MLP (torch model): Model to train.\n",
    "  - train_loader (torch dataloader): Dataloader to use to train the model.\n",
    "  - valid_loader (torch dataloader): Dataloader to use to validate the model.\n",
    "  - optimizer (torch optimizer): Optimizer to use to update the model.\n",
    "  - no_train (bool, optional): If True, the model is not trained for the\n",
    "    current epoch. Allows a baseline (chance) performance to be computed in the\n",
    "    first epoch before training starts.\n",
    "\n",
    "  Returns:\n",
    "  - epoch_results_dict (dict): Dictionary storing epoch results on training\n",
    "    and validation data.\n",
    "  \"\"\"\n",
    "\n",
    "  criterion = torch.nn.NLLLoss()\n",
    "\n",
    "  epoch_results_dict = dict()\n",
    "  for dataset in [\"train\", \"valid\"]:\n",
    "    for sub_str in [\"correct_by_class\", \"seen_by_class\"]:\n",
    "      epoch_results_dict[f\"{dataset}_{sub_str}\"] = {\n",
    "          i:0 for i in range(MLP.num_outputs)\n",
    "          }\n",
    "\n",
    "  MLP.train()\n",
    "  train_losses, train_acc = list(), list()\n",
    "  for X, y in train_loader:\n",
    "    y_pred = MLP(X, y=y)\n",
    "    loss = criterion(torch.log(y_pred), y)\n",
    "    acc = (torch.argmax(y_pred.detach(), axis=1) == y).sum() / len(y)\n",
    "    train_losses.append(loss.item() * len(y))\n",
    "    train_acc.append(acc.item() * len(y))\n",
    "    update_results_by_class_in_place(\n",
    "        y, y_pred.detach(), epoch_results_dict, dataset=\"train\",\n",
    "        num_classes=MLP.num_outputs\n",
    "        )\n",
    "    optimizer.zero_grad()\n",
    "    if not no_train:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  num_items = len(train_loader.dataset)\n",
    "  epoch_results_dict[\"avg_train_losses\"] = np.sum(train_losses) / num_items\n",
    "  epoch_results_dict[\"avg_train_accuracies\"] = np.sum(train_acc) / num_items * 100\n",
    "\n",
    "  MLP.eval()\n",
    "  valid_losses, valid_acc = list(), list()\n",
    "  with torch.no_grad():\n",
    "    for X, y in valid_loader:\n",
    "      y_pred = MLP(X)\n",
    "      loss = criterion(torch.log(y_pred), y)\n",
    "      acc = (torch.argmax(y_pred, axis=1) == y).sum() / len(y)\n",
    "      valid_losses.append(loss.item() * len(y))\n",
    "      valid_acc.append(acc.item() * len(y))\n",
    "      update_results_by_class_in_place(\n",
    "          y, y_pred.detach(), epoch_results_dict, dataset=\"valid\"\n",
    "          )\n",
    "\n",
    "  num_items = len(valid_loader.dataset)\n",
    "  epoch_results_dict[\"avg_valid_losses\"] = np.sum(valid_losses) / num_items\n",
    "  epoch_results_dict[\"avg_valid_accuracies\"] = np.sum(valid_acc) / num_items * 100\n",
    "\n",
    "  return epoch_results_dict\n",
    "\n",
    "\n",
    "def update_results_by_class_in_place(y, y_pred, result_dict, dataset=\"train\",\n",
    "                                     num_classes=10):\n",
    "  \"\"\"\n",
    "  Updates results dictionary in place during a training epoch by adding data\n",
    "  needed to compute the accuracies for each class.\n",
    "\n",
    "  Arguments:\n",
    "  - y (torch Tensor): target labels\n",
    "  - y_pred (torch Tensor): predicted targets\n",
    "  - result_dict (dict): Dictionary storing epoch results on training\n",
    "    and validation data.\n",
    "  - dataset (str, optional): Dataset for which results are being added.\n",
    "  - num_classes (int, optional): Number of classes.\n",
    "  \"\"\"\n",
    "\n",
    "  correct_by_class = None\n",
    "  seen_by_class = None\n",
    "\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  if len(y) != len(y_pred):\n",
    "    raise RuntimeError(\"Number of predictions does not match number of targets.\")\n",
    "\n",
    "  for i in result_dict[f\"{dataset}_seen_by_class\"].keys():\n",
    "    idxs = np.where(y == int(i))[0]\n",
    "    result_dict[f\"{dataset}_seen_by_class\"][int(i)] += len(idxs)\n",
    "\n",
    "    num_correct = int(sum(y[idxs] == y_pred[idxs]))\n",
    "    result_dict[f\"{dataset}_correct_by_class\"][int(i)] += num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xneZmDfAvMYc",
   "metadata": {
    "id": "xneZmDfAvMYc"
   },
   "source": [
    "Once the model and optimizer have been initialized, we can now **train our model** using backpropagation for a few epochs, and collect the classification results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_kjJdjWTz2b_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kjJdjWTz2b_",
    "outputId": "91ac03c6-fde4-419d-8331-85402083f2fe"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "MLP_results_dict = train_model(\n",
    "    MLP,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer=backprop_optimizer,\n",
    "    num_epochs=NUM_EPOCHS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V11y_907CDBj",
   "metadata": {
    "id": "V11y_907CDBj"
   },
   "source": [
    "**Note:** The training function does not use the `test_loader`. This additional dataloader can be used to evaluate the models if, for example, you need use the validation set for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SXed7igP3Bs_",
   "metadata": {
    "id": "SXed7igP3Bs_"
   },
   "source": [
    "## Section 3. Inspecting a model's performance\n",
    "\n",
    "The **results dictionary** (`MLP_results_dict`) returned by `train_model()` contains the following keys:\n",
    "- `avg_train_losses`: Average training loss per epoch.\n",
    "- `avg_valid_losses`: Average validation loss per epoch.\n",
    "- `avg_train_accuracies`: Average training accuracies per epoch.\n",
    "- `avg_valid_losses`: Average validation accuracies per epoch.\n",
    "- `train_correct_by_class`: Number of correctly classified training images for each class (last epoch only).\n",
    "- `train_seen_by_class`: Number of training images for each class (last epoch only).\n",
    "- `valid_correct_by_class`: Number of correctly classified validation images for each class (last epoch only).\n",
    "- `valid_seen_by_class`: Number of validation images for each class (last epoch only).\n",
    "\n",
    "Next, we can inspect our model's performance by visualizing various **metrics**, e.g., classification loss, accuracy, accuracy by class, weights. A few example functions are provided for plotted various metrics collected across learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ODYm9uG0qxBo",
   "metadata": {
    "cellView": "form",
    "id": "ODYm9uG0qxBo"
   },
   "outputs": [],
   "source": [
    "#@markdown `plot_results(results_dict)`: Plots classification losses and\n",
    "#@markdown accuracies across epochs for the training and validation sets.\n",
    "def plot_results(results_dict, num_classes=10, ax=None):\n",
    "  \"\"\"\n",
    "  Function for plotting losses and accuracies across learning.\n",
    "\n",
    "  Arguments:\n",
    "  - results_dict (dict): Dictionary storing results across epochs on training\n",
    "    and validation data.\n",
    "  - num_classes (float, optional): Number of classes, used to calculate chance\n",
    "    accuracy.\n",
    "  - ax (plt subplot, optional): Axis on which to plot results. If None, a new\n",
    "    axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which results were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  if ax is None:\n",
    "    _, ax = plt.subplots(figsize=(7, 3.5))\n",
    "\n",
    "  loss_ax = ax\n",
    "  acc_ax = None\n",
    "\n",
    "  chance = 100 / num_classes\n",
    "\n",
    "  plotted = False\n",
    "  for result_type in [\"losses\", \"accuracies\"]:\n",
    "    for dataset in [\"train\", \"valid\"]:\n",
    "      key = f\"avg_{dataset}_{result_type}\"\n",
    "      if key in results_dict.keys():\n",
    "        if result_type == \"losses\":\n",
    "          ylabel = \"Loss\"\n",
    "          plot_ax = loss_ax\n",
    "          ls = None\n",
    "        elif result_type == \"accuracies\":\n",
    "          if acc_ax is None:\n",
    "            acc_ax = ax.twinx()\n",
    "            acc_ax.spines[[\"right\"]].set_visible(True)\n",
    "            acc_ax.axhline(chance, ls=\"dashed\", color=\"k\", alpha=0.8)\n",
    "            acc_ax.set_ylim(-5, 105)\n",
    "          ylabel = \"Accuracy (%)\"\n",
    "          plot_ax = acc_ax\n",
    "          ls = \"dashed\"\n",
    "        else:\n",
    "          raise RuntimeError(f\"{result_type} result type not recognized.\")\n",
    "\n",
    "        data = results_dict[key]\n",
    "        plot_ax.plot(\n",
    "            data,\n",
    "            ls=ls,\n",
    "            label=dataset,\n",
    "            alpha=0.8,\n",
    "            color=get_plotting_color(dataset)\n",
    "            )\n",
    "        plot_ax.set_ylabel(ylabel)\n",
    "        plotted = True\n",
    "\n",
    "  if plotted:\n",
    "    ax.legend(loc=\"center left\")\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    ax.set_xticklabels([f\"{int(e)}\" for e in range(len(data))])\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    if ymin > 0:\n",
    "      ymin = 0\n",
    "      pad = (ymax - ymin) * 0.05\n",
    "      ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "  else:\n",
    "    raise RuntimeError(\"No data found to plot.\")\n",
    "\n",
    "  ax.set_title(\"Performance across learning\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "  return ax\n",
    "\n",
    "\n",
    "#@markdown `plot_scores_per_class(results_dict)`: Plots the classification\n",
    "#@markdown accuracies by class for the training and validation sets (for the last epoch).\n",
    "def plot_scores_per_class(results_dict, num_classes=10, ax=None):\n",
    "  \"\"\"\n",
    "  Function for plotting accuracy scores for each class.\n",
    "\n",
    "  Arguments:\n",
    "  - results_dict (dict): Dictionary storing results across epochs on training\n",
    "    and validation data.\n",
    "  - num_classes (int, optional): Number of classes in the data.\n",
    "  - ax (plt subplot, optional): Axis on which to plot accuracies. If None, a new\n",
    "    axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which accuracies were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  if ax is None:\n",
    "    _, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "  avgs = list()\n",
    "  ax.set_prop_cycle(None) # reset color cycle\n",
    "  for s, dataset in enumerate([\"train\", \"valid\"]):\n",
    "    correct_by_class = results_dict[f\"{dataset}_correct_by_class\"]\n",
    "    seen_by_class = results_dict[f\"{dataset}_seen_by_class\"]\n",
    "    xs, ys = list(), list()\n",
    "    for i, total in seen_by_class.items():\n",
    "      xs.append(i + 0.3 * (s - 0.5))\n",
    "      if total == 0:\n",
    "        ys.append(np.nan)\n",
    "      else:\n",
    "        ys.append(100 * correct_by_class[i] / total)\n",
    "\n",
    "    avg_key = f\"avg_{dataset}_accuracies\"\n",
    "    if avg_key in results_dict.keys():\n",
    "      ax.axhline(\n",
    "          results_dict[avg_key][-1], ls=\"dashed\", alpha=0.8,\n",
    "          color=get_plotting_color(dataset)\n",
    "          )\n",
    "\n",
    "    ax.bar(\n",
    "        xs, ys, label=dataset, width=0.3, alpha=0.8,\n",
    "        color=get_plotting_color(dataset)\n",
    "        )\n",
    "\n",
    "  ax.set_xticks(range(num_classes))\n",
    "  ax.set_xlabel(\"Class\")\n",
    "  ax.set_ylabel(\"Accuracy (%)\")\n",
    "  ax.set_title(\"Class scores\")\n",
    "  ax.set_ylim(-5, 105)\n",
    "\n",
    "  chance = 100 / num_classes\n",
    "  ax.axhline(chance, ls=\"dashed\", color=\"k\", alpha=0.8)\n",
    "\n",
    "  ax.legend()\n",
    "\n",
    "  return ax\n",
    "\n",
    "\n",
    "#@markdown `plot_weights(MLP)`: Plots weights before and after training.\n",
    "def plot_weights(MLP, shared_colorbar=False):\n",
    "  \"\"\"\n",
    "  Function for plotting model weights and biases before and after learning.\n",
    "\n",
    "  Arguments:\n",
    "  - MLP (torch model): Model for which to plot weights and biases.\n",
    "  - shared_colorbar (bool, optional): If True, one colorbar is shared for all\n",
    "      parameters.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot array): Axes on which weights and biases were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  param_names = MLP.list_parameters()\n",
    "\n",
    "  params_images = dict()\n",
    "  pre_means = dict()\n",
    "  post_means = dict()\n",
    "  vmin, vmax = np.inf, -np.inf\n",
    "  for param_name in param_names:\n",
    "    layer, param_type = param_name.split(\"_\")\n",
    "    init_params = getattr(MLP, f\"init_{layer}_{param_type}\").numpy()\n",
    "    separator = np.full((1, init_params.shape[-1]), np.nan)\n",
    "    last_params = getattr(getattr(MLP, layer), param_type).detach().numpy()\n",
    "    diff_params = last_params - init_params\n",
    "\n",
    "    params_image = np.vstack(\n",
    "        [init_params, separator, last_params, separator, diff_params]\n",
    "        )\n",
    "    vmin = min(vmin, np.nanmin(params_image))\n",
    "    vmax = min(vmax, np.nanmax(params_image))\n",
    "\n",
    "    params_images[param_name] = params_image\n",
    "    pre_means[param_name] = init_params.mean()\n",
    "    post_means[param_name] = last_params.mean()\n",
    "\n",
    "  nrows = len(param_names)\n",
    "  gridspec_kw = dict()\n",
    "  if len(param_names) == 4:\n",
    "    gridspec_kw[\"height_ratios\"] = [5, 1, 5, 1]\n",
    "    cbar_label = \"Weight/bias values\"\n",
    "  elif len(param_names) == 2:\n",
    "    gridspec_kw[\"height_ratios\"] = [5, 5]\n",
    "    cbar_label = \"Weight values\"\n",
    "  else:\n",
    "    raise NotImplementedError(\"Expected 2 parameters (weights only) or \"\n",
    "      f\"4 parameters (weights and biases), but found {len(param_names)}\"\n",
    "    )\n",
    "\n",
    "  if shared_colorbar:\n",
    "    nrows += 1\n",
    "    gridspec_kw[\"height_ratios\"].append(1)\n",
    "  else:\n",
    "    vmin, vmax = None, None\n",
    "\n",
    "  fig, axes = plt.subplots(\n",
    "      nrows, 1, figsize=(6, nrows + 3), gridspec_kw=gridspec_kw\n",
    "      )\n",
    "\n",
    "  for i, (param_name, params_image) in enumerate(params_images.items()):\n",
    "    layer, param_type = param_name.split(\"_\")\n",
    "    layer_str = \"First\" if layer == \"lin1\" else \"Second\"\n",
    "    param_str = \"weights\" if param_type == \"weight\" else \"biases\"\n",
    "\n",
    "    axes[i].set_title(f\"{layer_str} linear layer {param_str} (pre, post and diff)\")\n",
    "    im = axes[i].imshow(params_image, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "    if not shared_colorbar:\n",
    "      cbar = fig.colorbar(im, ax=axes[i], aspect=10)\n",
    "      cbar.ax.axhline(pre_means[param_name], ls=\"dotted\", color=\"k\", alpha=0.5)\n",
    "      cbar.ax.axhline(post_means[param_name], color=\"k\", alpha=0.5)\n",
    "\n",
    "    if param_type == \"weight\":\n",
    "      axes[i].set_xlabel(\"Input dim.\")\n",
    "      axes[i].set_ylabel(\"Output dim.\")\n",
    "    axes[i].spines[[\"left\", \"bottom\"]].set_visible(False)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "  if shared_colorbar:\n",
    "    cax = axes[-1]\n",
    "    cbar = fig.colorbar(im, cax=cax, orientation=\"horizontal\", location=\"bottom\")\n",
    "    cax.set_xlabel(cbar_label)\n",
    "\n",
    "  return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xv-TyVnyxsi0",
   "metadata": {
    "id": "Xv-TyVnyxsi0"
   },
   "source": [
    "### 3.1 Loss and accuracy across learning\n",
    "\n",
    "First, we can look at how the loss (full lines) and accuracy (dashed lines) evolve across learning. The model appears to **learn quickly**, achieving a performance on the validation dataset that is **well above chance** accuracy (black dashed line), even with only 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mi2fbHCVyB2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "mi2fbHCVyB2J",
    "outputId": "16fd8df8-b14f-4957-d524-2b964fd97bea"
   },
   "outputs": [],
   "source": [
    "plot_results(MLP_results_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eJOogbz73qqH",
   "metadata": {
    "id": "eJOogbz73qqH"
   },
   "source": [
    "### 3.2 Final accuracy per class\n",
    "\n",
    "We can also look at the **accuracy breakdown** per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VGfRBSCjSQIc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "VGfRBSCjSQIc",
    "outputId": "d6a0f3ff-62c9-4e93-82e9-46e91ab25f26"
   },
   "outputs": [],
   "source": [
    "plot_scores_per_class(MLP_results_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lryCRdo7yK1K",
   "metadata": {
    "id": "lryCRdo7yK1K"
   },
   "source": [
    "### 3.3 Classified example images\n",
    "\n",
    "We can **visualize** examples of how the model has classified certain images (correctly or incorrectly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G32l0PgQRTXr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "G32l0PgQRTXr",
    "outputId": "83cfc0bc-e34c-4583-e8df-0a8e8e49a392"
   },
   "outputs": [],
   "source": [
    "plot_examples(valid_loader.dataset, MLP=MLP);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vkYhFP09lQ0O",
   "metadata": {
    "id": "vkYhFP09lQ0O"
   },
   "source": [
    "### 3.4 Weights before and after learning\n",
    "\n",
    "We can also observe how the weights changed through learning by visualizing the **initial** weights (top), the weights **after** learning (middle), and the **difference** between the two (bottom).\n",
    "\n",
    "The average weights **before** learning (dashed line) and **after** learning (full line) are plotted on the colorbar for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sMMJJ9zPlRGt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "sMMJJ9zPlRGt",
    "outputId": "1e7bed78-7628-4aaa-f023-641a8252ddcb"
   },
   "outputs": [],
   "source": [
    "plot_weights(MLP=MLP);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2MFqNy83VQOR",
   "metadata": {
    "id": "2MFqNy83VQOR"
   },
   "source": [
    "â“ **What other metrics might you collect or visualize to understand how the model is learning to perform this task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WNrnhwr5-n8Y",
   "metadata": {
    "id": "WNrnhwr5-n8Y"
   },
   "source": [
    "**Note:** In this section, we have visualized a variety of metrics that can be used to evaluate and compare models. Later in the project, you may want to **collect and record** these metrics for each trained model, instead of just visualizing the them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RHNaYXJw8S9U",
   "metadata": {
    "id": "RHNaYXJw8S9U"
   },
   "source": [
    "## Section 4. Implementing a biologically plausible learning rule.\n",
    "\n",
    "### 4.1 Hebbian learning\n",
    "\n",
    "Now, it is time to implement a more biologically plausible learning rule, like **Hebbian learning**. This rule is famously associated with the phrase _\"Neurons that fire together wire together.\"_\n",
    "\n",
    "In Hebbian learning, the weight $w_{ij}$ between pre-synaptic neuron _i_ and post-synaptic neuron _j_ is updated as follows:  \n",
    "${\\Delta}w_{ij} = {\\eta}(a_{i} \\cdot a_{j})$,  \n",
    "\n",
    "where:\n",
    "- ${\\eta}$ is the learning rate,\n",
    "- $a_{i}$ is the activation of the pre-synaptic neuron _i_, and\n",
    "- $a_{j}$ is the activation of the post-synaptic neuron _j_.\n",
    "\n",
    "This means that the **weight update** between two neurons is **proportional to the correlation** in their activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ey40TobfYVqo",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Ey40TobfYVqo",
    "outputId": "47f557ea-6e22-4d48-fd76-07ea078feb22"
   },
   "outputs": [],
   "source": [
    "#@markdown Hebbian learning schematic\n",
    "\n",
    "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/HebbianLearning.jpeg?raw=true\"\n",
    "\n",
    "display(Image(url = url, height=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LdicQs76bS7n",
   "metadata": {
    "id": "LdicQs76bS7n"
   },
   "source": [
    "**_(Left)_ Hebbian learning when only positive neural activity is allowed.**  \n",
    "\n",
    "Input neurons (top) are connected to output neurons (bottom). Active neurons are marked with + (active, light blue) and ++ (very active, dark blue). Connections marked with + will be weakly increased, and those marked with ++ will be strongly increased by Hebbian learning.\n",
    "\n",
    "**_(Right)_ Hebbian learning when positive and negative neural activity is allowed.**  \n",
    "Same as (Left), but negatively active neurons are marked with - (slightly active, light red) and -- (very active, dark red). Connections marked with - will be weakly decreased, and those marked with -- will be strongly decreased by Hebbian learning.\n",
    "\n",
    "#### 4.1.1 Preventing runaway potentiation\n",
    "Networks trained with Hebbian learning are typically implemented with neurons that only have **positive activations** _(Left)_, like neurons in the brain. This means that weights between neurons can only **increase**.\n",
    "\n",
    "Allowing neurons to have both **positive and negative** activations _(Right)_ would allow weights to also decrease with Hebbian learning. However, this is not typically done since **negative neural activity** (i.e., a negative firing rate) **is not biologically plausible**.\n",
    "\n",
    "Instead, various techniques can be used to prevent **runaway potentiation** in Hebbian learning (i.e., weights increasing more and more, without limit). These include normalization techniques like [Oja's rule](http://www.scholarpedia.org/article/Oja_learning_rule). In the example below, we take the approach of simply centering weight updates around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZRiBykdD7i-",
   "metadata": {
    "id": "DZRiBykdD7i-"
   },
   "source": [
    "### 4.2 Implementing learning rules in `torch`\n",
    "\n",
    "#### 4.2.1 Defining a custom autograd function\n",
    "\n",
    "One way to train a torch model using a learning rule other than backpropagation is to create a **custom autograd function**. With a custom autograd function, we can redefine the `forward()` and `backward()` methods that **pass** activations forward through the network, and pass gradients backward through the network. Specifically, this allows us to specify **how** the gradients used to update the model should be calculated.\n",
    "\n",
    "To implement Hebbian learning in the model, we will create a **custom autograd function** called `HebbianFunction`.\n",
    "\n",
    "#### 4.2.2 Defining the `forward()` method\n",
    "\n",
    "The forward method of an autograd function serves two purposes:  \n",
    "1) **Compute an output** for the given input.  \n",
    "2) Gather and store **all the information needed** to compute weight updates during the **backward** pass.\n",
    "\n",
    "As explained above, to calculate the Hebbian weight change between neurons in two layers, we need the **activity of the input** neurons and the **activity of the output** neurons. So, the `forward()` method should receive **input neuron activity** as its input, and return **output neuron activity** as its output.  \n",
    "\n",
    "The inputs to `forward()` are therefore:\n",
    "- `context`: Passed implictly to `forward()`, and used to store any information needed to calculate gradients during the backward pass.\n",
    "- `input`: The input neuron activity.\n",
    "- `weight`: The linear layer's weights.\n",
    "- `bias`: The linear layer's biases (can be `None`).\n",
    "- `nonlinearity`: The nonlinearity function, as it will be needed to calculate the output neuron's **activity**.\n",
    "- `target`: As will be explained later, for Hebbian learning, it can be very useful to use the targets to training the last layer of the network, instead of true output activity. So here, if targets are passed to `forward()`, this is stored instead of the output.\n",
    "\n",
    "In the forward pass, output neuron activity is computed, and the following variables are saved for the backward pass: `input`, `weight`, `bias` and `output_for_update` (i.e., the computed output neuron activity or the `target`, if it's provided, averaged across the batch).\n",
    "\n",
    "#### 4.2.3 Defining the `backward()` method\n",
    "The **`backward()`** method of an autograd function computes and returns gradients using only two input variables:\n",
    "- `context`: In which information was stored during the forward pass.\n",
    "- `grad_output`: The `grad_input` from the downstream layer (since gradients are computed **backwards** through the network.\n",
    "\n",
    "Here, for Hebbian learning, we do not use a backpropagated gradient. For this reason, `grad_output` is ignored and no `grad_input` is computed.\n",
    "\n",
    "Instead, the gradients for the weights and biases are computed using the variables stored in `context`:\n",
    "\n",
    "- `grad_weight`:\n",
    "  - Computed as input neuron activity multiplied by output activity.\n",
    "  - To avoid weight changes scaling linearly with the number of inputs, **we also divide by the number of input neurons**.\n",
    "\n",
    "- `grad_bias`:\n",
    "  - Computed simply as the output neuron activity, since biases have the same dimension as the output of a layer.\n",
    "  - Biases are enabled here. However, it should be noted that although they are used often in networks using error backpropagation, they are not used as often in Hebbian learning.\n",
    "\n",
    "**Notes:**\n",
    "- The `backward()` method expects to return **as many gradient values** as the number of **inputs** passed to the `forward()` method (except `context`). It may raise an error if it's not implemented this way. So this is why, in the example below, `backward()` returns `grad_nonlinearity` and `grad_target`, but they are both `None` values.\n",
    "- The `backward()` method computes the gradients, but **does _not_ apply weight updates**. These are applied when `optimizer.step()` is called. In the `BasicOptimizer`, defined above, the optimizer step optionally applies a weight decay, then subtracts the gradients computed here **multiplied by the learning rate**.\n",
    "- Standard optimizers, including the `BasicOptimizer`, **expect** to receive error gradients (i.e., values they should **subtract** from the parameters). However, we have computed Hebbian gradients (i.e., values that should be **added** to the parameters). For this reason, the gradients computed are multiplied by `-1` in the `backward()` method before they are returned.\n",
    "\n",
    "____________________________\n",
    "To learn more about `torch`'s autograd function and creating custom functions, see the following `torch` documentation and tutorials:\n",
    "- [A Gentle Introduction to `torch.autograd`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- The **Extending `torch.autograd`** section of [Extending PyTorch](https://pytorch.org/docs/stable/notes/extending.html).\n",
    "- An [example](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html) of a custom torch autograd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MUE037ycd112",
   "metadata": {
    "id": "MUE037ycd112"
   },
   "outputs": [],
   "source": [
    "class HebbianFunction(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  Gradient computing function class for Hebbian learning.\n",
    "  \"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(context, input, weight, bias=None, nonlinearity=None, target=None):\n",
    "    \"\"\"\n",
    "    Forward pass method for the layer. Computes the output of the layer and\n",
    "    stores variables needed for the backward pass.\n",
    "\n",
    "    Arguments:\n",
    "    - context (torch context): context in which variables can be stored for\n",
    "      the backward pass.\n",
    "    - input (torch tensor): input to the layer.\n",
    "    - weight (torch tensor): layer weights.\n",
    "    - bias (torch tensor, optional): layer biases.\n",
    "    - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
    "    - target (torch tensor, optional): layer target, if applicable.\n",
    "\n",
    "    Returns:\n",
    "    - output (torch tensor): layer output.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the output for the layer (linear layer with non-linearity)\n",
    "    output = input.mm(weight.t())\n",
    "    if bias is not None:\n",
    "      output += bias.unsqueeze(0).expand_as(output)\n",
    "    if nonlinearity is not None:\n",
    "      output = nonlinearity(output)\n",
    "\n",
    "    # calculate the output to use for the backward pass\n",
    "    output_for_update = output if target is None else target\n",
    "\n",
    "    # store variables in the context for the backward pass\n",
    "    context.save_for_backward(input, weight, bias, output_for_update)\n",
    "\n",
    "    return output\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(context, grad_output=None):\n",
    "    \"\"\"\n",
    "    Backward pass method for the layer. Computes and returns the gradients for\n",
    "    all variables passed to forward (returning None if not applicable).\n",
    "\n",
    "    Arguments:\n",
    "    - context (torch context): context in which variables can be stored for\n",
    "      the backward pass.\n",
    "    - input (torch tensor): input to the layer.\n",
    "    - weight (torch tensor): layer weights.\n",
    "    - bias (torch tensor, optional): layer biases.\n",
    "    - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
    "    - target (torch tensor, optional): layer target, if applicable.\n",
    "\n",
    "    Returns:\n",
    "    - grad_input (None): gradients for the input (None, since gradients are not\n",
    "      backpropagated in Hebbian learning).\n",
    "    - grad_weight (torch tensor): gradients for the weights.\n",
    "    - grad_bias (torch tensor or None): gradients for the biases, if they aren't\n",
    "      None.\n",
    "    - grad_nonlinearity (None): gradients for the nonlinearity (None, since\n",
    "      gradients do not apply to the non-linearities).\n",
    "    - grad_target (None): gradients for the targets (None, since\n",
    "      gradients do not apply to the targets).\n",
    "    \"\"\"\n",
    "\n",
    "    input, weight, bias, output_for_update = context.saved_tensors\n",
    "    grad_input = None\n",
    "    grad_weight = None\n",
    "    grad_bias = None\n",
    "    grad_nonlinearity = None\n",
    "    grad_target = None\n",
    "\n",
    "    input_needs_grad = context.needs_input_grad[0]\n",
    "    if input_needs_grad:\n",
    "      pass\n",
    "\n",
    "    weight_needs_grad = context.needs_input_grad[1]\n",
    "    if weight_needs_grad:\n",
    "      grad_weight = output_for_update.t().mm(input)\n",
    "      grad_weight = grad_weight / len(input) # average across batch\n",
    "\n",
    "      # center around 0\n",
    "      grad_weight = grad_weight - grad_weight.mean(axis=0) # center around 0\n",
    "\n",
    "      ## or apply Oja's rule (not compatible with clamping outputs to the targets!)\n",
    "      # oja_subtract = output_for_update.pow(2).mm(grad_weight).mean(axis=0)\n",
    "      # grad_weight = grad_weight - oja_subtract\n",
    "\n",
    "      # take the negative, as the gradient will be subtracted\n",
    "      grad_weight = -grad_weight\n",
    "\n",
    "    if bias is not None:\n",
    "      bias_needs_grad = context.needs_input_grad[2]\n",
    "      if bias_needs_grad:\n",
    "        grad_bias = output_for_update.mean(axis=0) # average across batch\n",
    "\n",
    "        # center around 0\n",
    "        grad_bias = grad_bias - grad_bias.mean()\n",
    "\n",
    "        ## or apply an adaptation of Oja's rule for biases\n",
    "        ## (not compatible with clamping outputs to the targets!)\n",
    "        # oja_subtract = (output_for_update.pow(2) * bias).mean(axis=0)\n",
    "        # grad_bias = grad_bias - oja_subtract\n",
    "\n",
    "        # take the negative, as the gradient will be subtracted\n",
    "        grad_bias = -grad_bias\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias, grad_nonlinearity, grad_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rc1ClVRliziw",
   "metadata": {
    "id": "rc1ClVRliziw"
   },
   "source": [
    "#### 4.2.4 Defining a `HebbianMultiLayerPerceptron` class\n",
    "Lastly, we provide a `HebbianMultiLayerPerceptron` class. This class inherits from `MultiLayerPerceptron`, but implements its own `forward()` method. This `forward()` method uses `HebbianFunction()` to send inputs through each layer of the network and its activation function, ensuring that gradients are computed correctly for Hebbian learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_7EF4yFxl6aS",
   "metadata": {
    "id": "_7EF4yFxl6aS"
   },
   "outputs": [],
   "source": [
    "class HebbianMultiLayerPerceptron(MultiLayerPerceptron):\n",
    "  \"\"\"\n",
    "  Hebbian multilayer perceptron with one hidden layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, clamp_output=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Initializes a Hebbian multilayer perceptron object\n",
    "\n",
    "    Arguments:\n",
    "    - clamp_output (bool, optional): if True, outputs are clamped to targets,\n",
    "      if available, when computing weight updates.\n",
    "    \"\"\"\n",
    "\n",
    "    self.clamp_output = clamp_output\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "  def forward(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through the network.\n",
    "\n",
    "    Arguments:\n",
    "    - X (torch.Tensor): Batch of input images.\n",
    "    - y (torch.Tensor, optional): Batch of targets, stored for the backward\n",
    "      pass to compute the gradients for the last layer.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred (torch.Tensor): Predicted targets.\n",
    "    \"\"\"\n",
    "\n",
    "    h = HebbianFunction.apply(\n",
    "        X.reshape(-1, self.num_inputs),\n",
    "        self.lin1.weight,\n",
    "        self.lin1.bias,\n",
    "        self.activation,\n",
    "    )\n",
    "\n",
    "    # if targets are provided, they can be used instead of the last layer's\n",
    "    # output to train the last layer.\n",
    "    if y is None or not self.clamp_output:\n",
    "      targets = None\n",
    "    else:\n",
    "      targets = torch.nn.functional.one_hot(\n",
    "          y, num_classes=self.num_outputs\n",
    "          ).float()\n",
    "\n",
    "    y_pred = HebbianFunction.apply(\n",
    "        h,\n",
    "        self.lin2.weight,\n",
    "        self.lin2.bias,\n",
    "        self.softmax,\n",
    "        targets\n",
    "    )\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VXSHDb_qybi3",
   "metadata": {
    "id": "VXSHDb_qybi3"
   },
   "source": [
    "### 4.3 Training a model with Hebbian learning\n",
    "\n",
    "#### 4.3.1 Simplifying the task to a 2-class task\n",
    "\n",
    "When implementing biologically plausible learning rules, one often faces a **performance trade-off**. This is because it is often harder for a network to learn without the precise error gradients that error backpropagation offers. More sophisticated designs can really enhance performance. However, since we are starting with the basics, we will instead start by **simplifying** the task from a 10-class task to a 2-class task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wUbJaLxi5hIF",
   "metadata": {
    "cellView": "form",
    "id": "wUbJaLxi5hIF"
   },
   "outputs": [],
   "source": [
    "#@markdown The following function returns a dataset restricted to specific classes:\n",
    "\n",
    "#@markdown `restrict_classes(dataset)`: Keeps specified classes in a dataset.\n",
    "\n",
    "def restrict_classes(dataset, classes=[6], keep=True):\n",
    "  \"\"\"\n",
    "  Removes or keeps specified classes in a dataset.\n",
    "\n",
    "  Arguments:\n",
    "  - dataset (torch dataset or subset): Dataset with class targets.\n",
    "  - classes (list): List of classes to keep or remove.\n",
    "  - keep (bool): If True, the classes specified are kept. If False, they are\n",
    "  removed.\n",
    "\n",
    "  Returns:\n",
    "  - new_dataset (torch dataset or subset): Datset restricted as specified.\n",
    "  \"\"\"\n",
    "\n",
    "  if hasattr(dataset, \"dataset\"):\n",
    "    indices = np.asarray(dataset.indices)\n",
    "    targets = dataset.dataset.targets[indices]\n",
    "    dataset = dataset.dataset\n",
    "  else:\n",
    "    indices = np.arange(len(dataset))\n",
    "    targets = dataset.targets\n",
    "\n",
    "  specified_idxs = np.isin(targets, np.asarray(classes))\n",
    "  if keep:\n",
    "    retain_indices = indices[specified_idxs]\n",
    "  else:\n",
    "    retain_indices = indices[~specified_idxs]\n",
    "\n",
    "  new_dataset = torch.utils.data.Subset(dataset, retain_indices)\n",
    "\n",
    "  return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Mk7rPb06VTu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "-Mk7rPb06VTu",
    "outputId": "5ea0e2dd-9200-4dfc-e9bb-44382e4cf574"
   },
   "outputs": [],
   "source": [
    "train_set_2_classes = restrict_classes(train_set, [0, 1])\n",
    "valid_set_2_classes = restrict_classes(valid_set, [0, 1])\n",
    "\n",
    "plot_class_distribution(train_set_2_classes, valid_set_2_classes)\n",
    "\n",
    "train_loader_2cls = torch.utils.data.DataLoader(train_set_2_classes, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader_2cls = torch.utils.data.DataLoader(valid_set_2_classes, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zijpfuzJ0j1-",
   "metadata": {
    "id": "zijpfuzJ0j1-"
   },
   "source": [
    "The number of examples for each class in the training and validation sets are shown. Dashed lines show what counts would be expected if there were the same number of examples in each class.\n",
    "\n",
    "#### 4.3.2 Training on a 2-class task\n",
    "\n",
    "We'll test the Hebbian learning model's performance with `10` epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kZBYpcr28Q55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kZBYpcr28Q55",
    "outputId": "59b99a5e-14f4-4010-81a2-fe5a8c8928cb"
   },
   "outputs": [],
   "source": [
    "HEBB_LR = 1e-4 # lower, since Hebbian gradients are much bigger than backprop gradients\n",
    "\n",
    "HebbianMLP_2cls = HebbianMultiLayerPerceptron(\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    num_outputs=2,\n",
    "    clamp_output=False,\n",
    ")\n",
    "\n",
    "Hebb_optimizer_2cls = BasicOptimizer(HebbianMLP_2cls.parameters(), lr=HEBB_LR)\n",
    "\n",
    "Hebb_results_dict_2cls = train_model(\n",
    "    HebbianMLP_2cls,\n",
    "    train_loader_2cls,\n",
    "    valid_loader_2cls,\n",
    "    Hebb_optimizer_2cls,\n",
    "    num_epochs=10\n",
    "    );\n",
    "\n",
    "plot_results(Hebb_results_dict_2cls, num_classes=2)\n",
    "plot_scores_per_class(Hebb_results_dict_2cls, num_classes=2)\n",
    "plot_examples(valid_loader_2cls.dataset, MLP=HebbianMLP_2cls, num_classes=2)\n",
    "plot_weights(HebbianMLP_2cls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T1XyUGfIbUCZ",
   "metadata": {
    "id": "T1XyUGfIbUCZ"
   },
   "source": [
    "Try running the previous cell **several times** to see how often the model succeeds in learning this simple classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EhnLVDwaYys2",
   "metadata": {
    "id": "EhnLVDwaYys2"
   },
   "source": [
    "â“ **Why does this network struggle to learn this simple classification task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8AoxUYdj0TQ8",
   "metadata": {
    "id": "8AoxUYdj0TQ8"
   },
   "source": [
    "#### 4.3.3 Training with targets\n",
    "\n",
    "As you may have notice, the classification **targets** do not actually appear in the basic Hebbian learning rule. How, then, can the network learn to perform a supervised task?\n",
    "\n",
    "To allow a network trained with Hebbian learning to learn a supervised task, one can take the approach of **clamping the outputs to the targets**. In other words, we can update the final layer's weights using the targets $t_{j}$ instead of the final layer's activations $a_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0OoyA23mZ5tP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0OoyA23mZ5tP",
    "outputId": "e700f404-007a-48c2-93ed-6a1d13e5fac6"
   },
   "outputs": [],
   "source": [
    "HebbianMLP_2cls = HebbianMultiLayerPerceptron(\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    num_outputs=2,\n",
    "    clamp_output=True, # clamp output to targets\n",
    ")\n",
    "\n",
    "Hebb_optimizer_2cls = BasicOptimizer(HebbianMLP_2cls.parameters(), lr=HEBB_LR)\n",
    "\n",
    "Hebb_results_dict_2cls = train_model(\n",
    "    HebbianMLP_2cls,\n",
    "    train_loader_2cls,\n",
    "    valid_loader_2cls,\n",
    "    Hebb_optimizer_2cls,\n",
    "    num_epochs=10\n",
    "    );\n",
    "\n",
    "plot_results(Hebb_results_dict_2cls, num_classes=2)\n",
    "plot_scores_per_class(Hebb_results_dict_2cls, num_classes=2)\n",
    "plot_examples(valid_loader_2cls.dataset, MLP=HebbianMLP_2cls, num_classes=2)\n",
    "plot_weights(HebbianMLP_2cls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MC1WJuKSbkMF",
   "metadata": {
    "id": "MC1WJuKSbkMF"
   },
   "source": [
    "Try running the previous cell **several times**. The model should now be more successful at learning this simple classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-SEuXwoqbsUn",
   "metadata": {
    "id": "-SEuXwoqbsUn"
   },
   "source": [
    "â“ **Is the model successful every time? If not, what might be contributing to the variability in performance?**  \n",
    "â“ **Going futher: How does changing the training hyperparameters (learning rate and number of epochs) affect network learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iU9OnWMvMki-",
   "metadata": {
    "id": "iU9OnWMvMki-"
   },
   "source": [
    "**Note:** The clamped outputs setting _cannot_ be used with Oja's rule (i.e., one of the Hebbian learning modifications used to prevent runaway weight increases). This is because the values subtracted when using Oja's rule would be calculated on the target outputs instead of the actual outputs, and these values would end up being too big.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zG-BkzDTcVZR",
   "metadata": {
    "id": "zG-BkzDTcVZR"
   },
   "source": [
    "#### 4.3.4 Increasing task difficulty\n",
    "\n",
    "Next, let's see what happens when we **increase the number of classes** in the task. The following function handles the entire initialization and training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jjQ1Bm8aeUh6",
   "metadata": {
    "cellView": "form",
    "id": "jjQ1Bm8aeUh6"
   },
   "outputs": [],
   "source": [
    "#@markdown `train_model_extended`: Initializes model and optimizer, restricts datasets to\n",
    "#@markdown specified classes, trains model. Returns trained model and results dictionary.\n",
    "\n",
    "def train_model_extended(model_type=\"backprop\", keep_num_classes=\"all\", lr=LR,\n",
    "                         num_epochs=5, partial_backprop=False,\n",
    "                         num_hidden=NUM_HIDDEN, bias=BIAS,\n",
    "                         batch_size=BATCH_SIZE, plot_distribution=False):\n",
    "  \"\"\"\n",
    "  Initializes model and optimizer, restricts datasets to specified classes and\n",
    "  trains the model. Returns the trained model and results dictionary.\n",
    "\n",
    "  Arguments:\n",
    "  - model_type (str, optional): model to initialize (\"backprop\" or \"Hebbian\")\n",
    "  - keep_num_classes (str or int, optional): number of classes to keep (from 0)\n",
    "  - lr (float or list, optional): learning rate for both or each layer\n",
    "  - num_epochs (int, optional): number of epochs to train model.\n",
    "  - partial_backprop (bool, optional): if True, backprop is used to train the\n",
    "    final Hebbian learning model.\n",
    "  - num_hidden (int, optional): number of hidden units in the hidden layer\n",
    "  - bias (bool, optional): if True, each linear layer will have biases in\n",
    "      addition to weights.\n",
    "  - batch_size (int, optional): batch size for dataloaders.\n",
    "  - plot_distribution (bool, optional): if True, dataset class distributions\n",
    "    are plotted.\n",
    "\n",
    "  Returns:\n",
    "  - MLP (torch module): Model\n",
    "  - results_dict (dict): Dictionary storing results across epochs on training\n",
    "    and validation data.\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(keep_num_classes, str):\n",
    "    if keep_num_classes == \"all\":\n",
    "      num_classes = 10\n",
    "      use_train_set = train_set\n",
    "      use_valid_set = valid_set\n",
    "    else:\n",
    "      raise ValueError(\"If 'keep_classes' is a string, it should be 'all'.\")\n",
    "  else:\n",
    "    num_classes = int(keep_num_classes)\n",
    "    use_train_set = restrict_classes(train_set, np.arange(keep_num_classes))\n",
    "    use_valid_set = restrict_classes(valid_set, np.arange(keep_num_classes))\n",
    "\n",
    "  if plot_distribution:\n",
    "    plot_class_distribution(use_train_set, use_valid_set)\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      use_train_set, batch_size=batch_size, shuffle=True\n",
    "      )\n",
    "  valid_loader = torch.utils.data.DataLoader(\n",
    "      use_valid_set, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "  model_params = {\n",
    "      \"num_hidden\": num_hidden,\n",
    "      \"num_outputs\": num_classes,\n",
    "      \"bias\": bias,\n",
    "  }\n",
    "\n",
    "  if model_type.lower() == \"backprop\":\n",
    "    Model = MultiLayerPerceptron\n",
    "  elif model_type.lower() == \"hebbian\":\n",
    "    if partial_backprop:\n",
    "      Model = HebbianBackpropMultiLayerPerceptron\n",
    "    else:\n",
    "      Model = HebbianMultiLayerPerceptron\n",
    "      model_params[\"clamp_output\"] = True\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        f\"Got {model_type} model type, but expected 'backprop' or 'hebbian'.\"\n",
    "        )\n",
    "\n",
    "  MLP = Model(**model_params)\n",
    "\n",
    "  if isinstance(lr, list):\n",
    "    if len(lr) != 2:\n",
    "      raise ValueError(\"If 'lr' is a list, it must be of length 2.\")\n",
    "    optimizer = BasicOptimizer([\n",
    "        {\"params\": MLP.lin1.parameters(), \"lr\": lr[0]},\n",
    "        {\"params\": MLP.lin2.parameters(), \"lr\": lr[1]},\n",
    "    ])\n",
    "  else:\n",
    "    optimizer = BasicOptimizer(MLP.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "  results_dict = train_model(\n",
    "      MLP,\n",
    "      train_loader,\n",
    "      valid_loader,\n",
    "      optimizer,\n",
    "      num_epochs=num_epochs\n",
    "      )\n",
    "\n",
    "  return MLP, results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruTgLxWUgQk4",
   "metadata": {
    "id": "ruTgLxWUgQk4"
   },
   "source": [
    "First, let's train a model on this task using error backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aDqCa3gzaPpk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aDqCa3gzaPpk",
    "outputId": "541cd1b6-9e25-4459-a00e-1940d51dcb9e"
   },
   "outputs": [],
   "source": [
    "MLP_3cls, results_dict_3cls = train_model_extended(\n",
    "    model_type=\"backprop\",\n",
    "    keep_num_classes=3,\n",
    "    num_epochs=5,\n",
    "    lr=LR,\n",
    "    plot_distribution=True,\n",
    ")\n",
    "\n",
    "plot_results(results_dict_3cls, num_classes=3)\n",
    "plot_scores_per_class(results_dict_3cls, num_classes=3)\n",
    "plot_weights(MLP_3cls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PeBuO23Bgee0",
   "metadata": {
    "id": "PeBuO23Bgee0"
   },
   "source": [
    "Now, let's try training a model with Hebbian learning (and outputs clamped to targets). Since the task is harder, we'll increase the number of training epochs to `15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Ao0NPeTjNgV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0Ao0NPeTjNgV",
    "outputId": "c75f8ecd-f838-44f9-8f9a-ead8060da23e"
   },
   "outputs": [],
   "source": [
    "HebbianMLP_3cls, Hebbian_results_dict_3cls = train_model_extended(\n",
    "    model_type=\"hebbian\",\n",
    "    keep_num_classes=3,\n",
    "    num_epochs=15,\n",
    "    lr=HEBB_LR,\n",
    ")\n",
    "\n",
    "plot_results(Hebbian_results_dict_3cls, num_classes=3)\n",
    "plot_scores_per_class(Hebbian_results_dict_3cls, num_classes=3)\n",
    "plot_weights(HebbianMLP_3cls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enzACY3lipfv",
   "metadata": {
    "id": "enzACY3lipfv"
   },
   "source": [
    "Try running the previous cell a few times to see how often the model is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZjmhXIcYiMFg",
   "metadata": {
    "id": "ZjmhXIcYiMFg"
   },
   "source": [
    "â“ **How is the model learning in each layer?**  \n",
    "â“ **How do the weight updates learned with Hebbian learning compare to those learned with error backpropagation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vVIsLsFniB5Z",
   "metadata": {
    "id": "vVIsLsFniB5Z"
   },
   "source": [
    "We can try using **different learning rates** to encourage more learning in the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcTZMxroh1H0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zcTZMxroh1H0",
    "outputId": "b33fcc18-987b-40ea-85c7-4170fbc94f84"
   },
   "outputs": [],
   "source": [
    "HebbianMLP_3cls, Hebbian_results_dict_3cls = train_model_extended(\n",
    "    model_type=\"hebbian\",\n",
    "    keep_num_classes=3,\n",
    "    num_epochs=15,\n",
    "    lr=[HEBB_LR / 4, HEBB_LR * 8], # learning rate for each layer\n",
    ")\n",
    "\n",
    "plot_results(Hebbian_results_dict_3cls, num_classes=3)\n",
    "plot_scores_per_class(Hebbian_results_dict_3cls, num_classes=3)\n",
    "plot_weights(HebbianMLP_3cls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uW2ruvzvkXVH",
   "metadata": {
    "id": "uW2ruvzvkXVH"
   },
   "source": [
    "Performance tends to be highly variable and unstable. At best, the network is able to classify 2 classes, but generally not all 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QeLG5w4qjXA_",
   "metadata": {
    "id": "QeLG5w4qjXA_"
   },
   "source": [
    "#### 4.3.5 Combining Hebbian learning and error backpropagation\n",
    "\n",
    "What happens if we use Hebbian learning for the first layer, but use error backpropagation to train the second layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kzv090_skftX",
   "metadata": {
    "cellView": "form",
    "id": "kzv090_skftX"
   },
   "outputs": [],
   "source": [
    "#@markdown `HebbianBackpropMultiLayerPerceptron()`: Class combining Hebbian learning and backpropagation.\n",
    "\n",
    "class HebbianBackpropMultiLayerPerceptron(MultiLayerPerceptron):\n",
    "  \"\"\"\n",
    "  Hybrid backprop/Hebbian multilayer perceptron with one hidden layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def forward(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through the network.\n",
    "\n",
    "    Arguments:\n",
    "    - X (torch.Tensor): Batch of input images.\n",
    "    - y (torch.Tensor, optional): Batch of targets, not used here.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred (torch.Tensor): Predicted targets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hebbian layer\n",
    "    h = HebbianFunction.apply(\n",
    "        X.reshape(-1, self.num_inputs),\n",
    "        self.lin1.weight,\n",
    "        self.lin1.bias,\n",
    "        self.activation,\n",
    "    )\n",
    "\n",
    "    # backprop layer\n",
    "    y_pred = self.softmax(self.lin2(h))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpGAeBtKjfI7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wpGAeBtKjfI7",
    "outputId": "ce484a35-cb91-459b-c1a6-045815c9bf48"
   },
   "outputs": [],
   "source": [
    "HybridMLP, Hybrid_results_dict = train_model_extended(\n",
    "    model_type=\"hebbian\",\n",
    "    num_epochs=5,\n",
    "    partial_backprop=True, # backprop on the final layer\n",
    "    lr=[HEBB_LR / 5, LR], # learning rates for each layer\n",
    ")\n",
    "\n",
    "plot_results(Hybrid_results_dict)\n",
    "plot_scores_per_class(Hybrid_results_dict)\n",
    "plot_weights(HybridMLP);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6n_jOl8RpqZL",
   "metadata": {
    "id": "6n_jOl8RpqZL"
   },
   "source": [
    "Using Hebbian learning and error backpropagation allows us to achieve above chance performance on the the full MNIST classification task, though performance is still much lower than when using error backpropagation on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nbEKmoKIqC7I",
   "metadata": {
    "id": "nbEKmoKIqC7I"
   },
   "source": [
    "â“ **What are some of the properties of Hebbian learning that might explain its weaker performance on this task when compared to error backpropagation?**   \n",
    "â“ **Going further: Are there tasks that Hebbian learning might be better at than error backpropagation?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-EMoAAdw9x5u",
   "metadata": {
    "id": "-EMoAAdw9x5u"
   },
   "source": [
    "### Section 4.4. Computing the variance and bias of a model's gradients.\n",
    "\n",
    "To better understand _how_ a model trained with a biologically plausible learning rule (e.g., Hebbian learning) learns, it can be useful to compare its learning to error backpropagation. Specifically, we can compare the gradients computed with both learning rules to one another.\n",
    "\n",
    "### Variance\n",
    "\n",
    "One property we can compute is the variance. The variance tells us **how consistent** are the gradients obtained for each weight **across examples** when computed with one learning rule compared to the other. We might expect a good learning rule to be more consistent, and therefore have **lower variance** in its gradients (_left_ column in the bullseye image).\n",
    "\n",
    "However, it would be unfair to compare the variance of small gradients (like those computed with error backpropagation) with the variance of large gradients (like those computed with Hebbian learning). So, we will estimate variance in the gradients using a **scale-invariant** measure: the **signal-to-noise ratio (SNR)**. Note that **high SNR** corresponds to **low variance**, and vice versa.\n",
    "\n",
    "### Bias\n",
    "Another property we can compute is how biased gradients computed with a specific learning rule are with respect to **ideal gradients**. Now, since the ideal gradients for learning a task are generally unknown, we need to estimate them. We can do so using error backpropagation, as it is the best algorithm we know for learning a task along the gradient of its error. A good learning rule would then be expected to have **low bias** in its gradients with respect to error backpropagation gradients.\n",
    "\n",
    "As with the variance, our bias estimate should be **scale-invariant**, so we will estimate it using the Cosine similarity. Note that **high Cosine similarity** with error backpropagation gradients corresponds to **low bias**, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vajWV3Llcasz",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "vajWV3Llcasz",
    "outputId": "bf225339-819d-4826-c2f8-bfba9c0d5fbd"
   },
   "outputs": [],
   "source": [
    "#@markdown Bias and variance schematic\n",
    "\n",
    "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/BiasVariance.jpeg?raw=true\"\n",
    "\n",
    "display(Image(url = url, height=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeb7LTQNtrOy",
   "metadata": {
    "id": "xeb7LTQNtrOy"
   },
   "source": [
    "The bullseye image illustrates the differences and interplay between bias and variance in a variable. In the **left column**, the variable being measured shows **low variance**, as the green dots are densely concentrated. In the **right column**, the dots are more dispersed, reflecting a higher variance.\n",
    "\n",
    "Although the examples in each column show the same variance, they show different biases. In examples in the **top row**, the variable being measured has a **low bias** with respect to the bullseye, as the dots are centered on the bullseye. In contrast, in the **bottom row**, the variable being measured has a **high bias** a variable with low bias with respect to the bullseye, as the dots are off-center with respect to the bullseye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K3HLQQsYu9Np",
   "metadata": {
    "id": "K3HLQQsYu9Np"
   },
   "source": [
    "#### 4.4.1 Estimating gradient variance using SNR\n",
    "\n",
    "The following functions measure and plot the SNR of the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D4tKRaq79yJN",
   "metadata": {
    "cellView": "form",
    "id": "D4tKRaq79yJN"
   },
   "outputs": [],
   "source": [
    "#@markdown `compute_gradient_SNR(MLP, dataset)`: Passes a dataset through a model\n",
    "#@markdown and computes the SNR of the gradients computed by the model for each example.\n",
    "#@markdown Returns a dictionary containing the gradient SNRs for each layer of a model.\n",
    "\n",
    "def compute_gradient_SNR(MLP, dataset):\n",
    "  \"\"\"\n",
    "  Computes gradient SNRs for a model given a dataset.\n",
    "\n",
    "  Arguments:\n",
    "  - MLP (torch model): Model for which to compute gradient SNRs.\n",
    "  - dataset (torch dataset): Dataset with which to compute gradient SNRs.\n",
    "\n",
    "  Returns:\n",
    "  - SNR_dict (dict): Dictionary compiling gradient SNRs for each parameter\n",
    "    (i.e., the weights and/or biases of each layer).\n",
    "  \"\"\"\n",
    "\n",
    "  MLP.eval()\n",
    "\n",
    "  criterion = torch.nn.NLLLoss()\n",
    "\n",
    "  gradients = {key: list() for key in MLP.list_parameters()}\n",
    "\n",
    "  # initialize a loader with a batch size of 1\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "  # collect gradients computed on the dataset of data\n",
    "  for X, y in loader:\n",
    "    y_pred = MLP(X, y=y)\n",
    "    loss = criterion(torch.log(y_pred), y)\n",
    "\n",
    "    MLP.zero_grad() # zero grad before\n",
    "    loss.backward()\n",
    "    for key, value in MLP.gather_gradient_dict().items():\n",
    "      gradients[key].append(value)\n",
    "    MLP.zero_grad() # zero grad after, since no optimzer step is taken\n",
    "\n",
    "  # aggregate the gradients\n",
    "  SNR_dict = {key: list() for key in MLP.list_parameters()}\n",
    "  for key, value in gradients.items():\n",
    "      SNR_dict[key] = compute_SNR(np.asarray(value))\n",
    "\n",
    "  return SNR_dict\n",
    "\n",
    "\n",
    "def compute_SNR(data, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calculates the average SNR for of data across the first axis.\n",
    "\n",
    "    Arguments:\n",
    "    - data (torch Tensor): items x gradients\n",
    "    - epsilon (float, optional): value added to the denominator to avoid\n",
    "      division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - avg_SNR (float): average SNR across data items\n",
    "    \"\"\"\n",
    "\n",
    "    absolute_mean = np.abs(np.mean(data, axis=0))\n",
    "    std = np.std(data, axis=0)\n",
    "    SNR_by_item = absolute_mean / (std + epsilon)\n",
    "    avg_SNR = np.mean(SNR_by_item)\n",
    "\n",
    "    return avg_SNR\n",
    "\n",
    "#@markdown `plot_gradient_SNRs(SNR_dict)`: Plots gradient SNRs collected in\n",
    "#@markdown a dictionary.\n",
    "def plot_gradient_SNRs(SNR_dict, width=0.5, ax=None):\n",
    "  \"\"\"\n",
    "  Plot gradient SNRs for various learning rules.\n",
    "\n",
    "  Arguments:\n",
    "  - SNR_dict (dict): Gradient SNRs for each learning rule.\n",
    "  - width (float, optional): Width of the bars.\n",
    "  - ax (plt subplot, optional): Axis on which to plot gradient SNRs. If None, a\n",
    "    new axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which gradient SNRs were plotted.  \"\"\"\n",
    "\n",
    "  if ax is None:\n",
    "    wid = min(8, len(SNR_dict) * 1.5)\n",
    "    _, ax = plt.subplots(figsize=(wid, 4))\n",
    "\n",
    "  xlabels = list()\n",
    "  SNR_means = list()\n",
    "  SNR_sems = list()\n",
    "  SNRs_scatter = list()\n",
    "  for m, (model_type, SNRs) in enumerate(SNR_dict.items()):\n",
    "    xlabels.append(model_type)\n",
    "    color = get_plotting_color(model_idx=m)\n",
    "    ax.bar(\n",
    "        m, np.mean(SNRs), yerr=scipy.stats.sem(SNRs),\n",
    "        alpha=0.5, width=width, capsize=5, color=color\n",
    "        )\n",
    "    s = [20 + i * 30 for i in range(len(SNRs))]\n",
    "    ax.scatter([m] * len(SNRs), SNRs, alpha=0.8, s=s, color=color, zorder=5)\n",
    "\n",
    "  x = np.arange(len(xlabels))\n",
    "  ax.set_xticks(x)\n",
    "  x_pad = (x.max() - x.min() + width) * 0.3\n",
    "  ax.set_xlim(x.min() - x_pad, x.max() + x_pad)\n",
    "  ax.set_xticklabels(xlabels, rotation=45)\n",
    "  ax.set_xlabel(\"Learning rule\")\n",
    "  ax.set_ylabel(\"SNR\")\n",
    "  ax.set_title(\"SNR of the gradients\")\n",
    "\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eAv_lAGLXGt5",
   "metadata": {
    "id": "eAv_lAGLXGt5"
   },
   "source": [
    "For the error backpropagation model and the Hebbian learning model, we compute the SNR **before training the model**, using the validation set. This allows us to evaluate the gradients a learning rule proposes for an untrained model. Notably, we pass one example at a time through the model, to obtain gradients for each example.  \n",
    "\n",
    "**Note:** Since we obtain a gradient SNR for each layer of the model, here we plot the gradient SNR averaged across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JN_OE7KnH-uS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "JN_OE7KnH-uS",
    "outputId": "07a99c64-c564-4e81-812d-454da8c4f618"
   },
   "outputs": [],
   "source": [
    "SNR_dict = dict()\n",
    "for model_type in [\"backprop\", \"Hebbian\"]:\n",
    "  model_params = {\n",
    "      \"num_hidden\": NUM_HIDDEN,\n",
    "      \"activation_type\": ACTIVATION,\n",
    "      \"bias\": BIAS,\n",
    "  }\n",
    "\n",
    "  if model_type == \"Hebbian\":\n",
    "    model_fct = HebbianMultiLayerPerceptron\n",
    "  else:\n",
    "    model_fct = MultiLayerPerceptron\n",
    "\n",
    "  model = model_fct(**model_params)\n",
    "\n",
    "  model_SNR_dict = compute_gradient_SNR(model, valid_loader.dataset)\n",
    "\n",
    "  SNR_dict[model_type] = [SNR for SNR in model_SNR_dict.values()]\n",
    "\n",
    "plot_gradient_SNRs(SNR_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMYWZi-EWi4A",
   "metadata": {
    "id": "YMYWZi-EWi4A"
   },
   "source": [
    "The dots in the plots show the SNR **for each layer** (small dot: first layer; larger dot: second layer).  \n",
    "\n",
    "Hebbian learning appears to have produced gradients with a very **high SNR** (and therefore lower variance) in the **first layer** (small dot). In constrast, the **second layer** (big dot) shows a **lower SNR** than error backpropagation.  \n",
    "\n",
    "Notably, this is evaluated on the full 10-class task which Hebbian learning struggles to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vL-q54f42KoR",
   "metadata": {
    "id": "vL-q54f42KoR"
   },
   "source": [
    "â“ **What does the Hebbian learning SNR look like for the 2-class version of the task?**  \n",
    "â“ **What might this mean about how Hebbian learning learns?**  \n",
    "â“ **Going further: How might this result relate to the Hebbian learning rule's performance on the classification task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PbVY0JzHW-9M",
   "metadata": {
    "id": "PbVY0JzHW-9M"
   },
   "source": [
    "#### 4.4.2 Estimating the gradient bias with respect to error backpropagation using the Cosine similarity.\n",
    "\n",
    "The following functions measure and plot the Cosine similarity of the gradients to error backpropagation gradients:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nXAl4lzJZmw_",
   "metadata": {
    "cellView": "form",
    "id": "nXAl4lzJZmw_"
   },
   "outputs": [],
   "source": [
    "#@markdown `train_and_calculate_cosine_sim(MLP, train_loader, valid_loader, optimizer)`:\n",
    "#@markdown Trains a model using a specific learning rule, while computing the cosine\n",
    "#@markdown similarity of the gradients proposed the learning rule compared to those proposed\n",
    "#@markdown by error backpropagation.\n",
    "def train_and_calculate_cosine_sim(MLP, train_loader, valid_loader, optimizer,\n",
    "                                   num_epochs=8):\n",
    "  \"\"\"\n",
    "  Train model across epochs, calculating the cosine similarity between the\n",
    "  gradients proposed by the learning rule it's trained with, compared to those\n",
    "  proposed by error backpropagation.\n",
    "\n",
    "  Arguments:\n",
    "  - MLP (torch model): Model to train.\n",
    "  - train_loader (torch dataloader): Dataloader to use to train the model.\n",
    "  - valid_loader (torch dataloader): Dataloader to use to validate the model.\n",
    "  - optimizer (torch optimizer): Optimizer to use to update the model.\n",
    "  - num_epochs (int, optional): Number of epochs to train model.\n",
    "\n",
    "  Returns:\n",
    "  - cosine_sim (dict): Dictionary storing the cosine similarity between the\n",
    "    model's learning rule and backprop across epochs, computed on the\n",
    "    validation data.\n",
    "  \"\"\"\n",
    "\n",
    "  criterion = torch.nn.NLLLoss()\n",
    "\n",
    "  cosine_sim_dict = {key: list() for key in MLP.list_parameters()}\n",
    "\n",
    "  for e in tqdm(range(num_epochs)):\n",
    "    MLP.train()\n",
    "    for X, y in train_loader:\n",
    "      y_pred = MLP(X, y=y)\n",
    "      loss = criterion(torch.log(y_pred), y)\n",
    "      optimizer.zero_grad()\n",
    "      if e != 0:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    MLP.eval()\n",
    "    lr_gradients_dict = {key: list() for key in cosine_sim_dict.keys()}\n",
    "    backprop_gradients_dict = {key: list() for key in cosine_sim_dict.keys()}\n",
    "\n",
    "    for X, y in valid_loader:\n",
    "      # collect gradients computed with learning rule\n",
    "      y_pred = MLP(X, y=y)\n",
    "      loss = criterion(torch.log(y_pred), y)\n",
    "\n",
    "      MLP.zero_grad()\n",
    "      loss.backward()\n",
    "      for key, value in MLP.gather_gradient_dict().items():\n",
    "        lr_gradients_dict[key].append(value)\n",
    "      MLP.zero_grad()\n",
    "\n",
    "      # collect gradients computed with backprop\n",
    "      y_pred = MLP.forward_backprop(X)\n",
    "      loss = criterion(torch.log(y_pred), y)\n",
    "\n",
    "      MLP.zero_grad()\n",
    "      loss.backward()\n",
    "      for key, value in MLP.gather_gradient_dict().items():\n",
    "        backprop_gradients_dict[key].append(value)\n",
    "      MLP.zero_grad()\n",
    "\n",
    "    for key in cosine_sim_dict.keys():\n",
    "      lr_grad = np.asarray(lr_gradients_dict[key])\n",
    "      bp_grad = np.asarray(backprop_gradients_dict[key])\n",
    "      if (lr_grad == 0).all():\n",
    "        warnings.warn(\n",
    "          f\"Learning rule computed all 0 gradients for epoch {e}. \"\n",
    "          \"Cosine similarity cannot be calculated.\"\n",
    "          )\n",
    "        epoch_cosine_sim = np.nan\n",
    "      elif (bp_grad == 0).all():\n",
    "        warnings.warn(\n",
    "          f\"Backprop. rule computed all 0 gradients for epoch {e}. \"\n",
    "          \"Cosine similarity cannot be calculated.\"\n",
    "          )\n",
    "        epoch_cosine_sim = np.nan\n",
    "      else:\n",
    "        epoch_cosine_sim = calculate_cosine_similarity(lr_grad, bp_grad)\n",
    "\n",
    "      cosine_sim_dict[key].append(epoch_cosine_sim)\n",
    "\n",
    "  return cosine_sim_dict\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(data1, data2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "    - data1 (torch Tensor): first vector\n",
    "    - data2 (torch Tensor): second vector\n",
    "    \"\"\"\n",
    "\n",
    "    data1 = data1.reshape(-1)\n",
    "    data2 = data2.reshape(-1)\n",
    "\n",
    "    numerator = np.dot(data1, data2)\n",
    "    denominator = (\n",
    "        np.sqrt(np.dot(data1, data1)) * np.sqrt(np.dot(data2, data2))\n",
    "      )\n",
    "\n",
    "    cosine_sim = numerator / denominator\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "#@markdown `plot_gradient_cosine_sims(cosine_sim_dict)`: Plots cosine similarity\n",
    "#@markdown of the gradients proposed by a model across learning to those proposed\n",
    "#@markdown by error backpropagation.\n",
    "def plot_gradient_cosine_sims(cosine_sim_dict, ax=None):\n",
    "  \"\"\"\n",
    "  Plot gradient cosine similarities to error backpropagation for various\n",
    "  learning rules.\n",
    "\n",
    "  Arguments:\n",
    "  - cosine_sim_dict (dict): Gradient cosine similarities for each learning rule.\n",
    "  - ax (plt subplot, optional): Axis on which to plot gradient cosine\n",
    "    similarities. If None, a new axis will be created.\n",
    "\n",
    "  Returns:\n",
    "  - ax (plt subplot): Axis on which gradient cosine similarities were plotted.\n",
    "  \"\"\"\n",
    "\n",
    "  if ax is None:\n",
    "    _, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "  max_num_epochs = 0\n",
    "  for m, (model_type, cosine_sims) in enumerate(cosine_sim_dict.items()):\n",
    "    cosine_sims = np.asarray(cosine_sims) # params x epochs\n",
    "    num_epochs = cosine_sims.shape[1]\n",
    "    x = np.arange(num_epochs)\n",
    "    cosine_sim_means = np.nanmean(cosine_sims, axis=0)\n",
    "    cosine_sim_sems = scipy.stats.sem(cosine_sims, axis=0, nan_policy=\"omit\")\n",
    "\n",
    "    ax.plot(x, cosine_sim_means, label=model_type, alpha=0.8)\n",
    "\n",
    "    color = get_plotting_color(model_idx=m)\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        cosine_sim_means - cosine_sim_sems,\n",
    "        cosine_sim_means + cosine_sim_sems,\n",
    "        alpha=0.3, lw=0, color=color\n",
    "        )\n",
    "\n",
    "    for i, param_cosine_sims in enumerate(cosine_sims):\n",
    "      s = 20 + i * 30\n",
    "      ax.scatter(x, param_cosine_sims, color=color, s=s, alpha=0.6)\n",
    "\n",
    "    max_num_epochs = max(max_num_epochs, num_epochs)\n",
    "\n",
    "  if max_num_epochs > 0:\n",
    "    x = np.arange(max_num_epochs)\n",
    "    xlabels = [f\"{int(e)}\" for e in x]\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "\n",
    "  ymin = ax.get_ylim()[0]\n",
    "  ymin = min(-0.1, ymin)\n",
    "  ax.set_ylim(ymin, 1.1)\n",
    "\n",
    "  ax.axhline(0, ls=\"dashed\", color=\"k\", zorder=-5, alpha=0.5)\n",
    "\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.set_ylabel(\"Cosine similarity\")\n",
    "  ax.set_title(\"Cosine similarity to backprop gradients\")\n",
    "  ax.legend()\n",
    "\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VgicQ6K-fiq5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "VgicQ6K-fiq5",
    "outputId": "f96f5896-0882-44b4-ba26-53ca51ed5443"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "cosine_sim_dict = dict()\n",
    "for model_type in [\"backprop\", \"Hebbian\"]:\n",
    "  model_params = {\n",
    "      \"num_hidden\": NUM_HIDDEN,\n",
    "      \"activation_type\": ACTIVATION,\n",
    "      \"bias\": BIAS,\n",
    "  }\n",
    "\n",
    "  if model_type == \"Hebbian\":\n",
    "    model_fct = HebbianMultiLayerPerceptron\n",
    "    lr = HEBB_LR\n",
    "  else:\n",
    "    model_fct = MultiLayerPerceptron\n",
    "    lr = LR\n",
    "\n",
    "  model = model_fct(**model_params)\n",
    "\n",
    "  optimizer = BasicOptimizer(model.parameters(), lr=lr)\n",
    "\n",
    "  print(f\"Collecting Cosine similarities for {model_type}-trained model...\")\n",
    "  model_cosine_sim_dict = train_and_calculate_cosine_sim(\n",
    "      model, train_loader, valid_loader, optimizer, num_epochs=NUM_EPOCHS\n",
    "      )\n",
    "\n",
    "  cosine_sim_dict[model_type] = [\n",
    "      cos_sim for cos_sim in model_cosine_sim_dict.values()\n",
    "      ]\n",
    "\n",
    "plot_gradient_cosine_sims(cosine_sim_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaoGA2A6o-vU",
   "metadata": {
    "id": "XaoGA2A6o-vU"
   },
   "source": [
    "As expected, gradient updates proposed by error backpropagation necessarily have **no bias** with respect to themselves (cosine similarity near 1.0). In contrast, although gradient updates proposed by Hebbian learning for the **second layer** (big dots) are **well aligned** with error backpropagation updates, the updates proposed for the **first layer** (small dots) are **highly biased** (cosine similarity near 0.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WZauiQLz-SFa",
   "metadata": {
    "id": "WZauiQLz-SFa"
   },
   "source": [
    "â“ **What might this result tell us about how Hebbian learning learns in this task?**  \n",
    "â“ **What does the Hebbian learning cosine similarity to error backpropagation look like for the 2-class version of the task?**  \n",
    "â“ **Going further: How might this result relate to the Hebbian learning rule's performance on the classification task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K_-hlQrK9Vla",
   "metadata": {
    "id": "K_-hlQrK9Vla"
   },
   "source": [
    "â“ **Taken together, what do the bias and variance properties of each layer tell us about how Hebbian learning learns in this task?**\n",
    "â“ **Going futher: How learning rule-specific are the bias and variance properties of the gradients compared to other performance or learning metrics?**  \n",
    "â“ **Going futher: How do the bias and variance of the gradients relate to the performance of a learning rule on a task?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HnhSbXUMrTHi",
   "metadata": {
    "id": "HnhSbXUMrTHi"
   },
   "source": [
    "## Section 5. Implementing additional learning rules.\n",
    "\n",
    "In this notebook, we implemented learning in a neural network using **Hebbian learning**, and examined how this learning rule performed under various scenarios. Hebbian learning is only one biologically plausible learning rule among many others. Importantly, many of these other rules are better suited to **supervised learning tasks** like image classification.\n",
    "\n",
    "Examples of basic biologically plausible learning rules to explore include **node perturbation**, **weight_perturbation**, **feedback alignment**, and the **Kolen Pollack**.\n",
    "\n",
    "Take a look at Neuromatch's NeuroAI tutorial for the Microlearning day for implementations of these algorithms using numpy. Then, see whether you can reimplement one or several of them using **custom `torch` autograd functions**, as demonstrated in this notebook.\n",
    "\n",
    "Implementing certain learning rules may also require making some changes to how the **optimizer step** is performed. To do so, you can adapt the `BasicOptimizer()` or any other `torch` optimizer, as needed.\n",
    "\n",
    "The following repositories could be very helpful resources, as they implement these learning rules using custom `torch` autograd functions:\n",
    "- **Feedback alignment:** https://github.com/L0SG/feedback-alignment-pytorch/blob/master/lib/fa_linear.py\n",
    "- **Kolen-Pollack:** https://github.com/limberc/DL-without-Weight-Transport-PyTorch/blob/master/linear.py. Also take a look at `main.py` to see how they adapt the SGD optimizer to produce the correct updates for Kolen Pollack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z_fCZ7ON_9hm",
   "metadata": {
    "id": "z_fCZ7ON_9hm"
   },
   "source": [
    "## Section 6. Tips & Suggestions\n",
    "\n",
    "Here are a few tips that may be helpful as you delve into questions from the project template.\n",
    "\n",
    "- **Testing whether the metrics are specific to a learning rule:** There are a few ways to assess whether certain performance and learning metrics are specific to a learning rule. Examples include:\n",
    "  - **Visualization:** You could plot the metrics or a lower-dimensional version of the metrics to visualize whether the metrics for each learning rule form separate clusters or whether they all mix together.\n",
    "  - **Classification:** You could test whether a linear classifier can be trained to correctly predict the learning rule from the metrics.\n",
    "\n",
    "- **Assessing how your models respond to more challenging learning scenarios**: There are several challenging learning scenarios you can implement. Examples include:  \n",
    "  - **Online learning:** Training with a batch size of one.\n",
    "  - **Non-stationary data:** Changing the distribution of the data across learning. Here, the `restrict_classes(dataset)` function may be useful. For example, you could initially train a model on a dataset with no examples from the `6` class, and then introduce examples from the `6` class partway through training to see how this affects learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IDFlqlbPwRfs",
   "metadata": {
    "id": "IDFlqlbPwRfs"
   },
   "source": [
    "## Additional References\n",
    "Original papers introducing these biologically plausible learning rules:\n",
    "- Node perturbation (Andes et al., 1990, IJCNN)\n",
    "- Weight perturbation ([Mazzoni et al., 1991, PNAS](https://www.pnas.org/doi/abs/10.1073/pnas.88.10.4433))\n",
    "- Feedback alignment ([Lillicrap et al., 2016, Nature Communications](https://www.nature.com/articles/ncomms13276))\n",
    "- Kolen-Pollack algorithm ([Kolen & Pollack, 1994, ICNN](https://ieeexplore.ieee.org/document/374486); [Akrout et al., 2019, NeurIPS](https://arxiv.org/abs/1904.05391))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PG_Ellxc8ABj",
   "metadata": {
    "id": "PG_Ellxc8ABj"
   },
   "source": [
    "### â­ We hope you enjoy working on your project and, through the process, make some interesting discoveries about the challenges and potentials of biologically plausible learning! â­"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00587a334b4e4d46bdb5b5cf59941499": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_77ac927bd67b46cb9c3935000debbd08",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Video available at https://youtube.com/watch?v=<video_id_1>\n"
        ]
       },
       {
        "data": {
         "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/<video_id_1>?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ",
         "text/plain": "<IPython.lib.display.YouTubeVideo at 0x78c181a40220>"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "02a13bafa5e049c2b5e202879dcea721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TabModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TabModel",
      "_titles": {
       "0": "Youtube",
       "1": "Bilibili",
       "2": "Osf"
      },
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TabView",
      "box_style": "",
      "children": [
       "IPY_MODEL_00587a334b4e4d46bdb5b5cf59941499",
       "IPY_MODEL_bd1e978893bf4c64baab06fc71290d28",
       "IPY_MODEL_4707c7edbca646f4a703a1a425026f53"
      ],
      "layout": "IPY_MODEL_4decccb058e44bcd949199d74f04d3f9",
      "selected_index": 0
     }
    },
    "4707c7edbca646f4a703a1a425026f53": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_7cd92d2728ad4048b02fe98892f5da18",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Video available at https://osf.io/<video_id_3>\n"
        ]
       },
       {
        "data": {
         "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/<video_id_3>/?direct%26mode=render?fs=1&autoplay=False\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ",
         "text/plain": "<__main__.PlayVideo at 0x78c180122770>"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "4decccb058e44bcd949199d74f04d3f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52e4ef1a33b3488492e5c516215e93b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77ac927bd67b46cb9c3935000debbd08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cd92d2728ad4048b02fe98892f5da18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd1e978893bf4c64baab06fc71290d28": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_52e4ef1a33b3488492e5c516215e93b4",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Video available at https://www.bilibili.com/video/<video_id_2>\n"
        ]
       },
       {
        "data": {
         "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=<video_id_2>&page=1?fs=1&autoplay=False\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ",
         "text/plain": "<__main__.PlayVideo at 0x78c181a401f0>"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
